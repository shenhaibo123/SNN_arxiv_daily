# 663、用于深度分布强化学习的多室神经元和种群编码改进的Spiking神经网络
- [ ] Multi-compartment Neuron and Population Encoding improved Spiking Neural Network for Deep Distributional Reinforcement Learning 
时间：2023年01月18日                         第一作者：Yinqian Sun                       [链接](https://arxiv.org/abs/2301.07275).                     
## 摘要：受大脑中具有二进制脉冲的信息处理的启发，脉冲神经网络（SNN）表现出显著的低能耗，更适合结合多尺度生物特征。刺突神经元作为SNN的基本信息处理单元，在大多数SNN中通常被简化，它们只考虑LIF点神经元，而不考虑生物神经元的多室结构特性。这限制了SNN的计算和学习能力。在本文中，我们结合生物启发多室神经元（MCN）模型和群体编码方法，提出了一种基于脑启发SNN的深度分布强化学习算法。所提出的多室神经元构建了顶端树突、基底树突和体细胞计算室的结构和功能，以实现接近生物神经元的计算能力。此外，我们提出了一个隐式分数emb
<details>	<summary>英文摘要</summary>	Inspired by the information processing with binary spikes in the brain, the spiking neural networks (SNNs) exhibit significant low energy consumption and are more suitable for incorporating multi-scale biological characteristics. Spiking Neurons, as the basic information processing unit of SNNs, are often simplified in most SNNs which only consider LIF point neuron and do not take into account the multi-compartmental structural properties of biological neurons. This limits the computational and learning capabilities of SNNs. In this paper, we proposed a brain-inspired SNN-based deep distributional reinforcement learning algorithm with combination of bio-inspired multi-compartment neuron (MCN) model and population coding method. The proposed multi-compartment neuron built the structure and function of apical dendritic, basal dendritic, and somatic computing compartments to achieve the computational power close to that of biological neurons. Besides, we present an implicit fractional embedding method based on spiking neuron population encoding. We tested our model on Atari games, and the experiment results show that the performance of our model surpasses the vanilla ANN-based FQF model and ANN-SNN conversion method based Spiking-FQF models. The ablation experiments show that the proposed multi-compartment neural model and quantile fraction implicit population spike representation play an important role in realizing SNN-based deep distributional reinforcement learning. </details>
<details>	<summary>邮件日期</summary>	2023年01月19日</details>

# 662、脉冲神经网络决策反馈均衡
- [ ] Spiking Neural Network Decision Feedback Equalization 
时间：2023年01月18日                         第一作者：Eike-Manuel Bansbach                       [链接](https://arxiv.org/abs/2211.04756).                     
<details>	<summary>注释</summary>	accepted for publication at SCC 2023 </details>
<details>	<summary>邮件日期</summary>	2023年01月19日</details>

# 661、通过故障感知阈值电压优化提高脉冲神经网络的可靠性
- [ ] Improving Reliability of Spiking Neural Networks through Fault Aware Threshold Voltage Optimization 
时间：2023年01月12日                         第一作者：Ayesha Siddique                       [链接](https://arxiv.org/abs/2301.05266).                     
## 摘要：蜘蛛神经网络借助于神经形态硬件，在计算机视觉方面取得了突破。然而，神经形态硬件缺乏并行性，因此限制了边缘设备上SNN的吞吐量和硬件加速。为了解决这个问题，最近提出了许多收缩阵列SNN加速器（systolicSNN），但其可靠性仍然是一个主要问题。在本文中，我们首先广泛分析了永久性故障对收缩神经网络的影响。然后，我们提出了一种新的故障缓解方法，即再训练中的故障感知阈值电压优化（FalVolt）。FalVolt在再培训中优化了每一层的阈值电压，以在存在故障时实现接近基线的分类精度。为了证明我们提出的缓解措施的有效性，我们在256x256系统SNN上对静态（即MNIST）和神经形态数据集（即N-MNIST和DVS手势）进行了分类，该系统具有卡住故障。我们经验丰富
<details>	<summary>英文摘要</summary>	Spiking neural networks have made breakthroughs in computer vision by lending themselves to neuromorphic hardware. However, the neuromorphic hardware lacks parallelism and hence, limits the throughput and hardware acceleration of SNNs on edge devices. To address this problem, many systolic-array SNN accelerators (systolicSNNs) have been proposed recently, but their reliability is still a major concern. In this paper, we first extensively analyze the impact of permanent faults on the SystolicSNNs. Then, we present a novel fault mitigation method, i.e., fault-aware threshold voltage optimization in retraining (FalVolt). FalVolt optimizes the threshold voltage for each layer in retraining to achieve the classification accuracy close to the baseline in the presence of faults. To demonstrate the effectiveness of our proposed mitigation, we classify both static (i.e., MNIST) and neuromorphic datasets (i.e., N-MNIST and DVS Gesture) on a 256x256 systolicSNN with stuck-at faults. We empirically show that the classification accuracy of a systolicSNN drops significantly even at extremely low fault rates (as low as 0.012\%). Our proposed FalVolt mitigation method improves the performance of systolicSNNs by enabling them to operate at fault rates of up to 60\%, with a negligible drop in classification accuracy (as low as 0.1\%). Our results show that FalVolt is 2x faster compared to other state-of-the-art techniques common in artificial neural networks (ANNs), such as fault-aware pruning and retraining without threshold voltage optimization. </details>
<details>	<summary>注释</summary>	Accepted full paper in DATE 2023 </details>
<details>	<summary>邮件日期</summary>	2023年01月16日</details>

# 660、安全感知近似脉冲神经网络
- [ ] Security-Aware Approximate Spiking Neural Networks 
时间：2023年01月12日                         第一作者：Syed Tihaam Ahmad                       [链接](https://arxiv.org/abs/2301.05264).                     
## 摘要：深度神经网络（DNN）和刺突神经网络（SNN）都以其对对抗性攻击的敏感性而闻名。因此，最近的研究人员广泛研究了DNN和SNN在对抗性攻击下的鲁棒性和防御。与精确SNN（AccSNN）相比，已知近似SNN（AxSNN）在超低功率应用中的能效高达4倍。不幸的是，AxSNN在对抗性攻击下的鲁棒性尚未得到研究。在本文中，我们首先广泛分析了具有不同结构参数和近似水平的AxSNN在两种基于梯度和两种神经形态攻击下的鲁棒性。然后，我们提出了两种新的防御方法，即精确缩放和近似量化感知滤波（AQF），以保护AxSNN。我们使用静态和神经形态数据集评估了这两种防御方法的有效性。我们的结果表明，AxSNN更容易受到对抗性攻击
<details>	<summary>英文摘要</summary>	Deep Neural Networks (DNNs) and Spiking Neural Networks (SNNs) are both known for their susceptibility to adversarial attacks. Therefore, researchers in the recent past have extensively studied the robustness and defense of DNNs and SNNs under adversarial attacks. Compared to accurate SNNs (AccSNN), approximate SNNs (AxSNNs) are known to be up to 4X more energy-efficient for ultra-low power applications. Unfortunately, the robustness of AxSNNs under adversarial attacks is yet unexplored. In this paper, we first extensively analyze the robustness of AxSNNs with different structural parameters and approximation levels under two gradient-based and two neuromorphic attacks. Then, we propose two novel defense methods, i.e., precision scaling and approximate quantization-aware filtering (AQF), for securing AxSNNs. We evaluated the effectiveness of these two defense methods using both static and neuromorphic datasets. Our results demonstrate that AxSNNs are more prone to adversarial attacks than AccSNNs, but precision scaling and AQF significantly improve the robustness of AxSNNs. For instance, a PGD attack on AxSNN results in a 72\% accuracy loss compared to AccSNN without any attack, whereas the same attack on the precision-scaled AxSNN leads to only a 17\% accuracy loss in the static MNIST dataset (4X robustness improvement). Similarly, a Sparse Attack on AxSNN leads to a 77\% accuracy loss when compared to AccSNN without any attack, whereas the same attack on an AxSNN with AQF leads to only a 2\% accuracy loss in the neuromorphic DVS128 Gesture dataset (38X robustness improvement). </details>
<details>	<summary>注释</summary>	Accepted full paper in DATE 2023 </details>
<details>	<summary>邮件日期</summary>	2023年01月16日</details>

# 659、语义匹配：为医疗保健调试XAI中的特征属性方法
- [ ] Semantic match: Debugging feature attribution methods in XAI for healthcare 
时间：2023年01月05日                         第一作者：Giovanni Cin\`a                       [链接](https://arxiv.org/abs/2301.02080).                     
## 摘要：最近，经认证的医疗保健人工智能（AI）工具激增，重新引发了围绕采用这项技术的争论。这类争论的一个线索涉及可解释的人工智能及其使人工智能设备更加透明和可信的承诺。一些活跃在医学人工智能领域的声音对可解释人工智能技术的可靠性表示担忧，特别是特征归因方法，质疑其在指南和标准中的使用和包含。尽管存在合理的担忧，但我们认为，对事后局部解释方法可行性的现有批评通过概括图像数据特有的问题，将婴儿与洗澡水混为一谈。我们首先将问题描述为解释和人类理解之间缺乏语义匹配。为了了解何时可以可靠地使用特征重要性，我们引入了低级别和高级别特征的特征重要性之间的区别。我们认为，对于低级别的数据类型
<details>	<summary>英文摘要</summary>	The recent spike in certified Artificial Intelligence (AI) tools for healthcare has renewed the debate around adoption of this technology. One thread of such debate concerns Explainable AI and its promise to render AI devices more transparent and trustworthy. A few voices active in the medical AI space have expressed concerns on the reliability of Explainable AI techniques and especially feature attribution methods, questioning their use and inclusion in guidelines and standards. Despite valid concerns, we argue that existing criticism on the viability of post-hoc local explainability methods throws away the baby with the bathwater by generalizing a problem that is specific to image data. We begin by characterizing the problem as a lack of semantic match between explanations and human understanding. To understand when feature importance can be used reliably, we introduce a distinction between feature importance of low- and high-level features. We argue that for data types where low-level features come endowed with a clear semantics, such as tabular data like Electronic Health Records (EHRs), semantic match can be obtained, and thus feature attribution methods can still be employed in a meaningful and useful way. </details>
<details>	<summary>邮件日期</summary>	2023年01月06日</details>

# 658、FireFly：用于Spiking神经网络的高吞吐量可重构硬件加速器
- [ ] FireFly: A High-Throughput and Reconfigurable Hardware Accelerator for Spiking Neural Networks 
时间：2023年01月05日                         第一作者：Jindong Li                        [链接](https://arxiv.org/abs/2301.01905).                     
## 摘要：Spiking神经网络（SNN）由于其强大的生物学解释性和高能量效率而被广泛应用。随着反向传播算法和替代梯度的引入，脉冲神经网络的结构变得更加复杂，与人工神经网络的性能差距逐渐缩小。然而，用于现场可编程门阵列（FPGA）的大多数SNN硬件实现不能满足算术或存储器效率要求，这严重限制了SNN的发展。他们不会深入研究二进制脉冲和突触权重之间的算术运算，也不会通过在小任务上使用过于昂贵的设备来假设无限制的片上RAM资源。为了提高运算效率，我们分析了脉冲神经元的神经动力学，将SNN算术运算推广到多路累加运算，并提出了利用DSP48E2硬件实现这种运算的高性能方法
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have been widely used due to their strong biological interpretability and high energy efficiency. With the introduction of the backpropagation algorithm and surrogate gradient, the structure of spiking neural networks has become more complex, and the performance gap with artificial neural networks has gradually decreased. However, most SNN hardware implementations for field-programmable gate arrays (FPGAs) cannot meet arithmetic or memory efficiency requirements, which significantly restricts the development of SNNs. They do not delve into the arithmetic operations between the binary spikes and synaptic weights or assume unlimited on-chip RAM resources by using overly expensive devices on small tasks. To improve arithmetic efficiency, we analyze the neural dynamics of spiking neurons, generalize the SNN arithmetic operation to the multiplex-accumulate operation, and propose a high-performance implementation of such operation by utilizing the DSP48E2 hard block in Xilinx Ultrascale FPGAs. To improve memory efficiency, we design a memory system to enable efficient synaptic weights and membrane voltage memory access with reasonable on-chip RAM consumption. Combining the above two improvements, we propose an FPGA accelerator that can process spikes generated by the firing neuron on-the-fly (FireFly). FireFly is implemented on several FPGA edge devices with limited resources but still guarantees a peak performance of 5.53TSOP/s at 300MHz. As a lightweight accelerator, FireFly achieves the highest computational density efficiency compared with existing research using large FPGA devices. </details>
<details>	<summary>邮件日期</summary>	2023年01月06日</details>

# 657、Spiking神经网络的在线训练
- [ ] Online Training Through Time for Spiking Neural Networks 
时间：2022年12月31日                         第一作者：Mingqing Xiao                       [链接](https://arxiv.org/abs/2210.04195).                     
<details>	<summary>注释</summary>	Accepted by NeurIPS 2022 </details>
<details>	<summary>邮件日期</summary>	2023年01月03日</details>

# 656、基于小波的在线序贯极值学习机网络的电能质量事件识别与分类
- [ ] Power Quality Event Recognition and Classification Using an Online Sequential Extreme Learning Machine Network based on Wavelets 
时间：2022年12月27日                         第一作者：Rahul Kumar Dubey                       [链接](https://arxiv.org/abs/2212.13375).                     
## 摘要：降低的系统可靠性和较高的维护成本可能是电力质量差的结果，这可能会干扰正常的设备性能，加速老化，甚至导致彻底的故障。本研究实现并测试了一个基于小波的在线顺序极端学习机（OS-ELM）分类器原型，用于检测瞬态条件下的电能质量问题。为了创建分类器，将OSELM网络模型和离散小波变换（DWT）方法相结合。首先，使用离散小波变换（DWT）多分辨率分析（MRA）提取不同分辨率下的失真信号特征。OSELM然后根据瞬态持续时间和能量特征对检索到的数据进行排序，以确定扰动的类型。所建议的方法需要更少的存储空间和处理时间，因为它可以在不改变信号原始质量的情况下最小化大量失真信号的特性
<details>	<summary>英文摘要</summary>	Reduced system dependability and higher maintenance costs may be the consequence of poor electric power quality, which can disturb normal equipment performance, speed up aging, and even cause outright failures. This study implements and tests a prototype of an Online Sequential Extreme Learning Machine (OS-ELM) classifier based on wavelets for detecting power quality problems under transient conditions. In order to create the classifier, the OSELM-network model and the discrete wavelet transform (DWT) method are combined. First, discrete wavelet transform (DWT) multi-resolution analysis (MRA) was used to extract characteristics of the distorted signal at various resolutions. The OSELM then sorts the retrieved data by transient duration and energy features to determine the kind of disturbance. The suggested approach requires less memory space and processing time since it can minimize a large quantity of the distorted signal's characteristics without changing the signal's original quality. Several types of transient events were used to demonstrate the classifier's ability to detect and categorize various types of power disturbances, including sags, swells, momentary interruptions, oscillatory transients, harmonics, notches, spikes, flickers, sag swell, sag mi, sag harm, swell trans, sag spike, and swell spike. </details>
<details>	<summary>邮件日期</summary>	2022年12月29日</details>

# 655、具有脉冲编码网络的闭式控制
- [ ] Closed-form control with spike coding networks 
时间：2022年12月25日                         第一作者：Filip S. Slijkhuis                       [链接](https://arxiv.org/abs/2212.12887).                     
## 摘要：使用脉冲神经网络（SNN）的有效和鲁棒控制仍然是一个开放的问题。虽然生物制剂的行为是通过稀疏和不规则的脉冲模式产生的，这种模式提供了鲁棒和有效的控制，但用于控制的大多数人工脉冲神经网络中的活动模式都是密集和规则的，这可能导致效率较低的代码。此外，对于大多数现有的控制解决方案，甚至对于完全识别的系统，网络培训或优化都是必要的，这使其在片上低功耗解决方案中的实现变得复杂。棘波编码网络（SCNs）的神经科学理论为在递归棘波神经网络中实现动态系统提供了一种完全解析的解决方案——同时保持不规则、稀疏和鲁棒的棘波活动——但目前还不清楚如何将其直接应用于控制问题。在这里，我们通过结合闭式最优估计和控制来扩展SCN理论。由此产生的网络
<details>	<summary>英文摘要</summary>	Efficient and robust control using spiking neural networks (SNNs) is still an open problem. Whilst behaviour of biological agents is produced through sparse and irregular spiking patterns, which provide both robust and efficient control, the activity patterns in most artificial spiking neural networks used for control are dense and regular -- resulting in potentially less efficient codes. Additionally, for most existing control solutions network training or optimization is necessary, even for fully identified systems, complicating their implementation in on-chip low-power solutions. The neuroscience theory of Spike Coding Networks (SCNs) offers a fully analytical solution for implementing dynamical systems in recurrent spiking neural networks -- while maintaining irregular, sparse, and robust spiking activity -- but it's not clear how to directly apply it to control problems. Here, we extend SCN theory by incorporating closed-form optimal estimation and control. The resulting networks work as a spiking equivalent of a linear-quadratic-Gaussian controller. We demonstrate robust spiking control of simulated spring-mass-damper and cart-pole systems, in the face of several perturbations, including input- and system-noise, system disturbances, and neural silencing. As our approach does not need learning or optimization, it offers opportunities for deploying fast and efficient task-specific on-chip spiking controllers with biologically realistic activity. </details>
<details>	<summary>注释</summary>	Under review in an IEEE journal </details>
<details>	<summary>邮件日期</summary>	2022年12月27日</details>

# 654、基于Spiking神经网络的坐姿识别
- [ ] Sitting Posture Recognition Using a Spiking Neural Network 
时间：2022年12月25日                         第一作者：Jianquan Wang                       [链接](https://arxiv.org/abs/2212.12908).                     
## 摘要：为了提高市民的生活质量，我们设计了一个个性化的智能椅子系统来识别坐姿。该系统可以从设计的传感器接收表面压力数据，并提供用于引导用户朝向正确坐姿的反馈。我们使用液体状态机和逻辑回归分类器构建了一个脉冲神经网络，用于对15种坐姿进行分类。为了让这个系统能够将我们的压力数据读入脉冲神经元，我们设计了一种算法，将类似地图的数据编码为余弦秩稀疏数据。由19名参与者的15个坐姿组成的实验结果表明，我们的SNN的预测精度为88.52%。
<details>	<summary>英文摘要</summary>	To increase the quality of citizens' lives, we designed a personalized smart chair system to recognize sitting behaviors. The system can receive surface pressure data from the designed sensor and provide feedback for guiding the user towards proper sitting postures. We used a liquid state machine and a logistic regression classifier to construct a spiking neural network for classifying 15 sitting postures. To allow this system to read our pressure data into the spiking neurons, we designed an algorithm to encode map-like data into cosine-rank sparsity data. The experimental results consisting of 15 sitting postures from 19 participants show that the prediction precision of our SNN is 88.52%. </details>
<details>	<summary>邮件日期</summary>	2022年12月27日</details>

# 653、螳螂：利用Spiking神经网络实现节能的自主移动代理
- [ ] Mantis: Enabling Energy-Efficient Autonomous Mobile Agents with Spiking Neural Networks 
时间：2022年12月24日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2212.12620).                     
## 摘要：无人驾驶飞行器（UAV）和移动机器人等自主移动代理在提高人类生产力方面显示出巨大潜力。由于这些移动代理通常由电池供电，因此它们需要低功率/能量消耗以具有较长的寿命。这些代理还需要适应不断变化的/动态的环境，特别是当部署在遥远或危险的位置时，因此需要高效的在线学习能力。这些要求可以通过使用Spiking神经网络（SNN）来满足，因为SNN由于稀疏计算而提供低功耗/能耗。然而，仍然需要一种方法来在自主移动代理上使用适当的SNN模型。为此，我们提出了一种Mantis方法，在自主移动代理上系统地使用SNN，以在动态环境中实现节能处理和自适应能力。我们螳螂的主要思想包括
<details>	<summary>英文摘要</summary>	Autonomous mobile agents such as unmanned aerial vehicles (UAVs) and mobile robots have shown huge potential for improving human productivity. These mobile agents require low power/energy consumption to have a long lifespan since they are usually powered by batteries. These agents also need to adapt to changing/dynamic environments, especially when deployed in far or dangerous locations, thus requiring efficient online learning capabilities. These requirements can be fulfilled by employing Spiking Neural Networks (SNNs) since SNNs offer low power/energy consumption due to sparse computations and efficient online learning due to bio-inspired learning mechanisms. However, a methodology is still required to employ appropriate SNN models on autonomous mobile agents. Towards this, we propose a Mantis methodology to systematically employ SNNs on autonomous mobile agents to enable energy-efficient processing and adaptive capabilities in dynamic environments. The key ideas of our Mantis include the optimization of SNN operations, the employment of a bio-plausible online learning mechanism, and the SNN model selection. The experimental results demonstrate that our methodology maintains high accuracy with a significantly smaller memory footprint and energy consumption (i.e., 3.32x memory reduction and 2.9x energy saving for an SNN model with 8-bit weights) compared to the baseline network with 32-bit weights. In this manner, our Mantis enables the employment of SNNs for resource- and energy-constrained mobile agents. </details>
<details>	<summary>注释</summary>	To appear at the 2023 International Conference on Automation, Robotics and Applications (ICARA), February 2023, Abu Dhabi, UAE. arXiv admin note: text overlap with arXiv:2206.08656 </details>
<details>	<summary>邮件日期</summary>	2022年12月27日</details>

# 652、hxtorch.snn：机器学习启发的BrainScaleS-2上的Spiking神经网络建模
- [ ] hxtorch.snn: Machine-learning-inspired Spiking Neural Network Modeling on BrainScaleS-2 
时间：2022年12月23日                         第一作者：Philipp Spilger                       [链接](https://arxiv.org/abs/2212.12210).                     
## 摘要：神经形态系统需要用户友好的软件来支持实验的设计和优化。在这项工作中，我们通过介绍我们为BrainScaleS-2神经形态系统开发的基于机器学习的建模框架来解决这一需求。这项工作比以前的工作有所改进，以前的工作要么侧重于BrainScaleS-2的矩阵乘法模式，要么缺乏完全自动化。我们的框架名为hxtorch.snn，支持PyTorch内的脉冲神经网络的硬件在环训练，包括支持全自动硬件实验工作流中的自动区分。此外，hxtorch.snn促进了硬件仿真和软件仿真之间的无缝转换。我们使用阴阳数据集展示了hxtorch.snn在分类任务中的能力，该数据集采用基于梯度的方法，具有替代梯度和BrainScaleS-2硬件系统中密集采样的膜观察结果。
<details>	<summary>英文摘要</summary>	Neuromorphic systems require user-friendly software to support the design and optimization of experiments. In this work, we address this need by presenting our development of a machine learning-based modeling framework for the BrainScaleS-2 neuromorphic system. This work represents an improvement over previous efforts, which either focused on the matrix-multiplication mode of BrainScaleS-2 or lacked full automation. Our framework, called hxtorch.snn, enables the hardware-in-the-loop training of spiking neural networks within PyTorch, including support for auto differentiation in a fully-automated hardware experiment workflow. In addition, hxtorch.snn facilitates seamless transitions between emulating on hardware and simulating in software. We demonstrate the capabilities of hxtorch.snn on a classification task using the Yin-Yang dataset employing a gradient-based approach with surrogate gradients and densely sampled membrane observations from the BrainScaleS-2 hardware system. </details>
<details>	<summary>邮件日期</summary>	2022年12月26日</details>

# 651、ReLU网络到Spiking神经网络的精确映射
- [ ] An Exact Mapping From ReLU Networks to Spiking Neural Networks 
时间：2022年12月23日                         第一作者：Ana Stanojevic                       [链接](https://arxiv.org/abs/2212.12522).                     
## 摘要：深度脉冲神经网络（SNN）提供了低功耗人工智能的前景。然而，从头开始训练深度SNN或将深度人工神经网络转换为SNN而不损失性能一直是一个挑战。在这里，我们提出了一个从具有整流线性单元（ReLU）的网络到每个神经元只发射一个脉冲的SNN的精确映射。对于我们的构造性证明，我们假设具有或不具有卷积层、批归一化和最大池化层的任意多层ReLU网络在某些训练集上被训练为高性能。此外，我们假设我们可以访问训练期间使用的输入数据的代表性示例以及训练后的ReLU网络的精确参数（权重和偏差）。从深度ReLU网络到SNN的映射导致CIFAR10、CIFAR100和类似ImageNet的数据集Places365和PASS的准确度下降了零。更一般地说，我们的工作表明，任意深度ReLU的网络都可以被
<details>	<summary>英文摘要</summary>	Deep spiking neural networks (SNNs) offer the promise of low-power artificial intelligence. However, training deep SNNs from scratch or converting deep artificial neural networks to SNNs without loss of performance has been a challenge. Here we propose an exact mapping from a network with Rectified Linear Units (ReLUs) to an SNN that fires exactly one spike per neuron. For our constructive proof, we assume that an arbitrary multi-layer ReLU network with or without convolutional layers, batch normalization and max pooling layers was trained to high performance on some training set. Furthermore, we assume that we have access to a representative example of input data used during training and to the exact parameters (weights and biases) of the trained ReLU network. The mapping from deep ReLU networks to SNNs causes zero percent drop in accuracy on CIFAR10, CIFAR100 and the ImageNet-like data sets Places365 and PASS. More generally our work shows that an arbitrary deep ReLU network can be replaced by an energy-efficient single-spike neural network without any loss of performance. </details>
<details>	<summary>邮件日期</summary>	2022年12月26日</details>

# 650、传感器和神经形态计算是您实现高效节能计算机视觉所需的一切
- [ ] In-Sensor & Neuromorphic Computing are all you need for Energy Efficient Computer Vision 
时间：2022年12月21日                         第一作者：Gourav Datta                       [链接](https://arxiv.org/abs/2212.10881).                     
## 摘要：由于高激活稀疏性和使用累积（AC）而不是昂贵的乘法和累积（MAC），神经形态脉冲神经网络（SNN）已成为几种计算机视觉（CV）应用中传统DNN的低功耗替代方案。然而，大多数现有的SNN需要多个时间步骤来获得可接受的推理精度，这阻碍了实时部署，增加了峰值活动，从而增加了能耗。最近的工作提出了直接编码，其直接馈送SNN的第一层中的模拟像素值，以显著减少时间步长的数量。尽管具有直接编码的第一层MAC的开销对于深度SNN是可忽略的，并且CV处理使用SNN是有效的，但是图像传感器和下游处理之间的数据传输消耗了大量带宽，并且可能支配总能量。为了减轻这种担忧，我们提出了一种传感器内计算硬件软件
<details>	<summary>英文摘要</summary>	Due to the high activation sparsity and use of accumulates (AC) instead of expensive multiply-and-accumulates (MAC), neuromorphic spiking neural networks (SNNs) have emerged as a promising low-power alternative to traditional DNNs for several computer vision (CV) applications. However, most existing SNNs require multiple time steps for acceptable inference accuracy, hindering real-time deployment and increasing spiking activity and, consequently, energy consumption. Recent works proposed direct encoding that directly feeds the analog pixel values in the first layer of the SNN in order to significantly reduce the number of time steps. Although the overhead for the first layer MACs with direct encoding is negligible for deep SNNs and the CV processing is efficient using SNNs, the data transfer between the image sensors and the downstream processing costs significant bandwidth and may dominate the total energy. To mitigate this concern, we propose an in-sensor computing hardware-software co-design framework for SNNs targeting image recognition tasks. Our approach reduces the bandwidth between sensing and processing by 12-96x and the resulting total energy by 2.32x compared to traditional CV processing, with a 3.8% reduction in accuracy on ImageNet. </details>
<details>	<summary>邮件日期</summary>	2022年12月22日</details>

# 649、突发通信增强了由Spiking神经网络控制的进化蜂群的觅食行为
- [ ] Emergent communication enhances foraging behaviour in evolved swarms controlled by Spiking Neural Networks 
时间：2022年12月16日                         第一作者：Cristian Jimenez Romero                       [链接](https://arxiv.org/abs/2212.08484).                     
## 摘要：蚂蚁等群居昆虫通过信息素进行交流，信息素使它们能够协调活动，以群体的形式解决复杂的任务，例如觅食。这种行为是通过进化过程形成的。在计算模型中，使用概率或行动规则来实现群体中的自我协调，以形成每个代理的决策和集体行为。然而，人工调整的决策规则可能会限制群体的行为。在这项工作中，我们在没有定义任何规则的情况下，研究了进化群体中自我协调和沟通的出现。我们进化出一群代表蚁群的代理。我们使用遗传算法来优化脉冲神经网络（SNN），该网络充当人工大脑来控制每个代理的行为。蜂群的目标是在最短的时间内找到最佳的觅食方式。在进化阶段，蚂蚁能够通过在食物附近沉积信息素来学习协作
<details>	<summary>英文摘要</summary>	Social insects such as ants communicate via pheromones which allows them to coordinate their activity and solve complex tasks as a swarm, e.g. foraging for food. This behaviour was shaped through evolutionary processes. In computational models, self-coordination in swarms has been implemented using probabilistic or action rules to shape the decision of each agent and the collective behaviour. However, manual tuned decision rules may limit the behaviour of the swarm. In this work we investigate the emergence of self-coordination and communication in evolved swarms without defining any rule. We evolve a swarm of agents representing an ant colony. We use a genetic algorithm to optimize a spiking neural network (SNN) which serves as an artificial brain to control the behaviour of each agent. The goal of the colony is to find optimal ways to forage for food in the shortest amount of time. In the evolutionary phase, the ants are able to learn to collaborate by depositing pheromone near food piles and near the nest to guide its cohorts. The pheromone usage is not encoded into the network; instead, this behaviour is established through the optimization procedure. We observe that pheromone-based communication enables the ants to perform better in comparison to colonies where communication did not emerge. We assess the foraging performance by comparing the SNN based model to a rule based system. Our results show that the SNN based model can complete the foraging task more efficiently in a shorter time. Our approach illustrates that even in the absence of pre-defined rules, self coordination via pheromone emerges as a result of the network optimization. This work serves as a proof of concept for the possibility of creating complex applications utilizing SNNs as underlying architectures for multi-agent interactions where communication and self-coordination is desired. </details>
<details>	<summary>注释</summary>	18, pages, 8 figures </details>
<details>	<summary>邮件日期</summary>	2022年12月19日</details>

# 648、确保刺突：关于刺突神经网络对对手示例的可转移性和安全性
- [ ] Securing the Spike: On the Transferability and Security of Spiking Neural Networks to Adversarial Examples 
时间：2022年12月12日                         第一作者：Nuo Xu                       [链接](https://arxiv.org/abs/2209.03358).                     
<details>	<summary>邮件日期</summary>	2022年12月14日</details>

# 647、绝热极限下基于能量的通用顺序情节记忆网络
- [ ] Energy-based General Sequential Episodic Memory Networks at the Adiabatic Limit 
时间：2022年12月11日                         第一作者：Arjun Karuvally                       [链接](https://arxiv.org/abs/2212.05563).                     
## 摘要：通用联想记忆模型（GAMM）具有一个恒定的依赖于状态的能量表面，它将输出动态引向固定点，从一组可以异步预加载的存储器中检索单个存储器。我们引入了一类新的通用顺序情节记忆模型（GSEMM），该模型在绝热极限下表现出随时间变化的能量表面，导致一系列亚稳态，即顺序情节记忆。动态能量表面是通过新引入的不对称突触实现的，在网络的隐藏层中具有信号传播延迟。我们研究了GSEMM类的两个记忆模型的理论和经验性质，它们的激活函数不同。LISEM在特征层具有非线性，而DSEM在隐藏层具有非线性。原则上，DSEM的存储容量随网络中神经元的数量呈指数增长。我们介绍了突触基础的学习规则
<details>	<summary>英文摘要</summary>	The General Associative Memory Model (GAMM) has a constant state-dependant energy surface that leads the output dynamics to fixed points, retrieving single memories from a collection of memories that can be asynchronously preloaded. We introduce a new class of General Sequential Episodic Memory Models (GSEMM) that, in the adiabatic limit, exhibit temporally changing energy surface, leading to a series of meta-stable states that are sequential episodic memories. The dynamic energy surface is enabled by newly introduced asymmetric synapses with signal propagation delays in the network's hidden layer. We study the theoretical and empirical properties of two memory models from the GSEMM class, differing in their activation functions. LISEM has non-linearities in the feature layer, whereas DSEM has non-linearity in the hidden layer. In principle, DSEM has a storage capacity that grows exponentially with the number of neurons in the network. We introduce a learning rule for the synapses based on the energy minimization principle and show it can learn single memories and their sequential relationships online. This rule is similar to the Hebbian learning algorithm and Spike-Timing Dependent Plasticity (STDP), which describe conditions under which synapses between neurons change strength. Thus, GSEMM combines the static and dynamic properties of episodic memory under a single theoretical framework and bridges neuroscience, machine learning, and artificial intelligence. </details>
<details>	<summary>邮件日期</summary>	2022年12月13日</details>

# 646、神经脉冲解码的拓扑深度学习框架
- [ ] A Topological Deep Learning Framework for Neural Spike Decoding 
时间：2022年12月01日                         第一作者：Edward C. Mitchell                       [链接](https://arxiv.org/abs/2212.05037).                     
## 摘要：大脑的空间定向系统使用不同的神经元群来辅助基于环境的导航。大脑对空间信息进行编码的方式之一是通过网格细胞，即覆盖在一起的多层神经元，提供基于环境的导航。这些神经元在一个集合中激发，其中几个神经元同时激发以激活单个网格。我们希望捕获这个激发结构，并使用它来解码网格单元数据。理解、表示和解码这些神经结构需要包含比传统基于图的模型更高阶连接性的模型。为此，在这项工作中，我们开发了一个用于神经脉冲序列解码的拓扑深度学习框架。我们的框架将无监督的简单复杂发现与深度学习的能力相结合，通过我们在此开发的一种新架构，称为简单卷积递归神经网络（SCRNN）。简单复形，不仅使用顶点和边的拓扑空间
<details>	<summary>英文摘要</summary>	The brain's spatial orientation system uses different neuron ensembles to aid in environment-based navigation. One of the ways brains encode spatial information is through grid cells, layers of decked neurons that overlay to provide environment-based navigation. These neurons fire in ensembles where several neurons fire at once to activate a single grid. We want to capture this firing structure and use it to decode grid cell data. Understanding, representing, and decoding these neural structures require models that encompass higher order connectivity than traditional graph-based models may provide. To that end, in this work, we develop a topological deep learning framework for neural spike train decoding. Our framework combines unsupervised simplicial complex discovery with the power of deep learning via a new architecture we develop herein called a simplicial convolutional recurrent neural network (SCRNN). Simplicial complexes, topological spaces that use not only vertices and edges but also higher-dimensional objects, naturally generalize graphs and capture more than just pairwise relationships. Additionally, this approach does not require prior knowledge of the neural activity beyond spike counts, which removes the need for similarity measurements. The effectiveness and versatility of the SCRNN is demonstrated on head direction data to test its performance and then applied to grid cell datasets with the task to automatically predict trajectories. </details>
<details>	<summary>邮件日期</summary>	2022年12月12日</details>

# 645、精度降低对Spiking神经网络影响的快速探索
- [ ] Fast Exploration of the Impact of Precision Reduction on Spiking Neural Networks 
时间：2022年11月22日                         第一作者：Sepide Saeedi                       [链接](https://arxiv.org/abs/2212.11782).                     
## 摘要：近似计算（AxC）技术以计算精度换取性能、能量和面积减少增益。当应用程序本质上能够容忍某些精度损失时，这种权衡尤其方便，如Spiking神经网络（SNN）的情况。当目标硬件达到计算边缘时，SNN是一个实用的选择，但这需要一些面积最小化策略。在这项工作中，我们使用区间算术（IA）模型来开发一种探索方法，该方法利用这种模型传播近似误差的能力，以检测何时近似超过应用程序的可容忍极限。实验结果证实了显著减少探测时间的能力，为进一步减小网络参数的大小提供了机会，并获得了更细粒度的结果。
<details>	<summary>英文摘要</summary>	Approximate Computing (AxC) techniques trade off the computation accuracy for performance, energy, and area reduction gains. The trade-off is particularly convenient when the applications are intrinsically tolerant to some accuracy loss, as in the Spiking Neural Networks (SNNs) case. SNNs are a practical choice when the target hardware reaches the edge of computing, but this requires some area minimization strategies. In this work, we employ an Interval Arithmetic (IA) model to develop an exploration methodology that takes advantage of the capability of such a model to propagate the approximation error to detect when the approximation exceeds tolerable limits by the application. Experimental results confirm the capability of reducing the exploration time significantly, providing the chance to reduce the network parameters' size further and with more fine-grained results. </details>
<details>	<summary>邮件日期</summary>	2022年12月23日</details>

# 644、为Spiking神经网络开发的模型
- [ ] Models Developed for Spiking Neural Networks 
时间：2022年12月08日                         第一作者：Shahriar Rezghi Shirsavar                       [链接](https://arxiv.org/abs/2212.04377).                     
## 摘要：深度神经网络（DNN）的出现再次引起了人们对人工神经网络（ANN）的极大关注。它们已经成为最先进的模型，并赢得了不同的机器学习挑战。尽管这些网络受到大脑的启发，但它们缺乏生物学上的合理性，与大脑相比，它们具有结构上的差异。Spiking神经网络（SNN）已经存在了很长一段时间，并且已经对其进行了研究，以了解大脑的动态。然而，它们在现实世界和复杂的机器学习任务中的应用有限。最近，他们在解决此类任务方面表现出了巨大的潜力。由于它们的能源效率和时间动态，它们的未来发展前景广阔。在这项工作中，我们回顾了SNN在图像分类任务中的结构和性能。这些比较表明，这些网络对于更复杂的问题表现出强大的能力。此外，简单的学习
<details>	<summary>英文摘要</summary>	Emergence of deep neural networks (DNNs) has raised enormous attention towards artificial neural networks (ANNs) once again. They have become the state-of-the-art models and have won different machine learning challenges. Although these networks are inspired by the brain, they lack biological plausibility, and they have structural differences compared to the brain. Spiking neural networks (SNNs) have been around for a long time, and they have been investigated to understand the dynamics of the brain. However, their application in real-world and complicated machine learning tasks were limited. Recently, they have shown great potential in solving such tasks. Due to their energy efficiency and temporal dynamics there are many promises in their future development. In this work, we reviewed the structures and performances of SNNs on image classification tasks. The comparisons illustrate that these networks show great capabilities for more complicated problems. Furthermore, the simple learning rules developed for SNNs, such as STDP and R-STDP, can be a potential alternative to replace the backpropagation algorithm used in DNNs. </details>
<details>	<summary>注释</summary>	9 pages, 4 figures, 2 tables ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2022年12月09日</details>

# 643、学会看透事件
- [ ] Learning to See Through with Events 
时间：2022年12月05日                         第一作者：Lei Yu                       [链接](https://arxiv.org/abs/2212.02219).                     
## 摘要：尽管合成孔径成像（SAI）可以通过模糊掉离焦前景遮挡，同时从多视图图像中恢复焦内遮挡场景来实现透视效果，但其性能通常会因密集遮挡和极端照明条件而恶化。为了解决这个问题，本文提出了一种基于事件的SAI（E-SAI）方法，该方法依赖于事件摄像机获取的具有极低延迟和高动态范围的异步事件。具体地说，收集的事件首先由重新聚焦网络模块重新聚焦，以对齐对焦事件，同时分散离焦事件。随后，提出了一种由脉冲神经网络（SNN）和卷积神经网络（CNN）组成的混合网络，以编码来自重新聚焦事件的时空信息，并重建被遮挡目标的视觉图像。大量实验表明，我们提出的E-SAI方法在处理非常密集的
<details>	<summary>英文摘要</summary>	Although synthetic aperture imaging (SAI) can achieve the seeing-through effect by blurring out off-focus foreground occlusions while recovering in-focus occluded scenes from multi-view images, its performance is often deteriorated by dense occlusions and extreme lighting conditions. To address the problem, this paper presents an Event-based SAI (E-SAI) method by relying on the asynchronous events with extremely low latency and high dynamic range acquired by an event camera. Specifically, the collected events are first refocused by a Refocus-Net module to align in-focus events while scattering out off-focus ones. Following that, a hybrid network composed of spiking neural networks (SNNs) and convolutional neural networks (CNNs) is proposed to encode the spatio-temporal information from the refocused events and reconstruct a visual image of the occluded targets. Extensive experiments demonstrate that our proposed E-SAI method can achieve remarkable performance in dealing with very dense occlusions and extreme lighting conditions and produce high-quality images from pure events. Codes and datasets are available at https://dvs-whu.cn/projects/esai/. </details>
<details>	<summary>注释</summary>	Accepted by IEEE TPAMI. arXiv admin note: text overlap with arXiv:2103.02376 </details>
<details>	<summary>邮件日期</summary>	2022年12月06日</details>

# 642、基于事件的视觉中Spiking卷积神经网络的对抗攻击
- [ ] Adversarial Attacks on Spiking Convolutional Neural Networks for Event-based Vision 
时间：2022年12月05日                         第一作者：Julian B\"uchel                       [链接](https://arxiv.org/abs/2110.02929).                     
<details>	<summary>注释</summary>	9 pages plus Supplementary Material. Accepted in Frontiers in Neuroscience -- Neuromorphic Engineering </details>
<details>	<summary>邮件日期</summary>	2022年12月06日</details>

# 641、THOR——一种具有7.29G TSOP$^2$/mm$^2Js能量吞吐量效率的神经形态处理器
- [ ] THOR -- A Neuromorphic Processor with 7.29G TSOP$^2$/mm$^2$Js Energy-Throughput Efficiency 
时间：2022年12月03日                         第一作者：Mayank Senapati                       [链接](https://arxiv.org/abs/2212.01696).                     
## 摘要：使用生物启发的Spiking神经网络（SNN）进行神经形态计算是一种很有前途的解决方案，可以满足边缘计算设备所需的能量吞吐量（ET）效率。已经提出了在模拟/混合信号域中仿真SNN的神经形态硬件架构，以实现比所有数字架构更高数量级的能量效率，然而代价是有限的可扩展性、对噪声的敏感性、复杂的验证和较差的灵活性。另一方面，最先进的数字神经形态架构关注于实现高能量效率（焦耳/突触操作（SOP））或吞吐量效率（SOP/秒/面积），导致ET效率低下。在这项工作中，我们介绍了THOR，一种全数字神经形态处理器，具有新颖的存储器层次结构和神经元更新架构，解决了能耗和吞吐量瓶颈。我们在28nm FDSOI CMOS技术中实现了THOR，我们的后布局结果要求
<details>	<summary>英文摘要</summary>	Neuromorphic computing using biologically inspired Spiking Neural Networks (SNNs) is a promising solution to meet Energy-Throughput (ET) efficiency needed for edge computing devices. Neuromorphic hardware architectures that emulate SNNs in analog/mixed-signal domains have been proposed to achieve order-of-magnitude higher energy efficiency than all-digital architectures, however at the expense of limited scalability, susceptibility to noise, complex verification, and poor flexibility. On the other hand, state-of-the-art digital neuromorphic architectures focus either on achieving high energy efficiency (Joules/synaptic operation (SOP)) or throughput efficiency (SOPs/second/area), resulting in poor ET efficiency. In this work, we present THOR, an all-digital neuromorphic processor with a novel memory hierarchy and neuron update architecture that addresses both energy consumption and throughput bottlenecks. We implemented THOR in 28nm FDSOI CMOS technology and our post-layout results demonstrate an ET efficiency of 7.29G $\text{TSOP}^2/\text{mm}^2\text{Js}$ at 0.9V, 400 MHz, which represents a 3X improvement over state-of-the-art digital neuromorphic processors. </details>
<details>	<summary>邮件日期</summary>	2022年12月06日</details>

# 640、损失整形增强Spiking神经网络中EventProp的精确梯度学习
- [ ] Loss shaping enhances exact gradient learning with EventProp in Spiking Neural Networks 
时间：2022年12月02日                         第一作者：Thomas Nowotny                       [链接](https://arxiv.org/abs/2212.01232).                     
## 摘要：在最近的一篇论文中，Wunderlich和Pehle介绍了EventProp算法，该算法能够通过精确梯度上的梯度下降来训练脉冲神经网络。在本文中，我们提出了EventProp的扩展，以支持更广泛的损失函数，并在利用稀疏性的GPU增强神经元网络框架中实现。GPU加速允许我们在更具挑战性的学习基准上广泛测试EventProp。我们发现EventProp在某些任务上表现良好，但在其他任务上存在学习缓慢或完全失败的问题。在这里，我们详细分析了这些问题，并发现它们与损失函数的精确梯度的使用有关，其本质上不提供由于脉冲创建或脉冲删除而导致的损失变化的信息。根据任务的细节和损失函数，使用EventProp降低精确的梯度会导致删除重要的峰值，从而导致损失的意外增加
<details>	<summary>英文摘要</summary>	In a recent paper Wunderlich and Pehle introduced the EventProp algorithm that enables training spiking neural networks by gradient descent on exact gradients. In this paper we present extensions of EventProp to support a wider class of loss functions and an implementation in the GPU enhanced neuronal networks framework which exploits sparsity. The GPU acceleration allows us to test EventProp extensively on more challenging learning benchmarks. We find that EventProp performs well on some tasks but for others there are issues where learning is slow or fails entirely. Here, we analyse these issues in detail and discover that they relate to the use of the exact gradient of the loss function, which by its nature does not provide information about loss changes due to spike creation or spike deletion. Depending on the details of the task and loss function, descending the exact gradient with EventProp can lead to the deletion of important spikes and so to an inadvertent increase of the loss and decrease of classification accuracy and hence a failure to learn. In other situations the lack of knowledge about the benefits of creating additional spikes can lead to a lack of gradient flow into earlier layers, slowing down learning. We eventually present a first glimpse of a solution to these problems in the form of `loss shaping', where we introduce a suitable weighting function into an integral loss to increase gradient flow from the output layer towards earlier layers. </details>
<details>	<summary>注释</summary>	18 pages, 7 figures MSC-class: 68T05 (Primary) 68T10, 68T07 (Secondary) ACM-class: I.2; I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2022年12月05日</details>

# 639、替代梯度Spiking神经网络作为大词汇连续语音识别的编码器
- [ ] Surrogate Gradient Spiking Neural Networks as Encoders for Large Vocabulary Continuous Speech Recognition 
时间：2022年12月01日                         第一作者：Alex                       [链接](https://arxiv.org/abs/2212.01187).                     
## 摘要：与传统的人工神经元相比，受生物启发的脉冲神经元可以传递稀疏的二进制信息，这也可以实现节能。最近的研究表明，使用替代梯度方法可以像标准递归神经网络一样训练脉冲神经网络。他们在语音命令识别任务上显示出了有希望的结果。使用相同的技术，我们证明了它们可以扩展到大词汇连续语音识别，在这种情况下，它们能够替换编码器中的LSTM，而性能损失很小。这表明它们可能适用于更复杂的顺序任务。此外，与它们经常出现的非峰值对应物相比，它们在不需要使用门的情况下显示出对爆炸性梯度问题的鲁棒性。
<details>	<summary>英文摘要</summary>	Compared to conventional artificial neurons that produce dense and real-valued responses, biologically-inspired spiking neurons transmit sparse and binary information, which can also lead to energy-efficient implementations. Recent research has shown that spiking neural networks can be trained like standard recurrent neural networks using the surrogate gradient method. They have shown promising results on speech command recognition tasks. Using the same technique, we show that they are scalable to large vocabulary continuous speech recognition, where they are capable of replacing LSTMs in the encoder with only minor loss of performance. This suggests that they may be applicable to more involved sequence-to-sequence tasks. Moreover, in contrast to their recurrent non-spiking counterparts, they show robustness to exploding gradient problems without the need to use gates. </details>
<details>	<summary>注释</summary>	Submitted to ICASSP 2023 </details>
<details>	<summary>邮件日期</summary>	2022年12月05日</details>

# 638、Spiking神经网络中的时间信息动力学研究
- [ ] Exploring Temporal Information Dynamics in Spiking Neural Networks 
时间：2022年11月30日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2211.14406).                     
<details>	<summary>注释</summary>	Accepted to AAAI2023 </details>
<details>	<summary>邮件日期</summary>	2022年12月01日</details>

# 637、基于跨模态跨域知识转移的无监督脉冲深度估计
- [ ] Unsupervised Spike Depth Estimation via Cross-modality Cross-domain Knowledge Transfer 
时间：2022年11月30日                         第一作者：Jiaming Liu                       [链接](https://arxiv.org/abs/2208.12527).                     
<details>	<summary>邮件日期</summary>	2022年12月01日</details>

# 636、具有忆阻突触的脉冲神经元网络中的序列学习
- [ ] Sequence learning in a spiking neuronal network with memristive synapses 
时间：2022年11月29日                         第一作者：Younes Bouhadjar                       [链接](https://arxiv.org/abs/2211.16592).                     
## 摘要：灵感来自大脑的计算提出了一套算法原理，有望推动人工智能的发展。它们赋予系统自学习能力、高效能源利用和高存储容量。大脑计算的核心概念是序列学习和预测。这种形式的计算对于我们几乎所有的日常任务都是必不可少的，比如运动生成、感知和语言。了解大脑是如何进行这种计算的，这不仅对推进神经科学很重要，而且也为新的技术脑启发应用铺平道路。先前开发的序列预测和回忆的脉冲神经网络实现通过局部的、受生物启发的可塑性规则以无监督的方式学习复杂的高阶序列。神经形态硬件是一种新兴的硬件类型，有望有效地运行这类算法。它模仿了大脑处理信息的方式
<details>	<summary>英文摘要</summary>	Brain-inspired computing proposes a set of algorithmic principles that hold promise for advancing artificial intelligence. They endow systems with self learning capabilities, efficient energy usage, and high storage capacity. A core concept that lies at the heart of brain computation is sequence learning and prediction. This form of computation is essential for almost all our daily tasks such as movement generation, perception, and language. Understanding how the brain performs such a computation is not only important to advance neuroscience but also to pave the way to new technological brain-inspired applications. A previously developed spiking neural network implementation of sequence prediction and recall learns complex, high-order sequences in an unsupervised manner by local, biologically inspired plasticity rules. An emerging type of hardware that holds promise for efficiently running this type of algorithm is neuromorphic hardware. It emulates the way the brain processes information and maps neurons and synapses directly into a physical substrate. Memristive devices have been identified as potential synaptic elements in neuromorphic hardware. In particular, redox-induced resistive random access memories (ReRAM) devices stand out at many aspects. They permit scalability, are energy efficient and fast, and can implement biological plasticity rules. In this work, we study the feasibility of using ReRAM devices as a replacement of the biological synapses in the sequence learning model. We implement and simulate the model including the ReRAM plasticity using the neural simulator NEST. We investigate the effect of different device properties on the performance characteristics of the sequence learning model, and demonstrate resilience with respect to different on-off ratios, conductance resolutions, device variability, and synaptic failure. </details>
<details>	<summary>注释</summary>	23 pages, 13 Figures </details>
<details>	<summary>邮件日期</summary>	2022年12月01日</details>

# 635、基于可穿戴的人体活动识别的时空脉冲神经网络
- [ ] Wearable-based Human Activity Recognition with Spatio-Temporal Spiking Neural Networks 
时间：2022年11月14日                         第一作者：Yuhang Li                       [链接](https://arxiv.org/abs/2212.02233).                     
## 摘要：我们研究了人类活动识别（HAR）任务，该任务基于可穿戴传感器的时间序列数据预测用户的日常活动。最近，研究人员使用端到端人工神经网络（ANN）在HAR中提取特征并执行分类。然而，ANN对可穿戴设备造成了巨大的计算负担，并且缺乏时间特征提取。在这项工作中，我们利用Spiking Neural Networks（SNNs）——一种受生物神经元启发的架构——来完成HAR任务。SNN允许特征的时空提取，并享受具有二进制峰值的低功耗计算。我们使用SNN在三个HAR数据集上进行了广泛的实验，证明SNN在精度方面与ANN不相上下，同时降低了高达94%的能耗。该代码在https://github.com/Intelligent-Computing-Lab-Yale/SNN_HAR
<details>	<summary>英文摘要</summary>	We study the Human Activity Recognition (HAR) task, which predicts user daily activity based on time series data from wearable sensors. Recently, researchers use end-to-end Artificial Neural Networks (ANNs) to extract the features and perform classification in HAR. However, ANNs pose a huge computation burden on wearable devices and lack temporal feature extraction. In this work, we leverage Spiking Neural Networks (SNNs)--an architecture inspired by biological neurons--to HAR tasks. SNNs allow spatio-temporal extraction of features and enjoy low-power computation with binary spikes. We conduct extensive experiments on three HAR datasets with SNNs, demonstrating that SNNs are on par with ANNs in terms of accuracy while reducing up to 94% energy consumption. The code is publicly available in https://github.com/Intelligent-Computing-Lab-Yale/SNN_HAR </details>
<details>	<summary>注释</summary>	Workshop on Learning from Time Series for Health </details>
<details>	<summary>邮件日期</summary>	2022年12月06日</details>

# 634、基于脉冲神经网络的医学数据分析综述
- [ ] Review of medical data analysis based on spiking neural networks 
时间：2022年11月13日                         第一作者：X. Li (1)                       [链接](https://arxiv.org/abs/2212.02234).                     
## 摘要：医学数据主要包括各种生物医学信号和医学图像，医生可以通过医学数据对患者的身体状况做出判断。然而，医疗数据的解释需要大量的人力成本，并且可能被误判，因此许多学者使用神经网络和深度学习对医疗数据进行分类和研究，从而提高医生的工作效率和准确性，实现疾病的早期发现和早期诊断，因此具有广泛的应用前景。然而，传统的神经网络具有高能耗和高延迟（计算速度慢）等缺点。本文介绍了近年来利用脑电图（EEG）、心电图（ECG）、肌电图（EMG）、磁共振成像（MRI）等医学数据，基于第三代神经网络脉冲神经网络的信号分类和疾病诊断研究，总结了其优缺点
<details>	<summary>英文摘要</summary>	Medical data mainly includes various biomedical signals and medical images, and doctors can make judgments on the physical condition of patients through medical data. However, the interpretation of medical data requires a lot of labor costs and may be misjudged, so many scholars use neural networks and deep learning to classify and study medical data, thereby improving doctors' work efficiency and accuracy, achieving early detection of diseases and early diagnosis, so it has a wide range of application prospects. However, traditional neural networks have disadvantages such as high energy consumption and high latency (slow calculation speed). This paper introduces the research on signal classification and disease diagnosis based on the third-generation neural network - pulse neural network in recent years, using medical data, such as electroencephalogram (EEG), electrocardiogram (ECG), electromyography (EMG), magnetic resonance imaging (MRI), etc., summarizes the advantages and disadvantages of pulse neural networks compared with traditional networks, and looks forward to the future development direction. </details>
<details>	<summary>注释</summary>	in Chinese language </details>
<details>	<summary>邮件日期</summary>	2022年12月06日</details>

# 633、无单脉冲限制的脉冲神经网络中基于时间的反向传播
- [ ] Timing-Based Backpropagation in Spiking Neural Networks Without Single-Spike Restrictions 
时间：2022年11月29日                         第一作者：Kakei Yamamoto                       [链接](https://arxiv.org/abs/2211.16113).                     
## 摘要：我们提出了一种用于训练脉冲神经网络（SNN）的新的反向传播算法，该算法在没有单个脉冲限制的情况下，在单个神经元的相对多个脉冲定时中编码信息。所提出的算法继承了传统的基于定时的方法的优点，因为它计算关于脉冲定时的精确梯度，这促进了理想的时间编码。与每个神经元最多发射一次的传统方法不同，所提出的算法允许每个神经元发射多次。这种扩展自然提高了SNN的计算能力。我们的SNN模型优于可比SNN模型，并实现了与非卷积人工神经网络一样高的精度。我们网络的脉冲计数特性根据突触后电流的时间常数和膜电位而改变。此外，我们发现存在具有最大测试精度的最佳时间常数。这是传统的
<details>	<summary>英文摘要</summary>	We propose a novel backpropagation algorithm for training spiking neural networks (SNNs) that encodes information in the relative multiple spike timing of individual neurons without single-spike restrictions. The proposed algorithm inherits the advantages of conventional timing-based methods in that it computes accurate gradients with respect to spike timing, which promotes ideal temporal coding. Unlike conventional methods where each neuron fires at most once, the proposed algorithm allows each neuron to fire multiple times. This extension naturally improves the computational capacity of SNNs. Our SNN model outperformed comparable SNN models and achieved as high accuracy as non-convolutional artificial neural networks. The spike count property of our networks was altered depending on the time constant of the postsynaptic current and the membrane potential. Moreover, we found that there existed the optimal time constant with the maximum test accuracy. That was not seen in conventional SNNs with single-spike restrictions on time-to-fast-spike (TTFS) coding. This result demonstrates the computational properties of SNNs that biologically encode information into the multi-spike timing of individual neurons. Our code would be publicly available. </details>
<details>	<summary>注释</summary>	10 pages, 5 figures ACM-class: I.5.1 </details>
<details>	<summary>邮件日期</summary>	2022年11月30日</details>

# 632、脉冲神经P系统的矩阵表示：再讨论
- [ ] Matrix representations of spiking neural P systems: Revisited 
时间：2022年11月28日                         第一作者：Henry N. Adorna                       [链接](https://arxiv.org/abs/2211.15156).                     
## 摘要：2010年，提出了无延迟SN P系统的矩阵表示，而在有延迟的SN P系统中，2017年提出了矩阵表示。这些表示使用计算机软件和硬件技术对SN P系统进行了一系列模拟。在这项工作中，我们重新审视了这些表示，并对SNP系统的计算行为提供了一些观察。在有延迟和无延迟的SNP系统中都考虑了配置可达性的概念。在具有延迟的SN P系统的情况下，提出了一种更好的下一配置计算方法。
<details>	<summary>英文摘要</summary>	In the 2010, matrix representation of SN P system without delay was presented while in the case of SN P systems with delay, matrix representation was suggested in the 2017. These representations brought about series of simulation of SN P systems using computer software and hardware technology. In this work, we revisit these representation and provide some observations on the behavior of the computations of SN P systems. The concept of reachability of configuration is considered in both SN P systems with and without delays. A better computation of next configuration is proposed in the case of SN P system with delay. </details>
<details>	<summary>注释</summary>	In: Gheorghe Paun (Ed) Proceedings of the 20th International Conference on Membrane Computing (CMC20), Editura Bibliostar, Ramnicu Valcea (2019) pp 227-247 </details>
<details>	<summary>邮件日期</summary>	2022年11月29日</details>

# 631、Spiking神经网络中的时间信息动力学研究
- [ ] Exploring Temporal Information Dynamics in Spiking Neural Networks 
时间：2022年11月26日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2211.14406).                     
## 摘要：大多数现有的脉冲神经网络（SNN）工作表明，SNN可以利用脉冲的时间信息动态。然而，对时间信息动态的明确分析仍然缺失。在本文中，我们提出了几个重要问题，以提供对SNN的基本理解：SNN内部的时间信息动态是什么？我们如何测量时间信息动态？时间信息动态如何影响整体学习绩效？为了回答这些问题，我们估计权重的Fisher信息，以实证的方式测量训练期间时间信息的分布。令人惊讶的是，随着训练的进行，费舍尔的信息开始集中在早期阶段。在训练之后，我们观察到信息在前几个时间段变得高度集中，这一现象我们称之为时间信息集中。我们观察到，时间信息集中现象是一种常见的现象
<details>	<summary>英文摘要</summary>	Most existing Spiking Neural Network (SNN) works state that SNNs may utilize temporal information dynamics of spikes. However, an explicit analysis of temporal information dynamics is still missing. In this paper, we ask several important questions for providing a fundamental understanding of SNNs: What are temporal information dynamics inside SNNs? How can we measure the temporal information dynamics? How do the temporal information dynamics affect the overall learning performance? To answer these questions, we estimate the Fisher Information of the weights to measure the distribution of temporal information during training in an empirical manner. Surprisingly, as training goes on, Fisher information starts to concentrate in the early timesteps. After training, we observe that information becomes highly concentrated in earlier few timesteps, a phenomenon we refer to as temporal information concentration. We observe that the temporal information concentration phenomenon is a common learning feature of SNNs by conducting extensive experiments on various configurations such as architecture, dataset, optimization strategy, time constant, and timesteps. Furthermore, to reveal how temporal information concentration affects the performance of SNNs, we design a loss function to change the trend of temporal information. We find that temporal information concentration is crucial to building a robust SNN but has little effect on classification accuracy. Finally, we propose an efficient iterative pruning method based on our observation on temporal information concentration. Code is available at https://github.com/Intelligent-Computing-Lab-Yale/Exploring-Temporal-Information-Dynamics-in-Spiking-Neural-Networks. </details>
<details>	<summary>注释</summary>	Accepted to AAAI2023 </details>
<details>	<summary>邮件日期</summary>	2022年11月29日</details>

# 630、组合对抗机器学习的博弈论混合专家
- [ ] Game Theoretic Mixed Experts for Combinational Adversarial Machine Learning 
时间：2022年11月26日                         第一作者：Ethan Rathbun                       [链接](https://arxiv.org/abs/2211.14669).                     
## 摘要：对抗性机器学习的最新进展表明，被认为是强大的防御实际上容易受到针对其弱点而专门定制的对抗性攻击。这些防御措施包括随机变换障碍（BaRT）、友好对抗训练（FAT）、垃圾就是宝藏（TiT）以及由视觉变换器（ViT）、大转移模型和Spiking神经网络（SNN）组成的集成模型。一个自然的问题出现了：如何最好地利用对抗性防御的组合来阻止此类攻击？在本文中，我们提供了一个综合对抗性攻击和防御的博弈论框架，以回答这个问题。除了我们的框架之外，我们还进行了第一次对抗性防御可转移性研究，以进一步激发对利用多种防御体系结构的组合防御的需求。我们的框架被称为博弈论混合专家（Game），旨在找到防御的混合纳什策略
<details>	<summary>英文摘要</summary>	Recent advances in adversarial machine learning have shown that defenses considered to be robust are actually susceptible to adversarial attacks which are specifically tailored to target their weaknesses. These defenses include Barrage of Random Transforms (BaRT), Friendly Adversarial Training (FAT), Trash is Treasure (TiT) and ensemble models made up of Vision Transformers (ViTs), Big Transfer models and Spiking Neural Networks (SNNs). A natural question arises: how can one best leverage a combination of adversarial defenses to thwart such attacks? In this paper, we provide a game-theoretic framework for ensemble adversarial attacks and defenses which answers this question. In addition to our framework we produce the first adversarial defense transferability study to further motivate a need for combinational defenses utilizing a diverse set of defense architectures. Our framework is called Game theoretic Mixed Experts (GaME) and is designed to find the Mixed-Nash strategy for a defender when facing an attacker employing compositional adversarial attacks. We show that this framework creates an ensemble of defenses with greater robustness than multiple state-of-the-art, single-model defenses in addition to combinational defenses with uniform probability distributions. Overall, our framework and analyses advance the field of adversarial machine learning by yielding new insights into compositional attack and defense formulations. </details>
<details>	<summary>注释</summary>	21 pages, 6 figures ACM-class: I.2; I.4 </details>
<details>	<summary>邮件日期</summary>	2022年11月29日</details>

# 629、基于Spiking神经网络的动态图表示学习
- [ ] Scaling Up Dynamic Graph Representation Learning via Spiking Neural Networks 
时间：2022年11月26日                         第一作者：Jintang Li                       [链接](https://arxiv.org/abs/2208.10364).                     
<details>	<summary>注释</summary>	Accepted by AAAI 2023.Contains appendix with additional details about algorithms and experiments. Code available at https://github.com/EdisonLeeeee/SpikeNet </details>
<details>	<summary>邮件日期</summary>	2022年11月29日</details>

# 628、PC-SNN：基于Spiking神经网络预测编码的局部Hebbian突触可塑性监督学习
- [ ] PC-SNN: Supervised Learning with Local Hebbian Synaptic Plasticity based on Predictive Coding in Spiking Neural Networks 
时间：2022年11月24日                         第一作者：Mengting Lan                       [链接](https://arxiv.org/abs/2211.15386).                     
## 摘要：被认为是第三代神经网络，事件驱动的Spiking神经网络（SNN）与生物似本地学习规则相结合，使其有望为SNN构建低功耗的神经形态硬件。然而，由于脉冲神经网络的非线性和离散性，SNN的训练仍然很困难，仍在讨论中。源于梯度下降，backop在多层SNN中取得了惊人的成功。然而，它被认为缺乏生物合理性，同时消耗了相对较高的计算资源。在本文中，我们提出了一种受预测编码理论启发的新的学习算法，并表明它可以完全自主地和成功地执行监督学习，作为反向操作，仅利用局部Hebbian可塑性。此外，与最先进的多层SNN相比，该方法实现了良好的性能：加州理工学院人脸/摩托车数据集的测试精度为99.25%，84.25%
<details>	<summary>英文摘要</summary>	Deemed as the third generation of neural networks, the event-driven Spiking Neural Networks(SNNs) combined with bio-plausible local learning rules make it promising to build low-power, neuromorphic hardware for SNNs. However, because of the non-linearity and discrete property of spiking neural networks, the training of SNN remains difficult and is still under discussion. Originating from gradient descent, backprop has achieved stunning success in multi-layer SNNs. Nevertheless, it is assumed to lack biological plausibility, while consuming relatively high computational resources. In this paper, we propose a novel learning algorithm inspired by predictive coding theory and show that it can perform supervised learning fully autonomously and successfully as the backprop, utilizing only local Hebbian plasticity. Furthermore, this method achieves a favorable performance compared to the state-of-the-art multi-layer SNNs: test accuracy of 99.25% for the Caltech Face/Motorbike dataset, 84.25% for the ETH-80 dataset, 98.1% for the MNIST dataset and 98.5% for the neuromorphic dataset: N-MNIST. Furthermore, our work provides a new perspective on how supervised learning algorithms are directly implemented in spiking neural circuitry, which may give some new insights into neuromorphological calculation in neuroscience. </details>
<details>	<summary>注释</summary>	15 pages, 11figs ACM-class: I.2.3; I.2.10 </details>
<details>	<summary>邮件日期</summary>	2022年11月29日</details>

# 627、基于发育可塑性的深刺自适应修剪和人工神经网络
- [ ] Developmental Plasticity-inspired Adaptive Pruning for Deep Spiking and Artificial Neural Networks 
时间：2022年11月23日                         第一作者：Bing Han                       [链接](https://arxiv.org/abs/2211.12714).                     
## 摘要：发育可塑性在不断学习过程中对大脑结构的塑造起着至关重要的作用，以应对动态变化的环境。然而，现有的深度人工神经网络（ANN）和脉冲神经网络（SNN）的网络压缩方法很少从大脑的发育可塑性机制中获得灵感，因此限制了它们高效、快速和准确地学习的能力。本文提出了一种基于发育可塑性的适应性修剪（DPAP）方法，其灵感来源于树突棘、突触和神经元的适应性发育修剪，遵循“要么使用，要么失去，逐渐衰退”原则。所提出的DPAP模型考虑了多种生物学现实机制（如树突脊柱动态可塑性、活动依赖性神经脉冲轨迹、局部突触可塑性），并添加了自适应修剪策略，从而可以在学习过程中动态优化网络结构
<details>	<summary>英文摘要</summary>	Developmental plasticity plays a vital role in shaping the brain's structure during ongoing learning in response to the dynamically changing environments. However, the existing network compression methods for deep artificial neural networks (ANNs) and spiking neural networks (SNNs) draw little inspiration from the brain's developmental plasticity mechanisms, thus limiting their ability to learn efficiently, rapidly, and accurately. This paper proposed a developmental plasticity-inspired adaptive pruning (DPAP) method, with inspiration from the adaptive developmental pruning of dendritic spines, synapses, and neurons according to the "use it or lose it, gradually decay" principle. The proposed DPAP model considers multiple biologically realistic mechanisms (such as dendritic spine dynamic plasticity, activity-dependent neural spiking trace, local synaptic plasticity), with the addition of an adaptive pruning strategy, so that the network structure can be dynamically optimized during learning without any pre-training and retraining. We demonstrated that the proposed DPAP method applied to deep ANNs and SNNs could learn efficient network architectures that retain only relevant important connections and neurons. Extensive comparative experiments show consistent and remarkable performance and speed boost with the extremely compressed networks on a diverse set of benchmark tasks, especially neuromorphic datasets for SNNs. This work explores how developmental plasticity enables the complex deep networks to gradually evolve into brain-like efficient and compact structures, eventually achieving state-of-the-art (SOTA) performance for biologically realistic SNNs. </details>
<details>	<summary>邮件日期</summary>	2022年11月24日</details>

# 626、MSS深度网：多步Spiking神经网络深度预测
- [ ] MSS-DepthNet: Depth Prediction with Multi-Step Spiking Neural Network 
时间：2022年11月22日                         第一作者：Xiaoshan Wu                       [链接](https://arxiv.org/abs/2211.12156).                     
## 摘要：事件摄像机由于其高时间分辨率和低功耗特性，被认为在计算机视觉和机器人应用中具有巨大潜力。然而，事件摄像机输出的事件流具有现有计算机视觉算法无法处理的异步、稀疏特性。Spiking神经网络是一种新的基于事件的计算范式，被认为非常适合处理事件摄像机任务。然而，深度SNN的直接训练存在退化问题。这项工作通过提出一种脉冲神经网络架构来解决这些问题，该架构设计了新的残差块，并结合了多维注意力模块，重点关注深度预测问题。此外，还提出了一种新的SNN事件流表示方法。该模型在MVSEC数据集上优于相同大小的先前ANN网络，并显示出极大的计算效率。
<details>	<summary>英文摘要</summary>	Event cameras are considered to have great potential for computer vision and robotics applications because of their high temporal resolution and low power consumption characteristics. However, the event stream output from event cameras has asynchronous, sparse characteristics that existing computer vision algorithms cannot handle. Spiking neural network is a novel event-based computational paradigm that is considered to be well suited for processing event camera tasks. However, direct training of deep SNNs suffers from degradation problems. This work addresses these problems by proposing a spiking neural network architecture with a novel residual block designed and multi-dimension attention modules combined, focusing on the problem of depth prediction. In addition, a novel event stream representation method is explicitly proposed for SNNs. This model outperforms previous ANN networks of the same size on the MVSEC dataset and shows great computational efficiency. </details>
<details>	<summary>邮件日期</summary>	2022年11月23日</details>

# 625、基于修剪和再生的Spiking神经网络自适应稀疏结构开发
- [ ] Adaptive Sparse Structure Development with Pruning and Regeneration for Spiking Neural Networks 
时间：2022年11月22日                         第一作者：Bing Han                       [链接](https://arxiv.org/abs/2211.12219).                     
## 摘要：Spiking神经网络（SNN）在生物学上更合理，计算效率更高。因此，SNN具有利用大脑发育的稀疏结构可塑性来缓解深度神经网络因其复杂和固定结构而导致的能量问题的天然优势。然而，以往的SNN压缩工作缺乏来自大脑发育可塑性机制的深入启发。本文提出了一种新的SNN（SD-SNN）自适应结构发展方法，引入了基于树突脊柱可塑性的突触约束、神经元修剪和突触再生。我们发现，突触约束和神经元修剪可以检测和消除SNN中的大量冗余，结合突触再生可以有效地防止和修复过度修剪。此外，受神经营养假说的启发，神经元修剪率和突触再生率在边学习边修剪过程中被自适应地调整，
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) are more biologically plausible and computationally efficient. Therefore, SNNs have the natural advantage of drawing the sparse structural plasticity of brain development to alleviate the energy problems of deep neural networks caused by their complex and fixed structures. However, previous SNNs compression works are lack of in-depth inspiration from the brain development plasticity mechanism. This paper proposed a novel method for the adaptive structural development of SNN (SD-SNN), introducing dendritic spine plasticity-based synaptic constraint, neuronal pruning and synaptic regeneration. We found that synaptic constraint and neuronal pruning can detect and remove a large amount of redundancy in SNNs, coupled with synaptic regeneration can effectively prevent and repair over-pruning. Moreover, inspired by the neurotrophic hypothesis, neuronal pruning rate and synaptic regeneration rate were adaptively adjusted during the learning-while-pruning process, which eventually led to the structural stability of SNNs. Experimental results on spatial (MNIST, CIFAR-10) and temporal neuromorphic (N-MNIST, DVS-Gesture) datasets demonstrate that our method can flexibly learn appropriate compression rate for various tasks and effectively achieve superior performance while massively reducing the network energy consumption. Specifically, for the spatial MNIST dataset, our SD-SNN achieves 99.51\% accuracy at the pruning rate 49.83\%, which has a 0.05\% accuracy improvement compared to the baseline without compression. For the neuromorphic DVS-Gesture dataset, 98.20\% accuracy with 1.09\% improvement is achieved by our method when the compression rate reaches 55.50\%. </details>
<details>	<summary>邮件日期</summary>	2022年11月23日</details>

# 624、Spikformer：当Spiking神经网络遇到变压器时
- [ ] Spikformer: When Spiking Neural Network Meets Transformer 
时间：2022年11月22日                         第一作者：Zhaokun Zhou                       [链接](https://arxiv.org/abs/2209.15425).                     
<details>	<summary>邮件日期</summary>	2022年11月23日</details>

# 623、一种用于深度强化学习的低延迟自适应编码Spiking框架
- [ ] A Low Latency Adaptive Coding Spiking Framework for Deep Reinforcement Learning 
时间：2022年11月21日                         第一作者：Lang Qin                       [链接](https://arxiv.org/abs/2211.11760).                     
## 摘要：在过去几年中，借助于深度神经网络，深度强化学习（DRL）在许多复杂任务上取得了巨大成功。Spiking神经网络（SNN）已被用于在专用神经形态硬件上实现具有超高能量效率的深度神经网络，近年来，SNN与强化学习的结合受到了越来越多的关注，而大多数方法仍然在巨大的能量消耗和高延迟下工作。这项工作提出了基于SNN的DRL的自适应编码Spiking框架（ACSF），同时实现了低延迟和高能效。受生物学中经典条件反射的启发，我们分别用棘波编码器、SNN和棘波解码器模拟受体、中央中间神经元和效应器。我们使用我们提出的ACSF来估计强化学习中的价值函数，并进行广泛的实验来验证我们提出的框架的有效性。
<details>	<summary>英文摘要</summary>	With the help of Deep Neural Networks, Deep Reinforcement Learning (DRL) has achieved great success on many complex tasks during the past few years. Spiking Neural Networks (SNNs) have been used for the implementation of Deep Neural Networks with superb energy efficiency on dedicated neuromorphic hardware, and recent years have witnessed increasing attention on combining SNNs with Reinforcement Learning, whereas most approaches still work with huge energy consumption and high latency. This work proposes the Adaptive Coding Spiking Framework (ACSF) for SNN-based DRL and achieves low latency and great energy efficiency at the same time. Inspired by classical conditioning in biology, we simulate receptors, central interneurons, and effectors with spike encoders, SNNs, and spike decoders, respectively. We use our proposed ACSF to estimate the value function in reinforcement learning and conduct extensive experiments to verify the effectiveness of our proposed framework. </details>
<details>	<summary>邮件日期</summary>	2022年11月23日</details>

# 622、HALSIE——同时利用图像和事件模式的混合学习分割方法
- [ ] HALSIE -- Hybrid Approach to Learning Segmentation by Simultaneously Exploiting Image and Event Modalities 
时间：2022年11月19日                         第一作者：Shristi Das Biswas                       [链接](https://arxiv.org/abs/2211.10754).                     
## 摘要：在自主导航等具有挑战性的实时应用中，标准的基于帧的算法无法检索精确的分割地图，这是由于传统相机中普遍存在的有限动态范围和运动模糊。事件摄像机通过异步检测每像素强度的变化来解决这些限制，以生成具有高时间分辨率、高动态范围和无运动模糊的事件流。然而，事件摄像机输出不能直接用于生成可靠的分割图，因为它们仅在运动像素处捕获信息。为了补充缺失的上下文信息，我们假设将空间密集的帧与时间密集的事件融合可以生成具有细粒度预测的语义图。为此，我们提出了HALSIE，这是一种通过同时利用图像和事件模态来学习分割的混合方法。为了实现跨模态的高效学习，我们提出的混合框架包括两个输入分支，即Sp
<details>	<summary>英文摘要</summary>	Standard frame-based algorithms fail to retrieve accurate segmentation maps in challenging real-time applications like autonomous navigation, owing to the limited dynamic range and motion blur prevalent in traditional cameras. Event cameras address these limitations by asynchronously detecting changes in per-pixel intensity to generate event streams with high temporal resolution, high dynamic range, and no motion blur. However, event camera outputs cannot be directly used to generate reliable segmentation maps as they only capture information at the pixels in motion. To augment the missing contextual information, we postulate that fusing spatially dense frames with temporally dense events can generate semantic maps with fine-grained predictions. To this end, we propose HALSIE, a hybrid approach to learning segmentation by simultaneously leveraging image and event modalities. To enable efficient learning across modalities, our proposed hybrid framework comprises two input branches, a Spiking Neural Network (SNN) branch and a standard Artificial Neural Network (ANN) branch to process event and frame data respectively, while exploiting their corresponding neural dynamics. Our hybrid network outperforms the state-of-the-art semantic segmentation benchmarks on DDD17 and MVSEC datasets and shows comparable performance on the DSEC-Semantic dataset with upto 33.23$\times$ reduction in network parameters. Further, our method shows upto 18.92$\times$ improvement in inference cost compared to existing SOTA approaches, making it suitable for resource-constrained edge applications. </details>
<details>	<summary>邮件日期</summary>	2022年11月22日</details>

# 621、Spikeformer：一种训练高性能低延迟Spiking神经网络的新架构
- [ ] Spikeformer: A Novel Architecture for Training High-Performance Low-Latency Spiking Neural Network 
时间：2022年11月19日                         第一作者：Yudong Li                       [链接](https://arxiv.org/abs/2211.10686).                     
## 摘要：在过去几年中，Spiking神经网络（SNN）在性能和效率方面都取得了巨大进步，但其独特的工作模式使得训练高性能低延迟SNN变得困难。因此，SNN的发展仍然落后于传统的人工神经网络（ANN）。为了弥补这一差距，人们提出了许多非凡的工作。然而，这些工作主要基于相同类型的网络结构（即CNN），并且它们的性能比它们的ANN同行差，这限制了SNN的应用。为此，我们提出了一种新的基于Transformer的SNN，称为“Spikeformer”，它在静态数据集和神经形态数据集上都优于其ANN对应物，可能是CNN训练高性能SNN的替代架构。首先，为了解决香草模型中“数据饥饿”和训练周期不稳定的问题，我们设计了卷积令牌化器（CT）模块，这提高了原始模型的准确性
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have made great progress on both performance and efficiency over the last few years,but their unique working pattern makes it hard to train a high-performance low-latency SNN.Thus the development of SNNs still lags behind traditional artificial neural networks (ANNs).To compensate this gap,many extraordinary works have been proposed.Nevertheless,these works are mainly based on the same kind of network structure (i.e.CNN) and their performance is worse than their ANN counterparts,which limits the applications of SNNs.To this end,we propose a novel Transformer-based SNN,termed "Spikeformer",which outperforms its ANN counterpart on both static dataset and neuromorphic dataset and may be an alternative architecture to CNN for training high-performance SNNs.First,to deal with the problem of "data hungry" and the unstable training period exhibited in the vanilla model,we design the Convolutional Tokenizer (CT) module,which improves the accuracy of the original model on DVS-Gesture by more than 16%.Besides,in order to better incorporate the attention mechanism inside Transformer and the spatio-temporal information inherent to SNN,we adopt spatio-temporal attention (STA) instead of spatial-wise or temporal-wise attention.With our proposed method,we achieve competitive or state-of-the-art (SOTA) SNN performance on DVS-CIFAR10,DVS-Gesture,and ImageNet datasets with the least simulation time steps (i.e.low latency).Remarkably,our Spikeformer outperforms other SNNs on ImageNet by a large margin (i.e.more than 5%) and even outperforms its ANN counterpart by 3.1% and 2.2% on DVS-Gesture and ImageNet respectively,indicating that Spikeformer is a promising architecture for training large-scale SNNs and may be more suitable for SNNs compared to CNN.We believe that this work shall keep the development of SNNs in step with ANNs as much as possible.Code will be available. </details>
<details>	<summary>邮件日期</summary>	2022年11月22日</details>

# 620、用于数据驱动的坐标、控制方程和基本常数发现的贝叶斯自动编码器
- [ ] Bayesian autoencoders for data-driven discovery of coordinates, governing equations and fundamental constants 
时间：2022年11月19日                         第一作者：L. Mars Gao                        [链接](https://arxiv.org/abs/2211.10575).                     
## 摘要：在$\ell_1$约束下基于自动编码器的非线性动力学稀疏识别（SINDy）的最新进展允许从时空数据（包括模拟视频帧）中联合发现控制方程和潜在坐标系统。然而，基于$\ell_1$的稀疏推理很难对真实数据进行正确的识别，因为测量值有噪声，样本大小通常有限。为了解决低数据和高噪声状态下的数据驱动的物理发现，我们提出了贝叶斯SINDy自动编码器，它结合了分层贝叶斯稀疏先验：脉冲和板条高斯拉索。贝叶斯SINDy自动编码器能够在理论上保证不确定性估计的情况下联合发现控制方程和坐标系统。为了解决贝叶斯分层设置的具有挑战性的计算可处理性，我们采用了具有Stochatic梯度Langevin动力学（SGLD）的自适应经验贝叶斯方法
<details>	<summary>英文摘要</summary>	Recent progress in autoencoder-based sparse identification of nonlinear dynamics (SINDy) under $\ell_1$ constraints allows joint discoveries of governing equations and latent coordinate systems from spatio-temporal data, including simulated video frames. However, it is challenging for $\ell_1$-based sparse inference to perform correct identification for real data due to the noisy measurements and often limited sample sizes. To address the data-driven discovery of physics in the low-data and high-noise regimes, we propose Bayesian SINDy autoencoders, which incorporate a hierarchical Bayesian sparsifying prior: Spike-and-slab Gaussian Lasso. Bayesian SINDy autoencoder enables the joint discovery of governing equations and coordinate systems with a theoretically guaranteed uncertainty estimate. To resolve the challenging computational tractability of the Bayesian hierarchical setting, we adapt an adaptive empirical Bayesian method with Stochatic gradient Langevin dynamics (SGLD) which gives a computationally tractable way of Bayesian posterior sampling within our framework. Bayesian SINDy autoencoder achieves better physics discovery with lower data and fewer training epochs, along with valid uncertainty quantification suggested by the experimental studies. The Bayesian SINDy autoencoder can be applied to real video data, with accurate physics discovery which correctly identifies the governing equation and provides a close estimate for standard physics constants like gravity $g$, for example, in videos of a pendulum. </details>
<details>	<summary>注释</summary>	28 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2022年11月22日</details>

# 619、SMS：用于微分方程有效长时间积分的Spiking Marching方案
- [ ] SMS: Spiking Marching Scheme for Efficient Long Time Integration of Differential Equations 
时间：2022年11月17日                         第一作者：Qian Zhang                       [链接](https://arxiv.org/abs/2211.09928).                     
## 摘要：我们提出了一种基于Spiking神经网络（SNN）的显式数值格式，用于时间相关的常微分方程和偏微分方程（ODEs，PDE）的长时间积分。该方法的核心元素是SNN，该SNN被训练为在前一时间步使用关于解决方案的脉冲编码信息来预测下一时间步的脉冲编码的信息。在训练网络之后，它作为一个显式的数值方案运行，在给定脉冲编码初始条件的情况下，可以用于在未来的时间步长计算解。解码器用于将演进的脉冲编码解转换回函数值。我们给出了对不同复杂度的ODE和PDE使用所提出方法的数值实验结果。
<details>	<summary>英文摘要</summary>	We propose a Spiking Neural Network (SNN)-based explicit numerical scheme for long time integration of time-dependent Ordinary and Partial Differential Equations (ODEs, PDEs). The core element of the method is a SNN, trained to use spike-encoded information about the solution at previous timesteps to predict spike-encoded information at the next timestep. After the network has been trained, it operates as an explicit numerical scheme that can be used to compute the solution at future timesteps, given a spike-encoded initial condition. A decoder is used to transform the evolved spiking-encoded solution back to function values. We present results from numerical experiments of using the proposed method for ODEs and PDEs of varying complexity. </details>
<details>	<summary>注释</summary>	14 pages, 7 figures Report-no: PNNL-SA-179601 MSC-class: 65M99 </details>
<details>	<summary>邮件日期</summary>	2022年11月21日</details>

# 618、脉冲神经元泄漏和网络复发对基于事件的时空模式识别的影响
- [ ] Impact of spiking neurons leakages and network recurrences on event-based spatio-temporal pattern recognition 
时间：2022年11月14日                         第一作者：Mohamed Sadek Bouanane                       [链接](https://arxiv.org/abs/2211.07761).                     
## 摘要：Spiking神经网络与神经形态硬件和基于事件的传感器相结合，对边缘的低延迟和低功耗推理越来越感兴趣。然而，文献中提出了具有不同生物合理性水平、不同计算特征和复杂性的多个脉冲神经元模型。因此，需要定义正确的生物学抽象级别，以便在神经形态硬件中获得准确、高效和快速推理的最佳性能。在此背景下，我们探讨了突触和膜泄漏对刺突神经元的影响。我们使用基于事件的视觉和听觉模式识别的前馈和递归拓扑来对抗具有不同计算复杂性的三个神经模型。我们的结果表明，就准确性而言，当数据中同时存在时间信息和网络中的显式重复时，泄漏是重要的。此外，无需泄漏
<details>	<summary>英文摘要</summary>	Spiking neural networks coupled with neuromorphic hardware and event-based sensors are getting increased interest for low-latency and low-power inference at the edge. However, multiple spiking neuron models have been proposed in the literature with different levels of biological plausibility and different computational features and complexities. Consequently, there is a need to define the right level of abstraction from biology in order to get the best performance in accurate, efficient and fast inference in neuromorphic hardware. In this context, we explore the impact of synaptic and membrane leakages in spiking neurons. We confront three neural models with different computational complexities using feedforward and recurrent topologies for event-based visual and auditory pattern recognition. Our results show that, in terms of accuracy, leakages are important when there are both temporal information in the data and explicit recurrence in the network. In addition, leakages do not necessarily increase the sparsity of spikes flowing in the network. We also investigate the impact of heterogeneity in the time constant of leakages, and the results show a slight improvement in accuracy when using data with a rich temporal structure. These results advance our understanding of the computational role of the neural leakages and network recurrences, and provide valuable insights for the design of compact and energy-efficient neuromorphic hardware for embedded systems. </details>
<details>	<summary>邮件日期</summary>	2022年11月16日</details>

# 617、鸡尾酒会效应和McGurk效应的Motif拓扑改进Spiking神经网络
- [ ] Motif-topology improved Spiking Neural Network for the Cocktail Party Effect and McGurk Effect 
时间：2022年11月12日                         第一作者：Shuncheng Jia                        [链接](https://arxiv.org/abs/2211.07641).                     
## 摘要：在人工神经网络（ANN）和脉冲神经网络（SNN）中，网络结构和学习原理是形成复杂函数的关键。SNN被认为是新一代人工网络，它结合了比ANN更多的生物学特征，包括动态脉冲神经元、功能特定的结构和高效的学习范式。网络架构也被认为体现了网络的功能。在这里，我们提出了一种Motif拓扑改进的SNN（M-SNN），用于有效的多感官整合和认知现象模拟。我们模拟的认知现象模拟包括鸡尾酒会效应和麦格克效应，这是许多研究人员讨论的。我们的M-SNN由称为网络基元的元运算符构成。从空间或时间数据集中预先学习的来自人工网络的3节点网络图案拓扑的来源。在单感官分类任务中，结果显示了准确性
<details>	<summary>英文摘要</summary>	Network architectures and learning principles are playing key in forming complex functions in artificial neural networks (ANNs) and spiking neural networks (SNNs). SNNs are considered the new-generation artificial networks by incorporating more biological features than ANNs, including dynamic spiking neurons, functionally specified architectures, and efficient learning paradigms. Network architectures are also considered embodying the function of the network. Here, we propose a Motif-topology improved SNN (M-SNN) for the efficient multi-sensory integration and cognitive phenomenon simulations. The cognitive phenomenon simulation we simulated includes the cocktail party effect and McGurk effect, which are discussed by many researchers. Our M-SNN constituted by the meta operator called network motifs. The source of 3-node network motifs topology from artificial one pre-learned from the spatial or temporal dataset. In the single-sensory classification task, the results showed the accuracy of M-SNN using network motif topologies was higher than the pure feedforward network topology without using them. In the multi-sensory integration task, the performance of M-SNN using artificial network motif was better than the state-of-the-art SNN using BRP (biologically-plausible reward propagation). Furthermore, the M-SNN could better simulate the cocktail party effect and McGurk effect with lower computational cost. We think the artificial network motifs could be considered as some prior knowledge that would contribute to the multi-sensory integration of SNNs and provide more benefits for simulating the cognitive phenomenon. </details>
<details>	<summary>邮件日期</summary>	2022年11月16日</details>

# 616、脉冲神经网络中通过传播延迟的局部学习
- [ ] Local learning through propagation delays in spiking neural networks 
时间：2022年10月27日                         第一作者：J{\o}rgen Jensen Farner                       [链接](https://arxiv.org/abs/2211.08397).                     
## 摘要：我们为脉冲神经网络提出了一种新的局部学习规则，其中脉冲传播时间经历活动依赖性可塑性。我们的可塑性规则调整了突触前脉冲时间，以产生更强、更快的反应。输入通过延迟编码进行编码，输出通过匹配输出脉冲活动的类似模式进行解码。我们演示了该方法在三层前馈网络中的使用，该网络具有来自手写数字数据库的输入。网络在训练后不断提高其分类精度，使用这种方法的训练也允许网络推广到训练期间不可见的输入类。我们提出的方法利用了刺突神经元支持许多不同时间锁定的刺突序列的能力，每个刺突序列都可以由不同的输入激活激活。这里展示的概念证明证明了局部延迟学习在扩展spi的记忆容量和可推广性方面的巨大潜力
<details>	<summary>英文摘要</summary>	We propose a novel local learning rule for spiking neural networks in which spike propagation times undergo activity-dependent plasticity. Our plasticity rule aligns pre-synaptic spike times to produce a stronger and more rapid response. Inputs are encoded by latency coding and outputs decoded by matching similar patterns of output spiking activity. We demonstrate the use of this method in a three-layer feedfoward network with inputs from a database of handwritten digits. Networks consistently improve their classification accuracy after training, and training with this method also allowed networks to generalize to an input class unseen during training. Our proposed method takes advantage of the ability of spiking neurons to support many different time-locked sequences of spikes, each of which can be activated by different input activations. The proof-of-concept shown here demonstrates the great potential for local delay learning to expand the memory capacity and generalizability of spiking neural networks. </details>
<details>	<summary>注释</summary>	4 pages, 4 figures; longer version under preparation </details>
<details>	<summary>邮件日期</summary>	2022年11月16日</details>

# 615、人工神经网络模型到速率编码脉冲神经网络的低延迟转换
- [ ] Low Latency Conversion of Artificial Neural Network Models to Rate-encoded Spiking Neural Networks 
时间：2022年10月27日                         第一作者：Zhanglu Yan                       [链接](https://arxiv.org/abs/2211.08410).                     
## 摘要：Spiking神经网络（SNN）非常适合资源受限的应用，因为它们不需要昂贵的乘数。在典型的速率编码SNN中，在全局固定时间窗内的一系列二进制脉冲用于激发神经元。该时间窗口中的最大峰值数也是网络执行单个推断的延迟，并决定了模型的总体能效。本文的目的是在将ANN转换为其等效SNN时，在保持精度的同时减少这一点。最先进的转换方案产生了SNN，其精度仅与大窗口尺寸的ANN相当。在本文中，我们从理解从预先存在的ANN模型转换为标准速率编码SNN模型时的信息损失开始。根据这些见解，我们提出了一套新颖的技术，这些技术共同减轻了转换过程中的信息损失，并实现了最先进的SNN精度以及非常低的延迟
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are well suited for resource-constrained applications as they do not need expensive multipliers. In a typical rate-encoded SNN, a series of binary spikes within a globally fixed time window is used to fire the neurons. The maximum number of spikes in this time window is also the latency of the network in performing a single inference, as well as determines the overall energy efficiency of the model. The aim of this paper is to reduce this while maintaining accuracy when converting ANNs to their equivalent SNNs. The state-of-the-art conversion schemes yield SNNs with accuracies comparable with ANNs only for large window sizes. In this paper, we start with understanding the information loss when converting from pre-existing ANN models to standard rate-encoded SNN models. From these insights, we propose a suite of novel techniques that together mitigate the information lost in the conversion, and achieve state-of-art SNN accuracies along with very low latency. Our method achieved a Top-1 SNN accuracy of 98.73% (1 time step) on the MNIST dataset, 76.38% (8 time steps) on the CIFAR-100 dataset, and 93.71% (8 time steps) on the CIFAR-10 dataset. On ImageNet, an SNN accuracy of 75.35%/79.16% was achieved with 100/200 time steps. </details>
<details>	<summary>邮件日期</summary>	2022年11月16日</details>

# 614、使用Synthesizer Transformer模型预测鲸鱼交易和CryptoQuant数据的比特币波动率峰值
- [ ] Forecasting Bitcoin volatility spikes from whale transactions and CryptoQuant data using Synthesizer Transformer models 
时间：2022年10月06日                         第一作者：Dorien Herremans                       [链接](https://arxiv.org/abs/2211.08281).                     
## 摘要：与传统金融市场相比，加密货币市场波动性很大。因此，预测其波动性对于风险管理至关重要。在本文中，我们调查了CryptoQuant数据（例如链上分析、交易所和矿工数据）和鲸鱼警报推文，并探讨了它们与比特币次日波动率的关系，重点关注极端波动率峰值。我们提出了一种用于预测波动性的深度学习合成器-变压器模型。我们的结果表明，在使用CryptoQuant数据和鲸鱼警报推文预测比特币的极端波动峰值时，该模型优于现有的最先进模型。我们使用Captum XAI库分析了我们的模型，以研究哪些特性最重要。我们还用不同的基准交易策略对我们的预测结果进行了后验，结果表明，我们能够在保持稳定利润的同时最大限度地减少亏损。我们的研究结果强调，所提出的方法是一个有用的工具
<details>	<summary>英文摘要</summary>	The cryptocurrency market is highly volatile compared to traditional financial markets. Hence, forecasting its volatility is crucial for risk management. In this paper, we investigate CryptoQuant data (e.g. on-chain analytics, exchange and miner data) and whale-alert tweets, and explore their relationship to Bitcoin's next-day volatility, with a focus on extreme volatility spikes. We propose a deep learning Synthesizer Transformer model for forecasting volatility. Our results show that the model outperforms existing state-of-the-art models when forecasting extreme volatility spikes for Bitcoin using CryptoQuant data as well as whale-alert tweets. We analysed our model with the Captum XAI library to investigate which features are most important. We also backtested our prediction results with different baseline trading strategies and the results show that we are able to minimize drawdown while keeping steady profits. Our findings underscore that the proposed method is a useful tool for forecasting extreme volatility movements in the Bitcoin market. </details>
<details>	<summary>注释</summary>	Co-first authors </details>
<details>	<summary>邮件日期</summary>	2022年11月16日</details>

# 613、大脑激发的神经元沉默机制，实现可靠的序列识别
- [ ] Brain inspired neuronal silencing mechanism to enable reliable sequence identification 
时间：2022年03月24日                         第一作者：Shiri Hodassman                       [链接](https://arxiv.org/abs/2203.13028).                     
## 摘要：实时序列识别是人工神经网络（ANN）的核心用例，从识别时间事件到识别验证码。现有的方法应用了递归神经网络，其存在训练困难；然而，在没有反馈回路的情况下执行该功能仍然是一个挑战。在这里，我们提出了一种无反馈回路的高精度前馈序列识别网络（ID网络）的实验神经元长期可塑性机制，其中输入对象具有给定的顺序和时间。这种机制使神经元在最近的脉冲活动后暂时沉默。因此，暂态对象作用于不同的动态创建的前馈子网络。ID网络被证明能够可靠地识别10个手写数字序列，并被推广到具有在图像序列上训练的连续激活节点的深度卷积ANN。与直觉相反的是，它们的分类性能，即使是有限的努
<details>	<summary>英文摘要</summary>	Real-time sequence identification is a core use-case of artificial neural networks (ANNs), ranging from recognizing temporal events to identifying verification codes. Existing methods apply recurrent neural networks, which suffer from training difficulties; however, performing this function without feedback loops remains a challenge. Here, we present an experimental neuronal long-term plasticity mechanism for high-precision feedforward sequence identification networks (ID-nets) without feedback loops, wherein input objects have a given order and timing. This mechanism temporarily silences neurons following their recent spiking activity. Therefore, transitory objects act on different dynamically created feedforward sub-networks. ID-nets are demonstrated to reliably identify 10 handwritten digit sequences, and are generalized to deep convolutional ANNs with continuous activation nodes trained on image sequences. Counterintuitively, their classification performance, even with a limited number of training examples, is high for sequences but low for individual objects. ID-nets are also implemented for writer-dependent recognition, and suggested as a cryptographic tool for encrypted authentication. The presented mechanism opens new horizons for advanced ANN algorithms. </details>
<details>	<summary>注释</summary>	38 pages, 11 figures Journal-ref: Sci Rep 12, 16003 (2022) DOI: 10.1038/s41598-022-20337-x </details>
<details>	<summary>邮件日期</summary>	2022年11月16日</details>

# 612、GLIF：一种用于脉冲神经网络的统一门控泄漏集成和火灾神经元
- [ ] GLIF: A Unified Gated Leaky Integrate-and-Fire Neuron for Spiking Neural Networks 
时间：2022年11月13日                         第一作者：Xingting Yao                       [链接](https://arxiv.org/abs/2210.13768).                     
<details>	<summary>注释</summary>	Accepted at NeurIPS 2022 Spotlight </details>
<details>	<summary>邮件日期</summary>	2022年11月15日</details>

# 611、NeuroHSMD：神经形态混合脉冲运动检测器
- [ ] NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector 
时间：2022年11月12日                         第一作者：Pedro Machado                       [链接](https://arxiv.org/abs/2112.06102).                     
<details>	<summary>邮件日期</summary>	2022年11月15日</details>

# 610、SNN和ANN之舞：结合脉冲计时和重建注意力解决绑定问题
- [ ] Dance of SNN and ANN: Solving binding problem by combining spike timing and reconstructive attention 
时间：2022年11月11日                         第一作者：Hao Zheng                       [链接](https://arxiv.org/abs/2211.06027).                     
## 摘要：绑定问题是阻碍人工神经网络（ANN）对人类感知世界的合成理解的根本挑战之一，因为当呈现具有多个对象的复杂数据时，生成因子的非纠缠和分布式表示可能会干扰并导致模糊。在本文中，我们提出了一种大脑启发的混合神经网络（HNN），该网络通过将脉冲计时动力学（通过脉冲神经网络，SNN）与重建注意力（通过ANN）相结合，将源自神经科学的时间绑定理论引入到ANN中。脉冲定时为分组提供了额外的维度，而重建反馈将脉冲协调为时间相干状态。通过ANN和SNN的迭代交互，该模型在SNN编码空间中以交替的同步触发时间连续绑定多个对象。在二值图像的合成数据集上评估了模型的有效性。通过visu
<details>	<summary>英文摘要</summary>	The binding problem is one of the fundamental challenges that prevent the artificial neural network (ANNs) from a compositional understanding of the world like human perception, because disentangled and distributed representations of generative factors can interfere and lead to ambiguity when complex data with multiple objects are presented. In this paper, we propose a brain-inspired hybrid neural network (HNN) that introduces temporal binding theory originated from neuroscience into ANNs by integrating spike timing dynamics (via spiking neural networks, SNNs) with reconstructive attention (by ANNs). Spike timing provides an additional dimension for grouping, while reconstructive feedback coordinates the spikes into temporal coherent states. Through iterative interaction of ANN and SNN, the model continuously binds multiple objects at alternative synchronous firing times in the SNN coding space. The effectiveness of the model is evaluated on synthetic datasets of binary images. By visualization and analysis, we demonstrate that the binding is explainable, soft, flexible, and hierarchical. Notably, the model is trained on single object datasets without explicit supervision on grouping, but successfully binds multiple objects on test datasets, showing its compositional generalization capability. Further results show its binding ability in dynamic situations. </details>
<details>	<summary>邮件日期</summary>	2022年11月14日</details>

# 609、通过时间前向传播精确在线训练动态脉冲神经网络
- [ ] Accurate online training of dynamical spiking neural networks through Forward Propagation Through Time 
时间：2022年11月11日                         第一作者：Bojian Yin                       [链接](https://arxiv.org/abs/2112.11231).                     
<details>	<summary>注释</summary>	12 pages, 4 figures </details>
<details>	<summary>邮件日期</summary>	2022年11月14日</details>

# 608、脉冲神经网络决策反馈均衡
- [ ] Spiking Neural Network Decision Feedback Equalization 
时间：2022年11月11日                         第一作者：Eike-Manuel Bansbach                       [链接](https://arxiv.org/abs/2211.04756).                     
<details>	<summary>注释</summary>	Submitted to SCC 2023 </details>
<details>	<summary>邮件日期</summary>	2022年11月14日</details>

# 607、脉冲神经网络决策反馈均衡
- [ ] Spiking Neural Network Decision Feedback Equalization 
时间：2022年11月09日                         第一作者：Eike-Manuel Bansbach                       [链接](https://arxiv.org/abs/2211.04756).                     
## 摘要：在过去的几年中，人工神经网络（ANN）已经成为解决通信工程中难以用传统方法解决的任务的事实标准。与此同时，人工智能社区将其研究转向了生物学启发的、类似大脑的脉冲神经网络（SNNs），这一网络有望实现极其节能的计算。在本文中，我们研究了SNN在超低复杂度接收机信道均衡中的应用。我们提出了一种基于SNN的均衡器，其反馈结构类似于决策反馈均衡器（DFE）。为了将真实世界数据转换为脉冲信号，我们引入了一种新的三值编码，并将其与传统的对数尺度编码进行了比较。我们表明，对于三个不同的示例性信道，我们的方法明显优于传统的线性均衡器。我们强调，主要是将信道输出转换为脉冲会带来较小的性能损失。建议的SNN具有dec
<details>	<summary>英文摘要</summary>	In the past years, artificial neural networks (ANNs) have become the de-facto standard to solve tasks in communications engineering that are difficult to solve with traditional methods. In parallel, the artificial intelligence community drives its research to biology-inspired, brain-like spiking neural networks (SNNs), which promise extremely energy-efficient computing. In this paper, we investigate the use of SNNs in the context of channel equalization for ultra-low complexity receivers. We propose an SNN-based equalizer with a feedback structure akin to the decision feedback equalizer (DFE). For conversion of real-world data into spike signals we introduce a novel ternary encoding and compare it with traditional log-scale encoding. We show that our approach clearly outperforms conventional linear equalizers for three different exemplary channels. We highlight that mainly the conversion of the channel output to spikes introduces a small performance penalty. The proposed SNN with a decision feedback structure enables the path to competitive energy-efficient transceivers. </details>
<details>	<summary>注释</summary>	Submitted to SCC 2023 </details>
<details>	<summary>邮件日期</summary>	2022年11月10日</details>

# 606、用于图像稀疏表示和动态视觉传感器数据压缩的脉冲采样网络
- [ ] Spiking sampling network for image sparse representation and dynamic vision sensor data compression 
时间：2022年11月08日                         第一作者：Chunming Jiang                       [链接](https://arxiv.org/abs/2211.04166).                     
## 摘要：稀疏表示已经引起了极大的关注，因为它可以极大地节省存储资源，并在低维空间中找到数据的代表性特征。因此，它可以广泛应用于工程领域，包括特征提取、压缩感知、信号去噪、图像聚类和字典学习，仅举几个例子。在本文中，我们提出了一种脉冲采样网络。这个网络由脉冲神经元组成，它可以根据输入动态决定哪些像素点应该保留，哪些像素点需要掩蔽。我们的实验表明，与随机采样相比，该方法能够更好地稀疏表示原始图像，并有助于图像重建。因此，我们使用这种方法来压缩来自动态视觉传感器的海量数据，这大大降低了对事件数据的存储要求。
<details>	<summary>英文摘要</summary>	Sparse representation has attracted great attention because it can greatly save storage re- sources and find representative features of data in a low-dimensional space. As a result, it may be widely applied in engineering domains including feature extraction, compressed sensing, signal denoising, picture clustering, and dictionary learning, just to name a few. In this paper, we propose a spiking sampling network. This network is composed of spiking neurons, and it can dynamically decide which pixel points should be retained and which ones need to be masked according to the input. Our experiments demonstrate that this approach enables better sparse representation of the original image and facilitates image reconstruction compared to random sampling. We thus use this approach for compressing massive data from the dynamic vision sensor, which greatly reduces the storage requirements for event data. </details>
<details>	<summary>邮件日期</summary>	2022年11月09日</details>

# 605、用于时空分类的异质递归脉冲神经网络
- [ ] Heterogeneous Recurrent Spiking Neural Network for Spatio-Temporal Classification 
时间：2022年09月22日                         第一作者：Biswadeep Chakraborty                        [链接](https://arxiv.org/abs/2211.04297).                     
## 摘要：脉冲神经网络经常被吹捧为第三波人工智能的大脑启发学习模型。尽管最近用监督反向传播训练的SNN显示出与深度网络相当的分类精度，但基于无监督学习的SNN的性能仍然低得多。本文提出了一种具有无监督学习的异构递归脉冲神经网络（HRSNN），用于基于RGB（KTH，UCF11，UCF101）和基于事件的数据集（DVS128手势）的视频活动识别任务的时空分类。HRSNN的关键新颖之处在于，HRSNN中的重复层由具有不同放电/弛豫动力学的异质神经元组成，并且通过具有不同学习动力学的异质脉冲时间依赖性可塑性（STDP）对每个突触进行训练。我们表明，这种结构和学习方法异质性的新组合优于当前的同质脉冲神经网络。我们进一步表明，HRSNN
<details>	<summary>英文摘要</summary>	Spiking Neural Networks are often touted as brain-inspired learning models for the third wave of Artificial Intelligence. Although recent SNNs trained with supervised backpropagation show classification accuracy comparable to deep networks, the performance of unsupervised learning-based SNNs remains much lower. This paper presents a heterogeneous recurrent spiking neural network (HRSNN) with unsupervised learning for spatio-temporal classification of video activity recognition tasks on RGB (KTH, UCF11, UCF101) and event-based datasets (DVS128 Gesture). The key novelty of the HRSNN is that the recurrent layer in HRSNN consists of heterogeneous neurons with varying firing/relaxation dynamics, and they are trained via heterogeneous spike-time-dependent-plasticity (STDP) with varying learning dynamics for each synapse. We show that this novel combination of heterogeneity in architecture and learning method outperforms current homogeneous spiking neural networks. We further show that HRSNN can achieve similar performance to state-of-the-art backpropagation trained supervised SNN, but with less computation (fewer neurons and sparse connection) and less training data. </details>
<details>	<summary>注释</summary>	32 pages, 11 Figures, 4 Tables. arXiv admin note: text overlap with arXiv:1511.03198 by other authors </details>
<details>	<summary>邮件日期</summary>	2022年11月09日</details>

# 604、基于脉冲的局部突触可塑性：计算模型和神经形态回路综述
- [ ] Spike-based local synaptic plasticity: A survey of computational models and neuromorphic circuits 
时间：2022年11月05日                         第一作者：Lyes Khacef                       [链接](https://arxiv.org/abs/2209.15536).                     
<details>	<summary>邮件日期</summary>	2022年11月08日</details>

# 603、基于神经振荡的梯度掩蔽对抗防御
- [ ] Adversarial Defense via Neural Oscillation inspired Gradient Masking 
时间：2022年11月04日                         第一作者：Chunming Jiang                       [链接](https://arxiv.org/abs/2211.02223).                     
## 摘要：脉冲神经网络（SNN）由于其低功耗、低延迟和生物合理性而备受关注。随着它们被广泛部署在用于低功耗脑启发计算的神经形态设备中，安全问题变得越来越重要。然而，与深度神经网络（DNN）相比，SNN目前缺乏针对对抗性攻击的专门设计的防御方法。受神经膜电位振荡的启发，我们提出了一种新的神经模型，该模型结合了生物激励振荡机制，以增强SNN的安全性。我们的实验表明，在各种架构和数据集上，具有神经振荡神经元的SNN比具有LIF神经元的普通SNN具有更好的对抗性攻击能力。此外，我们提出了一种防御方法，通过替换振荡的形式来改变模型的梯度，这种方法隐藏了原始的训练梯度，并使攻击者迷惑于使用“假”神经元的梯度来生成
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) attract great attention due to their low power consumption, low latency, and biological plausibility. As they are widely deployed in neuromorphic devices for low-power brain-inspired computing, security issues become increasingly important. However, compared to deep neural networks (DNNs), SNNs currently lack specifically designed defense methods against adversarial attacks. Inspired by neural membrane potential oscillation, we propose a novel neural model that incorporates the bio-inspired oscillation mechanism to enhance the security of SNNs. Our experiments show that SNNs with neural oscillation neurons have better resistance to adversarial attacks than ordinary SNNs with LIF neurons on kinds of architectures and datasets. Furthermore, we propose a defense method that changes model's gradients by replacing the form of oscillation, which hides the original training gradients and confuses the attacker into using gradients of 'fake' neurons to generate invalid adversarial samples. Our experiments suggest that the proposed defense method can effectively resist both single-step and iterative attacks with comparable defense effectiveness and much less computational costs than adversarial training methods on DNNs. To the best of our knowledge, this is the first work that establishes adversarial defense through masking surrogate gradients on SNNs. </details>
<details>	<summary>邮件日期</summary>	2022年11月07日</details>

# 602、基于铁电隧道结的集成和火灾神经元
- [ ] A Ferroelectric Tunnel Junction-based Integrate-and-Fire Neuron 
时间：2022年11月04日                         第一作者：Paolo Gibertini                       [链接](https://arxiv.org/abs/2211.02598).                     
## 摘要：基于事件的神经形态系统通过使用人工神经元和突触以脉冲形式异步处理数据，提供了一种低功耗的解决方案。铁电隧道结（FTJ）是超低功耗存储器件，非常适合集成在这些系统中。在这里，我们提出了一种混合FTJ-CMOS集成和Fire神经元，它构成了用于边缘计算的新一代神经形态网络的基本构建块。我们演示了通过调谐FTJ装置的开关可实现的电可调谐神经动力学。
<details>	<summary>英文摘要</summary>	Event-based neuromorphic systems provide a low-power solution by using artificial neurons and synapses to process data asynchronously in the form of spikes. Ferroelectric Tunnel Junctions (FTJs) are ultra low-power memory devices and are well-suited to be integrated in these systems. Here, we present a hybrid FTJ-CMOS Integrate-and-Fire neuron which constitutes a fundamental building block for new-generation neuromorphic networks for edge computing. We demonstrate electrically tunable neural dynamics achievable by tuning the switching of the FTJ device. </details>
<details>	<summary>邮件日期</summary>	2022年11月07日</details>

# 601、用于脉冲神经网络的无ADC内存计算硬件的硬件/软件协同设计
- [ ] Hardware/Software co-design with ADC-Less In-memory Computing Hardware for Spiking Neural Networks 
时间：2022年11月03日                         第一作者：Marco Paul E. Apolinario                       [链接](https://arxiv.org/abs/2211.02167).                     
## 摘要：脉冲神经网络（SNN）是一种生物可信的模型，在资源受限的边缘设备上实现顺序任务的节能实现具有巨大潜力。然而，基于标准GPU的商业边缘平台没有优化以部署SNN，导致高能量和延迟。虽然模拟内存计算（IMC）平台可以作为节能推理引擎，但它们受到高精度ADC（HP-ADC）巨大的能量、延迟和面积要求的困扰，从而掩盖了内存计算的好处。我们提出了一种硬件/软件协同设计方法，以将SNN部署到无ADC IMC架构中，使用感测放大器作为1位ADC来取代传统的HP ADC并缓解上述问题。我们提出的框架通过执行硬件感知训练而导致最小的精度下降，并且能够从简单的图像分类任务扩展到更复杂的序列回归任务。com上的实验
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) are bio-plausible models that hold great potential for realizing energy-efficient implementations of sequential tasks on resource-constrained edge devices. However, commercial edge platforms based on standard GPUs are not optimized to deploy SNNs, resulting in high energy and latency. While analog In-Memory Computing (IMC) platforms can serve as energy-efficient inference engines, they are accursed by the immense energy, latency, and area requirements of high-precision ADCs (HP-ADC), overshadowing the benefits of in-memory computations. We propose a hardware/software co-design methodology to deploy SNNs into an ADC-Less IMC architecture using sense-amplifiers as 1-bit ADCs replacing conventional HP-ADCs and alleviating the above issues. Our proposed framework incurs minimal accuracy degradation by performing hardware-aware training and is able to scale beyond simple image classification tasks to more complex sequential regression tasks. Experiments on complex tasks of optical flow estimation and gesture recognition show that progressively increasing the hardware awareness during SNN training allows the model to adapt and learn the errors due to the non-idealities associated with ADC-Less IMC. Also, the proposed ADC-Less IMC offers significant energy and latency improvements, $2-7\times$ and $8.9-24.6\times$, respectively, depending on the SNN model and the workload, compared to HP-ADC IMC. </details>
<details>	<summary>注释</summary>	12 pages, 13 figures </details>
<details>	<summary>邮件日期</summary>	2022年11月07日</details>

# 600、StereoSpike：利用Spiking神经网络进行深度学习
- [ ] StereoSpike: Depth Learning with a Spiking Neural Network 
时间：2022年11月03日                         第一作者：Ulysse Ran\c{c}on                       [链接](https://arxiv.org/abs/2109.13751).                     
<details>	<summary>邮件日期</summary>	2022年11月04日</details>

# 599、GLIF：一种用于脉冲神经网络的统一门控泄漏集成和火灾神经元
- [ ] GLIF: A Unified Gated Leaky Integrate-and-Fire Neuron for Spiking Neural Networks 
时间：2022年11月03日                         第一作者：Xingting Yao                       [链接](https://arxiv.org/abs/2210.13768).                     
<details>	<summary>注释</summary>	Accepted at NeurIPS 2022 </details>
<details>	<summary>邮件日期</summary>	2022年11月04日</details>

# 598、用于高效图形表示学习的脉冲变分图自动编码器
- [ ] Spiking Variational Graph Auto-Encoders for Efficient Graph Representation Learning 
时间：2022年10月24日                         第一作者：Hanxuan Yang                       [链接](https://arxiv.org/abs/2211.01952).                     
## 摘要：图表示学习是一个基本的研究问题，有利于图结构数据的广泛应用。传统的基于人工神经网络的方法，如图神经网络（GNNs）和变分图自动编码器（VGAEs）在图上学习方面取得了很好的结果，但它们在训练和推理阶段的能耗极高。受脉冲神经网络（SNN）的生物保真度和能量效率的启发，最近的方法试图通过用脉冲神经元代替激活功能，使GNN适应SNN框架。然而，现有的基于SNN的GNN方法不能应用于由链路预测表示的更一般的多节点表示学习问题。此外，这些方法没有充分利用SNN的生物保真度，因为它们仍然需要昂贵的乘法累加（MAC）操作，这严重损害了能源效率。解决上述问题并改善能源
<details>	<summary>英文摘要</summary>	Graph representation learning is a fundamental research issue and benefits a wide range of applications on graph-structured data. Conventional artificial neural network-based methods such as graph neural networks (GNNs) and variational graph auto-encoders (VGAEs) have achieved promising results in learning on graphs, but they suffer from extremely high energy consumption during training and inference stages. Inspired by the bio-fidelity and energy-efficiency of spiking neural networks (SNNs), recent methods attempt to adapt GNNs to the SNN framework by substituting spiking neurons for the activation functions. However, existing SNN-based GNN methods cannot be applied to the more general multi-node representation learning problem represented by link prediction. Moreover, these methods did not fully exploit the bio-fidelity of SNNs, as they still require costly multiply-accumulate (MAC) operations, which severely harm the energy efficiency. To address the above issues and improve energy efficiency, in this paper, we propose an SNN-based deep generative method, namely the Spiking Variational Graph Auto-Encoders (S-VGAE) for efficient graph representation learning. To deal with the multi-node problem, we propose a probabilistic decoder that generates binary latent variables as spiking node representations and reconstructs graphs via the weighted inner product. To avoid the MAC operations for energy efficiency, we further decouple the propagation and transformation layers of conventional GNN aggregators. We conduct link prediction experiments on multiple benchmark graph datasets, and the results demonstrate that our model consumes significantly lower energy with the performances superior or comparable to other ANN- and SNN-based methods for graph representation learning. </details>
<details>	<summary>邮件日期</summary>	2022年11月04日</details>

# 597、基于脉冲神经网络的贝叶斯连续学习
- [ ] Bayesian Continual Learning via Spiking Neural Networks 
时间：2022年11月01日                         第一作者：Nicolas Skatchkovsky                       [链接](https://arxiv.org/abs/2208.13723).                     
<details>	<summary>注释</summary>	Accepted for publication in Frontiers in Computational Neuroscience </details>
<details>	<summary>邮件日期</summary>	2022年11月02日</details>

# 596、一种快速提取深度卷积神经网络的方法
- [ ] A Faster Approach to Spiking Deep Convolutional Neural Networks 
时间：2022年10月31日                         第一作者：Shahriar Rezghi Shirsavar (University of Tehran                       [链接](https://arxiv.org/abs/2210.17442).                     
## 摘要：脉冲神经网络（SNN）比当前的深度神经网络更接近大脑的动态。它们的低功耗和采样效率使这些网络变得有趣。最近，已经提出了几种深度卷积脉冲神经网络。这些网络旨在提高生物合理性，同时创建用于机器学习任务的强大工具。在这里，我们建议基于先前工作的网络结构，以提高网络运行时间和准确性。对网络的改进包括将训练迭代减少到一次，有效地使用主成分分析（PCA）降维、权重量化、分类的定时输出以及更好的超参数调整。此外，改变预处理步骤以允许处理彩色图像而不是仅处理黑白图像，以提高精度。所提出的结构将运行时细分，并引入了深度卷积SNN的有效方法。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have closer dynamics to the brain than current deep neural networks. Their low power consumption and sample efficiency make these networks interesting. Recently, several deep convolutional spiking neural networks have been proposed. These networks aim to increase biological plausibility while creating powerful tools to be applied to machine learning tasks. Here, we suggest a network structure based on previous work to improve network runtime and accuracy. Improvements to the network include reducing training iterations to only once, effectively using principal component analysis (PCA) dimension reduction, weight quantization, timed outputs for classification, and better hyperparameter tuning. Furthermore, the preprocessing step is changed to allow the processing of colored images instead of only black and white to improve accuracy. The proposed structure fractionalizes runtime and introduces an efficient approach to deep convolutional SNNs. </details>
<details>	<summary>注释</summary>	6 pages, 7 figures, to be published in the Asilomar 2022 conference ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2022年11月01日</details>

# 595、非线性回归的脉冲神经网络
- [ ] Spiking neural networks for nonlinear regression 
时间：2022年10月26日                         第一作者：Alex                       [链接](https://arxiv.org/abs/2210.03515).                     
<details>	<summary>邮件日期</summary>	2022年10月27日</details>

# 594、GLIF：一种用于脉冲神经网络的统一门控泄漏集成和火灾神经元
- [ ] GLIF: A Unified Gated Leaky Integrate-and-Fire Neuron for Spiking Neural Networks 
时间：2022年10月25日                         第一作者：Xingting Yao                       [链接](https://arxiv.org/abs/2210.13768).                     
## 摘要：几十年来，人们对脉冲神经网络（SNN）进行了研究，以结合其生物学合理性并利用其有希望的能源效率。在现有的SNN中，通常采用泄漏整合和激发（LIF）模型来描述脉冲神经元，并演化为具有不同生物学特征的多种变体。然而，大多数基于LIF的神经元在不同的神经元行为中仅支持单一的生物学特征，限制了它们的表达能力和神经元动态多样性。在本文中，我们提出了一种统一的脉冲神经元GLIF，以融合不同神经元行为中的不同生物特征，扩大脉冲神经元的表示空间。在GLIF中，用于确定融合生物特征比例的门控因子在训练期间是可学习的。结合所有可学习的膜相关参数，我们的方法可以使脉冲神经元不同并不断变化，从而增加脉冲神经元的异质性和适应性
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have been studied over decades to incorporate their biological plausibility and leverage their promising energy efficiency. Throughout existing SNNs, the leaky integrate-and-fire (LIF) model is commonly adopted to formulate the spiking neuron and evolves into numerous variants with different biological features. However, most LIF-based neurons support only single biological feature in different neuronal behaviors, limiting their expressiveness and neuronal dynamic diversity. In this paper, we propose GLIF, a unified spiking neuron, to fuse different bio-features in different neuronal behaviors, enlarging the representation space of spiking neurons. In GLIF, gating factors, which are exploited to determine the proportion of the fused bio-features, are learnable during training. Combining all learnable membrane-related parameters, our method can make spiking neurons different and constantly changing, thus increasing the heterogeneity and adaptivity of spiking neurons. Extensive experiments on a variety of datasets demonstrate that our method obtains superior performance compared with other SNNs by simply changing their neuronal formulations to GLIF. In particular, we train a spiking ResNet-19 with GLIF and achieve $77.35\%$ top-1 accuracy with six time steps on CIFAR-100, which has advanced the state-of-the-art. Codes are available at \url{https://github.com/Ikarosy/Gated-LIF}. </details>
<details>	<summary>注释</summary>	Accepted at NeurIPS 2022 </details>
<details>	<summary>邮件日期</summary>	2022年10月26日</details>

# 593、医学影像学学习不同的卷积滤波器吗？
- [ ] Does Medical Imaging learn different Convolution Filters? 
时间：2022年10月25日                         第一作者：Paul Gavrikov                        [链接](https://arxiv.org/abs/2210.13799).                     
## 摘要：最近的工作通过包含数百个异构图像模型的大规模研究，研究了学习卷积滤波器的分布。令人惊讶的是，平均而言，在包括学习任务、图像域或数据集在内的各种研究维度的比较中，分布仅显示出微小的偏差。然而，在所研究的图像域中，医学成像模型似乎通过“脉冲”分布显示出显著的异常值，因此，学习了不同于其他域的高度特异性滤波器簇。根据这一观察，我们更详细地研究了收集的医学成像模型。我们表明，异常值不是根本差异，而是由于某些架构中的特定处理。恰恰相反，对于标准化架构，我们发现基于医学数据训练的模型在过滤器分布上与基于其他领域数据训练的类似架构没有显著差异。我们的结论加强了预测
<details>	<summary>英文摘要</summary>	Recent work has investigated the distributions of learned convolution filters through a large-scale study containing hundreds of heterogeneous image models. Surprisingly, on average, the distributions only show minor drifts in comparisons of various studied dimensions including the learned task, image domain, or dataset. However, among the studied image domains, medical imaging models appeared to show significant outliers through "spikey" distributions, and, therefore, learn clusters of highly specific filters different from other domains. Following this observation, we study the collected medical imaging models in more detail. We show that instead of fundamental differences, the outliers are due to specific processing in some architectures. Quite the contrary, for standardized architectures, we find that models trained on medical data do not significantly differ in their filter distributions from similar architectures trained on data from other domains. Our conclusions reinforce previous hypotheses stating that pre-training of imaging models can be done with any kind of diverse image data. </details>
<details>	<summary>注释</summary>	Accepted at MedNeurIPS 2022 </details>
<details>	<summary>邮件日期</summary>	2022年10月26日</details>

# 592、SpikeSim:一种端到端的内存计算硬件评估工具，用于对脉冲神经网络进行基准测试
- [ ] SpikeSim: An end-to-end Compute-in-Memory Hardware Evaluation Tool for Benchmarking Spiking Neural Networks 
时间：2022年10月24日                         第一作者：Abhishek Moitra                       [链接](https://arxiv.org/abs/2210.12899).                     
## 摘要：SNN是面向能效机器智能的一个积极研究领域。与传统的神经网络相比，SNN使用时间脉冲数据和生物似然的神经元激活功能，如泄漏积分火/积分火（LIF/IF）进行数据处理。然而，在标准冯·诺依曼计算平台中，SNN会产生大量的点积运算，导致高内存和计算开销。如今，内存计算（IMC）架构已被提出，以缓解冯·诺依曼架构中普遍存在的“内存墙瓶颈”。尽管最近的工作提出了基于IMC的SNN硬件加速器，但以下几点被忽略了：1）由于在多个时间步长上重复模拟点积运算，交叉非理想性对SNN性能的不利影响，2）基本SNN特定组件（如LIF/IF和数据通信模块）的硬件开销。为此，我们建议使用SpikeSim，这是一种可以执行逼真性能的工具，ener
<details>	<summary>英文摘要</summary>	SNNs are an active research domain towards energy efficient machine intelligence. Compared to conventional ANNs, SNNs use temporal spike data and bio-plausible neuronal activation functions such as Leaky-Integrate Fire/Integrate Fire (LIF/IF) for data processing. However, SNNs incur significant dot-product operations causing high memory and computation overhead in standard von-Neumann computing platforms. Today, In-Memory Computing (IMC) architectures have been proposed to alleviate the "memory-wall bottleneck" prevalent in von-Neumann architectures. Although recent works have proposed IMC-based SNN hardware accelerators, the following have been overlooked- 1) the adverse effects of crossbar non-ideality on SNN performance due to repeated analog dot-product operations over multiple time-steps, 2) hardware overheads of essential SNN-specific components such as the LIF/IF and data communication modules. To this end, we propose SpikeSim, a tool that can perform realistic performance, energy, latency and area evaluation of IMC-mapped SNNs. SpikeSim consists of a practical monolithic IMC architecture called SpikeFlow for mapping SNNs. Additionally, the non-ideality computation engine (NICE) and energy-latency-area (ELA) engine performs hardware-realistic evaluation of SpikeFlow-mapped SNNs. Based on 65nm CMOS implementation and experiments on CIFAR10, CIFAR100 and TinyImagenet datasets, we find that the LIF/IF neuronal module has significant area contribution (>11% of the total hardware area). We propose SNN topological modifications leading to 1.24x and 10x reduction in the neuronal module's area and the overall energy-delay-product value, respectively. Furthermore, in this work, we perform a holistic comparison between IMC implemented ANN and SNNs and conclude that lower number of time-steps are the key to achieve higher throughput and energy-efficiency for SNNs compared to 4-bit ANNs. </details>
<details>	<summary>注释</summary>	14 pages, 22 figures </details>
<details>	<summary>邮件日期</summary>	2022年10月25日</details>

# 591、实现节能、低延迟和精确峰值LSTM
- [ ] Towards Energy-Efficient, Low-Latency and Accurate Spiking LSTMs 
时间：2022年10月23日                         第一作者：Gourav Datta                        [链接](https://arxiv.org/abs/2210.12613).                     
## 摘要：脉冲神经网络（SNN）已成为复杂视觉任务的一种极具吸引力的时空计算范式。然而，大多数现有的工作产生的模型需要许多时间步长，并且没有利用脉冲神经网络的固有时间动态，即使是连续任务。基于这一观察，我们提出了一种优化的脉冲长-短期记忆网络（LSTM）训练框架，该框架包括一种新的ANN到SNN转换框架，然后是SNN训练。特别是，我们在源LSTM架构中提出了新的激活函数，并明智地选择其中的一个子集进行转换，以集成和激发（IF）具有最佳偏置偏移的激活。此外，我们推导了从其非脉冲LSTM对应物转换的泄漏积分和激发（LIF）激活函数，这证明了需要联合优化权重、阈值和泄漏参数。我们还提出了一种流水线并行处理方案
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have emerged as an attractive spatio-temporal computing paradigm for complex vision tasks. However, most existing works yield models that require many time steps and do not leverage the inherent temporal dynamics of spiking neural networks, even for sequential tasks. Motivated by this observation, we propose an \rev{optimized spiking long short-term memory networks (LSTM) training framework that involves a novel ANN-to-SNN conversion framework, followed by SNN training}. In particular, we propose novel activation functions in the source LSTM architecture and judiciously select a subset of them for conversion to integrate-and-fire (IF) activations with optimal bias shifts. Additionally, we derive the leaky-integrate-and-fire (LIF) activation functions converted from their non-spiking LSTM counterparts which justifies the need to jointly optimize the weights, threshold, and leak parameter. We also propose a pipelined parallel processing scheme which hides the SNN time steps, significantly improving system latency, especially for long sequences. The resulting SNNs have high activation sparsity and require only accumulate operations (AC), in contrast to expensive multiply-and-accumulates (MAC) needed for ANNs, except for the input layer when using direct encoding, yielding significant improvements in energy efficiency. We evaluate our framework on sequential learning tasks including temporal MNIST, Google Speech Commands (GSC), and UCI Smartphone datasets on different LSTM architectures. We obtain test accuracy of 94.75% with only 2 time steps with direct encoding on the GSC dataset with 4.1x lower energy than an iso-architecture standard LSTM. </details>
<details>	<summary>邮件日期</summary>	2022年10月25日</details>

# 590、具有脉冲递归赢家通吃网络的生物合理变分策略梯度
- [ ] Biologically Plausible Variational Policy Gradient with Spiking Recurrent Winner-Take-All Networks 
时间：2022年10月21日                         第一作者：Zhile Yang                       [链接](https://arxiv.org/abs/2210.13225).                     
## 摘要：强化学习研究的一个方向是探索生物学上合理的模型和算法，以模拟生物智能并适应神经形态硬件。其中，奖励调节的脉冲时间依赖性可塑性（R-STDP）是一个新的分支，在能量效率方面具有良好的潜力。然而，当前的R-STDP方法依赖于局部学习规则的启发式设计，因此需要特定于任务的专家知识。在本文中，我们考虑了一个脉冲递归赢家通吃网络，并提出了一种新的R-STDP方法，即脉冲变分策略梯度（SVPG），其局部学习规则是从全局策略梯度中导出的，因此不需要启发式设计。在MNIST分类和体操倒立摆实验中，我们的SVPG实现了良好的训练性能，并且比传统方法对各种噪声具有更好的鲁棒性。
<details>	<summary>英文摘要</summary>	One stream of reinforcement learning research is exploring biologically plausible models and algorithms to simulate biological intelligence and fit neuromorphic hardware. Among them, reward-modulated spike-timing-dependent plasticity (R-STDP) is a recent branch with good potential in energy efficiency. However, current R-STDP methods rely on heuristic designs of local learning rules, thus requiring task-specific expert knowledge. In this paper, we consider a spiking recurrent winner-take-all network, and propose a new R-STDP method, spiking variational policy gradient (SVPG), whose local learning rules are derived from the global policy gradient and thus eliminate the need for heuristic designs. In experiments of MNIST classification and Gym InvertedPendulum, our SVPG achieves good training performance, and also presents better robustness to various kinds of noises than conventional methods. </details>
<details>	<summary>注释</summary>	Accepted to BMVC 2022 </details>
<details>	<summary>邮件日期</summary>	2022年10月25日</details>

# 589、前向传播脉冲神经网络的精确梯度计算
- [ ] Exact Gradient Computation for Spiking Neural Networks Through Forward Propagation 
时间：2022年10月18日                         第一作者：Jane H. Lee                       [链接](https://arxiv.org/abs/2210.15415).                     
## 摘要：脉冲神经网络（SNN）最近作为传统神经网络的替代品出现，因为它具有能量效率优势和更好地捕捉生物神经元机制的能力。然而，用于训练传统网络的经典反向传播算法因脉冲时间的硬阈值和不连续性而难以应用于SNN。因此，大多数先前的工作认为SNN相对于其权重的精确梯度不存在，并且专注于产生替代梯度的近似方法。在本文中，（1）通过将隐函数定理应用于离散脉冲时间的SNN，我们证明了尽管SNN在时间上是不可微的，但相对于其权重，SNN具有明确定义的梯度，并且（2）我们提出了一种新的训练算法，称为\emph{前向传播}（FP），它计算SNN的精确梯度。FP利用脉冲之间的因果关系结构，允许我们并行计算
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNN) have recently emerged as alternatives to traditional neural networks, owing to energy efficiency benefits and capacity to better capture biological neuronal mechanisms. However, the classic backpropagation algorithm for training traditional networks has been notoriously difficult to apply to SNN due to the hard-thresholding and discontinuities at spike times. Therefore, a large majority of prior work believes exact gradients for SNN w.r.t. their weights do not exist and has focused on approximation methods to produce surrogate gradients. In this paper, (1) by applying the implicit function theorem to SNN at the discrete spike times, we prove that, albeit being non-differentiable in time, SNNs have well-defined gradients w.r.t. their weights, and (2) we propose a novel training algorithm, called \emph{forward propagation} (FP), that computes exact gradients for SNN. FP exploits the causality structure between the spikes and allows us to parallelize computation forward in time. It can be used with other algorithms that simulate the forward pass, and it also provides insights on why other related algorithms such as Hebbian learning and also recently-proposed surrogate gradient methods may perform well. </details>
<details>	<summary>邮件日期</summary>	2022年10月28日</details>

# 588、利用光线密度融合从自我运动中基于事件的立体深度估计
- [ ] Event-based Stereo Depth Estimation from Ego-motion using Ray Density Fusion 
时间：2022年10月17日                         第一作者：Suman Ghosh                        [链接](https://arxiv.org/abs/2210.08927).                     
## 摘要：事件摄像机是仿生传感器，通过响应场景中的亮度变化来模拟人类视网膜。它们以微秒分辨率生成基于异步脉冲的输出，与传统相机相比具有高动态范围、低运动模糊和能效等优点。大多数基于事件的立体方法试图利用摄像机的高时间分辨率和摄像机间事件的同时性来建立匹配和估计深度。相比之下，这项工作研究了如何通过融合反向投影光线密度，在没有明确数据关联的情况下，从立体事件摄像机中估计深度，并证明了其对以自我为中心的方式记录的头戴式摄像机数据的有效性。代码和视频可在https://github.com/tub-rip/dvs_mcemvs
<details>	<summary>英文摘要</summary>	Event cameras are bio-inspired sensors that mimic the human retina by responding to brightness changes in the scene. They generate asynchronous spike-based outputs at microsecond resolution, providing advantages over traditional cameras like high dynamic range, low motion blur and power efficiency. Most event-based stereo methods attempt to exploit the high temporal resolution of the camera and the simultaneity of events across cameras to establish matches and estimate depth. By contrast, this work investigates how to estimate depth from stereo event cameras without explicit data association by fusing back-projected ray densities, and demonstrates its effectiveness on head-mounted camera data, which is recorded in an egocentric fashion. Code and video are available at https://github.com/tub-rip/dvs_mcemvs </details>
<details>	<summary>注释</summary>	6 pages, 3 figures, project page: https://github.com/tub-rip/dvs_mcemvs Journal-ref: 2nd International Ego4D Workshop at ECCV 2022 </details>
<details>	<summary>邮件日期</summary>	2022年10月18日</details>

# 587、使用Spiking DS ResNet的多级射击：实现更好、更深入的直接训练Spiking神经网络
- [ ] Multi-Level Firing with Spiking DS-ResNet: Enabling Better and Deeper Directly-Trained Spiking Neural Networks 
时间：2022年10月12日                         第一作者：Lang Feng                       [链接](https://arxiv.org/abs/2210.06386).                     
## 摘要：脉冲神经网络（SNN）是一种具有异步离散和稀疏特性的仿生神经网络，其在低能耗方面的优势日益显现。最近的研究致力于利用时空信息通过反向传播直接训练SNN。然而，脉冲活动的二元和不可微分特性迫使直接训练的SNN遭受严重的梯度消失和网络退化，这极大地限制了直接训练SNN的性能，并阻止其深入。在本文中，我们提出了一种基于现有时空反向传播（STBP）方法和脉冲休眠抑制残差网络（脉冲DS-ResNet）的多级激发（MLF）方法。MLF能够实现更有效的梯度传播和神经元的增量表达能力。Spiking DS ResNet可以有效地执行离散脉冲的身份映射，并提供更合适的连接
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are bio-inspired neural networks with asynchronous discrete and sparse characteristics, which have increasingly manifested their superiority in low energy consumption. Recent research is devoted to utilizing spatio-temporal information to directly train SNNs by backpropagation. However, the binary and non-differentiable properties of spike activities force directly trained SNNs to suffer from serious gradient vanishing and network degradation, which greatly limits the performance of directly trained SNNs and prevents them from going deeper. In this paper, we propose a multi-level firing (MLF) method based on the existing spatio-temporal back propagation (STBP) method, and spiking dormant-suppressed residual network (spiking DS-ResNet). MLF enables more efficient gradient propagation and the incremental expression ability of the neurons. Spiking DS-ResNet can efficiently perform identity mapping of discrete spikes, as well as provide a more suitable connection for gradient propagation in deep SNNs. With the proposed method, our model achieves superior performances on a non-neuromorphic dataset and two neuromorphic datasets with much fewer trainable parameters and demonstrates the great ability to combat the gradient vanishing and degradation problem in deep SNNs. </details>
<details>	<summary>邮件日期</summary>	2022年10月13日</details>

# 586、科学机器学习中的脉冲神经算子
- [ ] Spiking Neural Operators for Scientific Machine Learning 
时间：2022年10月12日                         第一作者：Adar Kahana                       [链接](https://arxiv.org/abs/2205.10130).                     
<details>	<summary>注释</summary>	16 pages, 6 figures and 4 tables </details>
<details>	<summary>邮件日期</summary>	2022年10月13日</details>

# 585、鲁棒和加速的单脉冲神经网络训练，适用于具有挑战性的时间任务
- [ ] Robust and accelerated single-spike spiking neural network training with applicability to challenging temporal tasks 
时间：2022年10月11日                         第一作者：Luke Taylor                       [链接](https://arxiv.org/abs/2205.15286).                     
<details>	<summary>注释</summary>	18 pages, 6 figures, under review at ICLR 2023 </details>
<details>	<summary>邮件日期</summary>	2022年10月13日</details>

# 584、STSC-SNN:Spiking神经网络时空突触与时间卷积的联系和注意
- [ ] STSC-SNN: Spatio-Temporal Synaptic Connection with Temporal Convolution and Attention for Spiking Neural Networks 
时间：2022年10月11日                         第一作者：Chengting Yu                       [链接](https://arxiv.org/abs/2210.05241).                     
## 摘要：脉冲神经网络（SNNs）作为神经形态计算中的算法模型之一，由于具有时间信息处理能力、低功耗和高生物可信性，受到了广泛的研究关注。高效提取时空特征的潜力使其适合于处理事件流。然而，SNN中现有的突触结构几乎是完全连接或空间2D卷积，两者都不能充分提取时间依赖性。在这项工作中，我们从生物突触中获得灵感，并提出了时空突触连接SNN（STSC-SNN）模型，以增强突触连接的时空感受野，从而建立跨层的时间依赖性。具体来说，我们结合了时间卷积和注意力机制来实现突触过滤和门控功能。我们表明，赋予突触模型时间依赖性可以提高
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs), as one of the algorithmic models in neuromorphic computing, have gained a great deal of research attention owing to temporal information processing capability, low power consumption, and high biological plausibility. The potential to efficiently extract spatio-temporal features makes it suitable for processing the event streams. However, existing synaptic structures in SNNs are almost full-connections or spatial 2D convolution, neither of which can extract temporal dependencies adequately. In this work, we take inspiration from biological synapses and propose a spatio-temporal synaptic connection SNN (STSC-SNN) model, to enhance the spatio-temporal receptive fields of synaptic connections, thereby establishing temporal dependencies across layers. Concretely, we incorporate temporal convolution and attention mechanisms to implement synaptic filtering and gating functions. We show that endowing synaptic models with temporal dependencies can improve the performance of SNNs on classification tasks. In addition, we investigate the impact of performance vias varied spatial-temporal receptive fields and reevaluate the temporal modules in SNNs. Our approach is tested on neuromorphic datasets, including DVS128 Gesture (gesture recognition), N-MNIST, CIFAR10-DVS (image classification), and SHD (speech digit recognition). The results show that the proposed model outperforms the state-of-the-art accuracy on nearly all datasets. </details>
<details>	<summary>邮件日期</summary>	2022年10月12日</details>

# 583、具有不同位置脉冲神经元的事件驱动触觉学习
- [ ] Event-Driven Tactile Learning with Various Location Spiking Neurons 
时间：2022年10月11日                         第一作者：Peng Kang                       [链接](https://arxiv.org/abs/2210.04277).                     
<details>	<summary>注释</summary>	This paper is under review in IEEE TNNLS. Please note that this paper is a journal extension of our previous conference paper: arXiv:2209.01080. This paper includes more novel models, data, experiments, and demonstration </details>
<details>	<summary>邮件日期</summary>	2022年10月12日</details>

# 582、在神经形态硬件上高效部署机器学习工作负载
- [ ] Energy-Efficient Deployment of Machine Learning Workloads on Neuromorphic Hardware 
时间：2022年10月10日                         第一作者：Peyton Ch                       [链接](https://arxiv.org/abs/2210.05006).                     
## 摘要：随着科技行业正朝着在较小的边缘计算设备上实现自然语言处理、路径规划、图像分类等任务的方向发展，对算法和硬件加速器更高效实现的需求已成为一个重要的研究领域。近年来，已经发布了几款边缘深度学习硬件加速器，专门致力于减少深度神经网络（DNN）消耗的功率和面积。另一方面，在离散时间序列数据上运行的脉冲神经网络（SNN）已被证明在部署在基于神经形态事件的专用/异步硬件上时，甚至比上述边缘DNN加速器实现了显著的功率降低。尽管神经形态硬件在加速边缘深度学习任务方面显示出巨大的潜力，但目前算法和硬件的空间有限，仍处于相当早期的发展阶段。因此，许多混合方法
<details>	<summary>英文摘要</summary>	As the technology industry is moving towards implementing tasks such as natural language processing, path planning, image classification, and more on smaller edge computing devices, the demand for more efficient implementations of algorithms and hardware accelerators has become a significant area of research. In recent years, several edge deep learning hardware accelerators have been released that specifically focus on reducing the power and area consumed by deep neural networks (DNNs). On the other hand, spiking neural networks (SNNs) which operate on discrete time-series data, have been shown to achieve substantial power reductions over even the aforementioned edge DNN accelerators when deployed on specialized neuromorphic event-based/asynchronous hardware. While neuromorphic hardware has demonstrated great potential for accelerating deep learning tasks at the edge, the current space of algorithms and hardware is limited and still in rather early development. Thus, many hybrid approaches have been proposed which aim to convert pre-trained DNNs into SNNs. In this work, we provide a general guide to converting pre-trained DNNs into SNNs while also presenting techniques to improve the deployment of converted SNNs on neuromorphic hardware with respect to latency, power, and energy. Our experimental results show that when compared against the Intel Neural Compute Stick 2, Intel's neuromorphic processor, Loihi, consumes up to 27x less power and 5x less energy in the tested image classification tasks by using our SNN improvement techniques. </details>
<details>	<summary>邮件日期</summary>	2022年10月12日</details>

# 581、用局部串联学习训练脉冲神经网络
- [ ] Training Spiking Neural Networks with Local Tandem Learning 
时间：2022年10月10日                         第一作者：Qu Yang                       [链接](https://arxiv.org/abs/2210.04532).                     
## 摘要：脉冲神经网络（SNN）被证明在生物学上比其前辈更合理，更节能。然而，对于深度SNN，尤其是对于部署在模拟计算基板上的深度SNN来说，缺乏一种有效且通用的训练方法。在本文中，我们提出了一种广义学习规则，称为局部串联学习（LTL）。LTL规则通过模仿预先训练的ANN的中间特征表示来遵循师生学习方法。通过解耦网络层的学习并利用高度信息量的监督信号，我们在CIFAR-10数据集的五个训练周期内证明了网络的快速收敛，同时具有较低的计算复杂度。我们的实验结果还表明，这样训练的SNN可以在CIFAR-10、CIFAR-100和Tiny ImageNet数据集上达到与其教师ANN相当的精度。此外，所提出的LTL规则是硬件友好的。它可以很容易地在芯片上实现
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are shown to be more biologically plausible and energy efficient over their predecessors. However, there is a lack of an efficient and generalized training method for deep SNNs, especially for deployment on analog computing substrates. In this paper, we put forward a generalized learning rule, termed Local Tandem Learning (LTL). The LTL rule follows the teacher-student learning approach by mimicking the intermediate feature representations of a pre-trained ANN. By decoupling the learning of network layers and leveraging highly informative supervisor signals, we demonstrate rapid network convergence within five training epochs on the CIFAR-10 dataset while having low computational complexity. Our experimental results have also shown that the SNNs thus trained can achieve comparable accuracies to their teacher ANNs on CIFAR-10, CIFAR-100, and Tiny ImageNet datasets. Moreover, the proposed LTL rule is hardware friendly. It can be easily implemented on-chip to perform fast parameter calibration and provide robustness against the notorious device non-ideality issues. It, therefore, opens up a myriad of opportunities for training and deployment of SNN on ultra-low-power mixed-signal neuromorphic computing chips.10 </details>
<details>	<summary>注释</summary>	Accepted by NeurIPS 2022 </details>
<details>	<summary>邮件日期</summary>	2022年10月11日</details>

# 580、脉冲神经网络的在线训练
- [ ] Online Training Through Time for Spiking Neural Networks 
时间：2022年10月09日                         第一作者：Mingqing Xiao                       [链接](https://arxiv.org/abs/2210.04195).                     
## 摘要：脉冲神经网络（SNN）是一种很有前途的以大脑为灵感的节能模型。训练方法的最新进展使深度SNN能够成功地执行低延迟的大规模任务。特别是，具有替代梯度（SG）的时间反向传播（BPTT）被广泛用于在非常小的时间步长内实现高性能。然而，它的代价是大量的内存消耗用于训练，缺乏优化的理论清晰度，以及与生物学习的在线属性和神经形态硬件的规则不一致。其他工作将SNN的脉冲表示与等效人工神经网络公式联系起来，并通过来自等效映射的梯度来训练SNN，以确保下降方向。但它们无法实现低延迟，也无法在线。在这项工作中，我们提出了SNN的在线时间训练（OTTT），它源自BPTT，通过跟踪突触前活动和杠杆来实现前向时间学习
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are promising brain-inspired energy-efficient models. Recent progress in training methods has enabled successful deep SNNs on large-scale tasks with low latency. Particularly, backpropagation through time (BPTT) with surrogate gradients (SG) is popularly used to achieve high performance in a very small number of time steps. However, it is at the cost of large memory consumption for training, lack of theoretical clarity for optimization, and inconsistency with the online property of biological learning and rules on neuromorphic hardware. Other works connect spike representations of SNNs with equivalent artificial neural network formulation and train SNNs by gradients from equivalent mappings to ensure descent directions. But they fail to achieve low latency and are also not online. In this work, we propose online training through time (OTTT) for SNNs, which is derived from BPTT to enable forward-in-time learning by tracking presynaptic activities and leveraging instantaneous loss and gradients. Meanwhile, we theoretically analyze and prove that gradients of OTTT can provide a similar descent direction for optimization as gradients based on spike representations under both feedforward and recurrent conditions. OTTT only requires constant training memory costs agnostic to time steps, avoiding the significant memory costs of BPTT for GPU training. Furthermore, the update rule of OTTT is in the form of three-factor Hebbian learning, which could pave a path for online on-chip learning. With OTTT, it is the first time that two mainstream supervised SNN training methods, BPTT with SG and spike representation-based training, are connected, and meanwhile in a biologically plausible form. Experiments on CIFAR-10, CIFAR-100, ImageNet, and CIFAR10-DVS demonstrate the superior performance of our method on large-scale static and neuromorphic datasets in small time steps. </details>
<details>	<summary>注释</summary>	Accepted by NeurIPS 2022 </details>
<details>	<summary>邮件日期</summary>	2022年10月11日</details>

# 579、具有不同位置脉冲神经元的事件驱动触觉学习
- [ ] Event-Driven Tactile Learning with Various Location Spiking Neurons 
时间：2022年10月09日                         第一作者：Peng Kang                       [链接](https://arxiv.org/abs/2210.04277).                     
## 摘要：触觉感应对于各种日常任务至关重要。事件驱动触觉传感器和脉冲神经网络（SNN）的新进展推动了相关领域的研究。然而，由于现有脉冲神经元的有限表示能力和数据的高时空复杂性，SNN支持的事件驱动触觉学习仍处于初级阶段。在本文中，为了提高现有脉冲神经元的表示能力，我们提出了一种称为“位置脉冲神经元”的新神经元模型，该模型使我们能够以一种新的方式提取基于事件的数据的特征。具体而言，基于经典的时间脉冲响应模型（TSRM），我们开发了位置脉冲响应模式（LSRM）。此外，基于最常用的时间泄漏集成和火灾（TLIF）模型，我们开发了位置泄漏集成和消防（LLIF）模式。通过利用新的位置脉冲神经元，我们提出了几种模型来捕捉复杂的时空依赖关系
<details>	<summary>英文摘要</summary>	Tactile sensing is essential for a variety of daily tasks. New advances in event-driven tactile sensors and Spiking Neural Networks (SNNs) spur the research in related fields. However, SNN-enabled event-driven tactile learning is still in its infancy due to the limited representation abilities of existing spiking neurons and high spatio-temporal complexity in the data. In this paper, to improve the representation capability of existing spiking neurons, we propose a novel neuron model called "location spiking neuron", which enables us to extract features of event-based data in a novel way. Specifically, based on the classical Time Spike Response Model (TSRM), we develop the Location Spike Response Model (LSRM). In addition, based on the most commonly-used Time Leaky Integrate-and-Fire (TLIF) model, we develop the Location Leaky Integrate-and-Fire (LLIF) model. By exploiting the novel location spiking neurons, we propose several models to capture the complex spatio-temporal dependencies in the event-driven tactile data. Extensive experiments demonstrate the significant improvements of our models over other works on event-driven tactile learning and show the superior energy efficiency of our models and location spiking neurons, which may unlock their potential on neuromorphic hardware. </details>
<details>	<summary>注释</summary>	under review. arXiv admin note: substantial text overlap with arXiv:2209.01080 </details>
<details>	<summary>邮件日期</summary>	2022年10月11日</details>

# 578、使用连续STDP学习的脉冲神经网络融合SLAM中基于事件的摄像机和雷达
- [ ] Fusing Event-based Camera and Radar for SLAM Using Spiking Neural Networks with Continual STDP Learning 
时间：2022年10月09日                         第一作者：Ali Safa                       [链接](https://arxiv.org/abs/2210.04236).                     
## 摘要：这项工作提出了一种融合基于事件的摄像机和调频连续波（FMCW）雷达的无人机导航SLAM架构。每个传感器都由生物启发的脉冲神经网络（SNN）处理，具有连续的脉冲时间依赖性可塑性（STDP）学习，如在大脑中观察到的。与大多数基于学习的SLAM系统%（a）需要获取必须执行导航的环境的代表性数据集，b）需要离线训练阶段）相比，我们的方法不需要任何离线训练阶段，而是SNN通过STDP不断地从飞行中的输入数据中学习特征。同时，SNN输出被用作环路闭合检测和映射校正的特征描述符。我们进行了大量实验，以对照最先进的RGB方法对我们的系统进行基准测试，并证明了我们的DVS雷达SLAM方法在强光照变化下的鲁棒性。
<details>	<summary>英文摘要</summary>	This work proposes a first-of-its-kind SLAM architecture fusing an event-based camera and a Frequency Modulated Continuous Wave (FMCW) radar for drone navigation. Each sensor is processed by a bio-inspired Spiking Neural Network (SNN) with continual Spike-Timing-Dependent Plasticity (STDP) learning, as observed in the brain. In contrast to most learning-based SLAM systems%, which a) require the acquisition of a representative dataset of the environment in which navigation must be performed and b) require an off-line training phase, our method does not require any offline training phase, but rather the SNN continuously learns features from the input data on the fly via STDP. At the same time, the SNN outputs are used as feature descriptors for loop closure detection and map correction. We conduct numerous experiments to benchmark our system against state-of-the-art RGB methods and we demonstrate the robustness of our DVS-Radar SLAM approach under strong lighting variations. </details>
<details>	<summary>邮件日期</summary>	2022年10月11日</details>

# 577、视觉感知的过参数化直接拟合模型
- [ ] Toward an Over-parameterized Direct-Fit Model of Visual Perception 
时间：2022年10月07日                         第一作者：Xin Li                       [链接](https://arxiv.org/abs/2210.03850).                     
## 摘要：在这篇文章中，我们重新讨论了简单和复杂细胞的计算建模问题，以用于过度参数化和直接拟合的视觉感知模型。与传统观点不同，我们强调了简单细胞和复杂细胞之间平行和顺序结合机制的差异。我们提出了一个将它们抽象为空间划分和组合的新建议，作为我们新层次结构的基础。我们的构造可以解释为现有k-d树的基于产品拓扑的泛化，使其适用于高维空间中的蛮力直接拟合。所构建的模型已应用于神经科学和心理学的几个经典实验。我们提供了对构建的视觉模型的反稀疏编码解释，并展示了它如何基于$\ell_｛\infty｝$优化导致类似动态规划（DP）的近似最近邻搜索。我们还简要讨论了两种可能的基于
<details>	<summary>英文摘要</summary>	In this paper, we revisit the problem of computational modeling of simple and complex cells for an over-parameterized and direct-fit model of visual perception. Unlike conventional wisdom, we highlight the difference in parallel and sequential binding mechanisms between simple and complex cells. A new proposal for abstracting them into space partitioning and composition is developed as the foundation of our new hierarchical construction. Our construction can be interpreted as a product topology-based generalization of the existing k-d tree, making it suitable for brute-force direct-fit in a high-dimensional space. The constructed model has been applied to several classical experiments in neuroscience and psychology. We provide an anti-sparse coding interpretation of the constructed vision model and show how it leads to a dynamic programming (DP)-like approximate nearest-neighbor search based on $\ell_{\infty}$-optimization. We also briefly discuss two possible implementations based on asymmetrical (decoder matters more) auto-encoder and spiking neural networks (SNN), respectively. </details>
<details>	<summary>邮件日期</summary>	2022年10月11日</details>

# 576、非线性回归的脉冲神经网络
- [ ] Spiking neural network for nonlinear regression 
时间：2022年10月06日                         第一作者：Alex                       [链接](https://arxiv.org/abs/2210.03515).                     
## 摘要：脉冲神经网络，也常被称为第三代神经网络，与传统的第二代神经网络相比，具有大幅减少记忆和能量消耗的潜力。受人类大脑无可争议的效率启发，他们引入了时间和神经元稀疏性，这可以被下一代神经形态硬件利用。为了打开工程应用的道路，我们在连续介质力学的背景下引入了这项令人兴奋的技术。然而，脉冲神经网络的性质对回归问题提出了挑战，这些问题在工程科学建模中经常出现。为了克服这个问题，提出了一种使用脉冲神经网络的回归框架。特别地，介绍了一种利用脉冲神经元的膜电位将二进制脉冲序列解码为实数的网络拓扑。由于本文的目的是简明扼要地介绍这种新方法
<details>	<summary>英文摘要</summary>	Spiking neural networks, also often referred to as the third generation of neural networks, carry the potential for a massive reduction in memory and energy consumption over traditional, second-generation neural networks. Inspired by the undisputed efficiency of the human brain, they introduce temporal and neuronal sparsity, which can be exploited by next-generation neuromorphic hardware. To open the pathway toward engineering applications, we introduce this exciting technology in the context of continuum mechanics. However, the nature of spiking neural networks poses a challenge for regression problems, which frequently arise in the modeling of engineering sciences. To overcome this problem, a framework for regression using spiking neural networks is proposed. In particular, a network topology for decoding binary spike trains to real numbers is introduced, utilizing the membrane potential of spiking neurons. As the aim of this contribution is a concise introduction to this new methodology, several different spiking neural architectures, ranging from simple spiking feed-forward to complex spiking long short-term memory neural networks, are derived. Several numerical experiments directed towards regression of linear and nonlinear, history-dependent material models are carried out. A direct comparison with counterparts of traditional neural networks shows that the proposed framework is much more efficient while retaining precision and generalizability. All code has been made publicly available in the interest of reproducibility and to promote continued enhancement in this new domain. </details>
<details>	<summary>邮件日期</summary>	2022年10月10日</details>

# 575、脉冲神经网络的基本组成模型
- [ ] A Basic Compositional Model for Spiking Neural Networks 
时间：2022年10月06日                         第一作者：Nancy Lynch                        [链接](https://arxiv.org/abs/1808.03884).                     
<details>	<summary>邮件日期</summary>	2022年10月10日</details>

# 574、SATA：Spiking神经网络的稀疏感知训练加速器
- [ ] SATA: Sparsity-Aware Training Accelerator for Spiking Neural Networks 
时间：2022年10月06日                         第一作者：Ruokai Yin                       [链接](https://arxiv.org/abs/2204.05422).                     
<details>	<summary>注释</summary>	13 pages. Accepted to IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (2022) </details>
<details>	<summary>邮件日期</summary>	2022年10月07日</details>

# 573、GLM-130B：一种开放的双语预训练模型
- [ ] GLM-130B: An Open Bilingual Pre-trained Model 
时间：2022年10月05日                         第一作者：Aohan Zeng                       [链接](https://arxiv.org/abs/2210.02414).                     
## 摘要：我们介绍了GLM-130B，一个双语（英语和汉语）预训练语言模型，具有1300亿个参数。这是一次尝试，试图开源一个至少与GPT-3一样好的100B规模模型，并揭示如何成功地预训练这样规模的模型。在这一努力的过程中，我们面临着许多意想不到的技术和工程挑战，特别是在损耗峰值和不收敛方面。在本文中，我们介绍了GLM-130B的训练过程，包括其设计选择、效率和稳定性的训练策略以及工程努力。最终的GLM-130B模型在广泛的流行英语基准测试中表现优于GPT-3 175B，而OPT-175B和BLOOM-176B没有表现出性能优势。在相关基准测试中，它也始终显著优于最大的中文模型ERNIE TITAN 3.0 260B。最后，我们利用GLM-130B的独特缩放特性来达到INT4量化
<details>	<summary>英文摘要</summary>	We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4$\times$RTX 3090 (24G) or 8$\times$RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B . </details>
<details>	<summary>注释</summary>	47 pages </details>
<details>	<summary>邮件日期</summary>	2022年10月06日</details>

# 572、在混合大脑领域：人脑和人工智能
- [ ] In the realm of hybrid Brain: Human Brain and AI 
时间：2022年10月04日                         第一作者：Hoda Fares                       [链接](https://arxiv.org/abs/2210.01461).                     
## 摘要：随着神经科学和工程学的最新发展，现在可以记录大脑信号并对其进行解码。此外，越来越多的刺激方法已经出现，以调节和影响大脑活动。当前的脑机接口（BCI）技术主要是治疗结果，它已经证明了其作为严重运动障碍患者的辅助和康复技术的效率。最近，人工智能（AI）和机器学习（ML）技术被用于解码大脑信号。除此之外，人工智能与先进脑机接口以植入式神经技术的形式结合，为神经和精神疾病的诊断、预测和治疗提供了新的可能性。在此背景下，我们设想开发闭环、智能、低功耗和小型化的神经接口，该接口将使用脑启发的人工智能技术和神经形态硬件来处理来自大脑的数据。这将
<details>	<summary>英文摘要</summary>	With the recent developments in neuroscience and engineering, it is now possible to record brain signals and decode them. Also, a growing number of stimulation methods have emerged to modulate and influence brain activity. Current brain-computer interface (BCI) technology is mainly on therapeutic outcomes, it already demonstrated its efficiency as assistive and rehabilitative technology for patients with severe motor impairments. Recently, artificial intelligence (AI) and machine learning (ML) technologies have been used to decode brain signals. Beyond this progress, combining AI with advanced BCIs in the form of implantable neurotechnologies grants new possibilities for the diagnosis, prediction, and treatment of neurological and psychiatric disorders. In this context, we envision the development of closed loop, intelligent, low-power, and miniaturized neural interfaces that will use brain inspired AI techniques with neuromorphic hardware to process the data from the brain. This will be referred to as Brain Inspired Brain Computer Interfaces (BI-BCIs). Such neural interfaces would offer access to deeper brain regions and better understanding for brain's functions and working mechanism, which improves BCIs operative stability and system's efficiency. On one hand, brain inspired AI algorithms represented by spiking neural networks (SNNs) would be used to interpret the multimodal neural signals in the BCI system. On the other hand, due to the ability of SNNs to capture rich dynamics of biological neurons and to represent and integrate different information dimensions such as time, frequency, and phase, it would be used to model and encode complex information processing in the brain and to provide feedback to the users. This paper provides an overview of the different methods to interface with the brain, presents future applications and discusses the merger of AI and BCIs. </details>
<details>	<summary>注释</summary>	41 Pages, 10 Figures, </details>
<details>	<summary>邮件日期</summary>	2022年10月05日</details>

# 571、基于事件的时序神经网络时间密集光流估计
- [ ] Event-based Temporally Dense Optical Flow Estimation with Sequential Neural Networks 
时间：2022年10月03日                         第一作者：Wachirawit Ponghiran                       [链接](https://arxiv.org/abs/2210.01244).                     
## 摘要：先前关于基于事件的光流估计的工作已经研究了几种基于梯度的学习方法，以训练用于预测光流的神经网络。然而，它们不利用事件数据流的快速数据速率，并且依赖于由固定时间段内（通常在两个灰度帧之间）的事件集合构建的时空表示。结果，仅在远低于基于事件的相机产生的速率数据的频率下评估光流，导致时间上稀疏的光流估计。为了预测时间密集的光流，我们将该问题视为一个顺序学习任务，并提出了一种训练方法来训练顺序网络，以便对事件流进行连续预测。我们提出了两种类型的网络：一种注重性能，另一种注重计算效率。我们首先在DSEC数据集上训练长短期存储网络（LSTM），并演示了10倍的时间密集光流
<details>	<summary>英文摘要</summary>	Prior works on event-based optical flow estimation have investigated several gradient-based learning methods to train neural networks for predicting optical flow. However, they do not utilize the fast data rate of event data streams and rely on a spatio-temporal representation constructed from a collection of events over a fixed period of time (often between two grayscale frames). As a result, optical flow is only evaluated at a frequency much lower than the rate data is produced by an event-based camera, leading to a temporally sparse optical flow estimation. To predict temporally dense optical flow, we cast the problem as a sequential learning task and propose a training methodology to train sequential networks for continuous prediction on an event stream. We propose two types of networks: one focused on performance and another focused on compute efficiency. We first train long-short term memory networks (LSTMs) on the DSEC dataset and demonstrated 10x temporally dense optical flow estimation over existing flow estimation approaches. The additional benefit of having a memory to draw long temporal correlations back in time results in a 19.7% improvement in flow prediction accuracy of LSTMs over similar networks with no memory elements. We subsequently show that the inherent recurrence of spiking neural networks (SNNs) enables them to learn and estimate temporally dense optical flow with 31.8% lesser parameters than LSTM, but with a slightly increased error. This demonstrates potential for energy-efficient implementation of fast optical flow prediction using SNNs. </details>
<details>	<summary>注释</summary>	There are 16 pages, 5 figures and 2 tables in total </details>
<details>	<summary>邮件日期</summary>	2022年10月05日</details>

# 570、由部分信息实现的高效脉冲变压器
- [ ] Efficient Spiking Transformer Enabled By Partial Information 
时间：2022年10月03日                         第一作者：Ziqing Wang                       [链接](https://arxiv.org/abs/2210.01208).                     
## 摘要：近年来，脉冲神经网络（SNN）由于其稀疏和异步通信特性而受到了广泛关注，因此可以部署在神经形态硬件中并实现极高的能效。然而，SNN目前难以实现与人工神经网络（ANN）相当的性能，因为其有限的可扩展性不允许大规模网络。特别是对于Transformer，作为一种在各种机器学习任务中表现出色的神经网络模型，它通过传统方法在SNN中的实现需要大量神经元，尤其是在自我注意模块中。受神经系统机制的启发，我们提出了一种有效的脉冲变压器（EST）框架，该框架由部分信息支持，以解决上述问题。在该模型中，我们不仅用合理数量的神经元实现了自我注意模块，还引入了部分信息自组织模式
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have received substantial attention in recent years due to their sparse and asynchronous communication nature, and thus can be deployed in neuromorphic hardware and achieve extremely high energy efficiency. However, SNNs currently can hardly realize a comparable performance to that of artificial neural networks (ANNs) because their limited scalability does not allow for large-scale networks. Especially for Transformer, as a model of ANNs that has accomplished remarkable performance in various machine learning tasks, its implementation in SNNs by conventional methods requires a large number of neurons, notably in the self-attention module. Inspired by the mechanisms in the nervous system, we propose an efficient spiking Transformer (EST) framework enabled by partial information to address the above problem. In this model, we not only implemented the self-attention module with a reasonable number of neurons, but also introduced partial-information self-attention (PSA), which utilizes only partial input signals, further reducing computational resources compared to conventional methods. The experimental results show that our EST can outperform the state-of-the-art SNN model in terms of accuracy and the number of time steps on both Cifar-10/100 and ImageNet datasets. In particular, the proposed EST model achieves 78.48% top-1 accuracy on the ImageNet dataset with only 16 time steps. In addition, our proposed PSA reduces flops by 49.8% with negligible performance loss compared to a self-attention module with full information. </details>
<details>	<summary>邮件日期</summary>	2022年10月05日</details>

# 569、使用机器人手臂的神经形态自适应控制算法随时间学习
- [ ] Learning over time using a neuromorphic adaptive control algorithm for robotic arms 
时间：2022年10月03日                         第一作者：Lazar Supic                        [链接](https://arxiv.org/abs/2210.01243).                     
## 摘要：在本文中，我们通过部署和彻底评估基于脉冲神经网络的自适应控制算法，探索了机器人手臂学习由手臂末端执行器可到达的位置（x，y，z）定义的底层操作空间的能力，包括干扰。尽管传统的机器人控制算法在适应新环境和动态环境方面存在局限性，但我们表明，机器人手臂可以随着时间的推移更快地学习操作空间和完成任务。我们还证明了基于SNN的自适应机器人控制算法能够在保持能量效率的同时实现快速响应。我们通过对自适应算法参数空间进行广泛搜索，并评估不同SNN网络大小、学习速率、动态机器人手臂轨迹和响应时间的算法性能，获得了这些结果。我们表明，机器人手臂在特定的实验场景中学习完成任务的速度要快15%，比如wi场景
<details>	<summary>英文摘要</summary>	In this paper, we explore the ability of a robot arm to learn the underlying operation space defined by the positions (x, y, z) that the arm's end-effector can reach, including disturbances, by deploying and thoroughly evaluating a Spiking Neural Network SNN-based adaptive control algorithm. While traditional control algorithms for robotics have limitations in both adapting to new and dynamic environments, we show that the robot arm can learn the operational space and complete tasks faster over time. We also demonstrate that the adaptive robot control algorithm based on SNNs enables a fast response while maintaining energy efficiency. We obtained these results by performing an extensive search of the adaptive algorithm parameter space, and evaluating algorithm performance for different SNN network sizes, learning rates, dynamic robot arm trajectories, and response times. We show that the robot arm learns to complete tasks 15% faster in specific experiment scenarios such as scenarios with six or nine random target points. </details>
<details>	<summary>邮件日期</summary>	2022年10月05日</details>

# 568、DOTIE——使用脉冲架构通过事件的时间隔离来检测对象
- [ ] DOTIE -- Detecting Objects through Temporal Isolation of Events using a Spiking Architecture 
时间：2022年10月03日                         第一作者：Manish Nagaraj                       [链接](https://arxiv.org/abs/2210.00975).                     
## 摘要：基于视觉的自主导航系统依靠快速准确的目标检测算法来避开障碍物。由于用于部署的硬件能量有限，为此类系统设计的算法和传感器需要计算效率高。生物启发的事件摄像机由于其速度、能效和对不同照明条件的鲁棒性，是此类系统的视觉传感器的良好候选。然而，传统的计算机视觉算法无法处理基于事件的输出，因为它们缺乏光度特征，如光强度和纹理。在这项工作中，我们提出了一种利用事件中固有的时间信息来有效检测运动对象的新技术。我们的技术包括一个轻量级的脉冲神经架构，它能够根据相应对象的速度来分离事件。然后，这些分离的事件在空间上进一步分组以确定对象边界。这个m
<details>	<summary>英文摘要</summary>	Vision-based autonomous navigation systems rely on fast and accurate object detection algorithms to avoid obstacles. Algorithms and sensors designed for such systems need to be computationally efficient, due to the limited energy of the hardware used for deployment. Biologically inspired event cameras are a good candidate as a vision sensor for such systems due to their speed, energy efficiency, and robustness to varying lighting conditions. However, traditional computer vision algorithms fail to work on event-based outputs, as they lack photometric features such as light intensity and texture. In this work, we propose a novel technique that utilizes the temporal information inherently present in the events to efficiently detect moving objects. Our technique consists of a lightweight spiking neural architecture that is able to separate events based on the speed of the corresponding objects. These separated events are then further grouped spatially to determine object boundaries. This method of object detection is both asynchronous and robust to camera noise. In addition, it shows good performance in scenarios with events generated by static objects in the background, where existing event-based algorithms fail. We show that by utilizing our architecture, autonomous navigation systems can have minimal latency and energy overheads for performing object detection. </details>
<details>	<summary>邮件日期</summary>	2022年10月04日</details>

# 567、多个射击事件中神经元种群的监督参数估计
- [ ] Supervised Parameter Estimation of Neuron Populations from Multiple Firing Events 
时间：2022年10月02日                         第一作者：Long Le                       [链接](https://arxiv.org/abs/2210.01767).                     
## 摘要：数学模型中生物神经元的放电动力学通常由模型参数决定，这些参数代表了神经元的潜在特性。参数估计问题寻求从单个神经元或神经元群体对外部刺激的反应以及它们之间的相互作用中恢复这些参数。文献中解决此问题的最常用方法是使用一些机械模型与基于仿真或基于解决方案的优化方案相结合。在本文中，我们研究了一种通过监督学习从由脉冲序列和参数标签对组成的训练集中学习神经元种群参数的自动方法。与之前的工作不同，这种自动学习不需要在推理时进行额外的模拟，也不需要在推导分析解或构建一些近似模型时获得专家知识。我们用不同的参数设置模拟了许多神经元群体
<details>	<summary>英文摘要</summary>	The firing dynamics of biological neurons in mathematical models is often determined by the model's parameters, representing the neurons' underlying properties. The parameter estimation problem seeks to recover those parameters of a single neuron or a neuron population from their responses to external stimuli and interactions between themselves. Most common methods for tackling this problem in the literature use some mechanistic models in conjunction with either a simulation-based or solution-based optimization scheme. In this paper, we study an automatic approach of learning the parameters of neuron populations from a training set consisting of pairs of spiking series and parameter labels via supervised learning. Unlike previous work, this automatic learning does not require additional simulations at inference time nor expert knowledge in deriving an analytical solution or in constructing some approximate models. We simulate many neuronal populations with different parameter settings using a stochastic neuron model. Using that data, we train a variety of supervised machine learning models, including convolutional and deep neural networks, random forest, and support vector regression. We then compare their performance against classical approaches including a genetic search, Bayesian sequential estimation, and a random walk approximate model. The supervised models almost always outperform the classical methods in parameter estimation and spike reconstruction errors, and computation expense. Convolutional neural network, in particular, is the best among all models across all metrics. The supervised models can also generalize to out-of-distribution data to a certain extent. </details>
<details>	<summary>注释</summary>	31 pages </details>
<details>	<summary>邮件日期</summary>	2022年10月05日</details>

# 566、基于RISC-V工具链和敏捷开发的开源神经形态处理器
- [ ] RISC-V Toolchain and Agile Development based Open-source Neuromorphic Processor 
时间：2022年10月02日                         第一作者：Jiulong Wang                       [链接](https://arxiv.org/abs/2210.00562).                     
## 摘要：近几十年来，旨在模仿大脑行为的神经形态计算在计算机科学的各个领域得到了发展。人工神经网络（ANN）是人工智能（AI）中的一个重要概念。它用于识别和分类。为了探索一种更好的方法来在硬件上模拟获得的大脑行为，这种方法既快速又节能，研究人员需要一种先进的方法，如神经形态计算。在这种情况下，脉冲神经网络（SNN）成为硬件实现的最佳选择。最近的工作集中于加速SNN计算。然而，大多数加速器解决方案基于CPU加速器架构，由于该结构中的复杂控制流，该架构能量效率低下。本文提出了文曲星22A，一种低功耗的神经形态处理器，它结合了通用CPU功能和SNN，通过RISC-V SNN扩展指令有效地计算它。文曲星22A的主要思想是
<details>	<summary>英文摘要</summary>	In recent decades, neuromorphic computing aiming to imitate brains' behaviors has been developed in various fields of computer science. The Artificial Neural Network (ANN) is an important concept in Artificial Intelligence (AI). It is utilized in recognition and classification. To explore a better way to simulate obtained brain behaviors, which is fast and energy-efficient, on hardware, researchers need an advanced method such as neuromorphic computing. In this case, Spiking Neural Network (SNN) becomes an optimal choice in hardware implementation. Recent works are focusing on accelerating SNN computing. However, most accelerator solutions are based on CPU-accelerator architecture which is energy-inefficient due to the complex control flows in this structure. This paper proposes Wenquxing 22A, a low-power neuromorphic processor that combines general-purpose CPU functions and SNN to efficiently compute it with RISC-V SNN extension instructions. The main idea of Wenquxing 22A is to integrate the SNN calculation unit into the pipeline of a general-purpose CPU to achieve low-power computing with customized RISC-V SNN instructions version 1.0 (RV-SNN V1.0), Streamlined Leaky Integrate-and-Fire (LIF) model, and the binary stochastic Spike-timing-dependent-plasticity (STDP). The source code of Wenquxing 22A is released online on Gitee and GitHub. We apply Wenquxing 22A to the recognition of the MNIST dataset to make a comparison with other SNN systems. Our experiment results show that Wenquxing 22A improves the energy expenses by 5.13 times over the accelerator solution, ODIN, with approximately classification accuracy, 85.00% for 3-bit ODIN online learning, and 91.91% for 1-bit Wenquxing 22A. </details>
<details>	<summary>注释</summary>	6 pages, 5 figures, conference or other essential info ACM-class: B.2.0 </details>
<details>	<summary>邮件日期</summary>	2022年10月04日</details>

# 565、一种新的可解释的脉冲神经网络分布外检测方法
- [ ] A Novel Explainable Out-of-Distribution Detection Approach for Spiking Neural Networks 
时间：2022年09月30日                         第一作者：Aitor Martinez Seras                       [链接](https://arxiv.org/abs/2210.00894).                     
## 摘要：由于与传统神经网络相比，脉冲神经网络的优势，包括其高效处理和建模复杂时间动态的固有能力，过去几年中，围绕脉冲神经网的研究已经开始。尽管存在这些差异，但当在现实环境中部署时，Spiking神经网络面临着与其他神经计算同行类似的问题。这项工作解决了可能阻碍这一系列模型可信度的实际情况之一：使用远离其训练数据分布的样本（也称为分布外或OoD数据）查询训练模型的可能性。具体地说，这项工作提出了一种新的OoD检测器，它可以识别输入到脉冲神经网络的测试示例是否属于训练数据的分布。为此，我们以脉冲计数模式的形式描述了网络隐藏层的内部激活
<details>	<summary>英文摘要</summary>	Research around Spiking Neural Networks has ignited during the last years due to their advantages when compared to traditional neural networks, including their efficient processing and inherent ability to model complex temporal dynamics. Despite these differences, Spiking Neural Networks face similar issues than other neural computation counterparts when deployed in real-world settings. This work addresses one of the practical circumstances that can hinder the trustworthiness of this family of models: the possibility of querying a trained model with samples far from the distribution of its training data (also referred to as Out-of-Distribution or OoD data). Specifically, this work presents a novel OoD detector that can identify whether test examples input to a Spiking Neural Network belong to the distribution of the data over which it was trained. For this purpose, we characterize the internal activations of the hidden layers of the network in the form of spike count patterns, which lay a basis for determining when the activations induced by a test instance is atypical. Furthermore, a local explanation method is devised to produce attribution maps revealing which parts of the input instance push most towards the detection of an example as an OoD sample. Experimental results are performed over several image classification datasets to compare the proposed detector to other OoD detection schemes from the literature. As the obtained results clearly show, the proposed detector performs competitively against such alternative schemes, and produces relevance attribution maps that conform to expectations for synthetically created OoD instances. </details>
<details>	<summary>注释</summary>	37 pages, 10 figures, under review MSC-class: 68T07 ACM-class: I.2 </details>
<details>	<summary>邮件日期</summary>	2022年10月04日</details>

# 564、基于脉冲的局部突触可塑性：计算模型和神经形态回路综述
- [ ] Spike-based local synaptic plasticity: A survey of computational models and neuromorphic circuits 
时间：2022年09月30日                         第一作者：Lyes Khacef                       [链接](https://arxiv.org/abs/2209.15536).                     
## 摘要：了解生物神经网络如何利用基于脉冲的局部可塑性机制进行学习，可以开发出强大、节能和自适应的神经形态处理系统。根据不同的方法，最近提出了大量基于脉冲的学习模型。然而，很难评估它们是否以及如何映射到神经形态硬件上，也很难比较它们的特性和实现的容易性。为此，在本次调查中，我们在一个统一的框架内，全面概述了代表性的脑启发突触可塑性模型和混合信号CMOS神经形态电路。我们回顾了建模突触可塑性的历史、自下而上和自上而下方法，并确定了可以支持基于脉冲学习规则的低延迟和低功耗硬件实现的计算原语。我们提供了基于突触前和突触后神经元的局部性原则的共同定义
<details>	<summary>英文摘要</summary>	Understanding how biological neural networks carry out learning using spike-based local plasticity mechanisms can lead to the development of powerful, energy-efficient, and adaptive neuromorphic processing systems. A large number of spike-based learning models have recently been proposed following different approaches. However, it is difficult to assess if and how they could be mapped onto neuromorphic hardware, and to compare their features and ease of implementation. To this end, in this survey, we provide a comprehensive overview of representative brain-inspired synaptic plasticity models and mixed-signal \acs{CMOS} neuromorphic circuits within a unified framework. We review historical, bottom-up, and top-down approaches to modeling synaptic plasticity, and we identify computational primitives that can support low-latency and low-power hardware implementations of spike-based learning rules. We provide a common definition of a locality principle based on pre- and post-synaptic neuron information, which we propose as a fundamental requirement for physical implementations of synaptic plasticity. Based on this principle, we compare the properties of these models within the same framework, and describe the mixed-signal electronic circuits that implement their computing primitives, pointing out how these building blocks enable efficient on-chip and online learning in neuromorphic processing systems. </details>
<details>	<summary>邮件日期</summary>	2022年10月03日</details>

# 563、Spikformer：当Spiking神经网络遇到变压器时
- [ ] Spikformer: When Spiking Neural Network Meets Transformer 
时间：2022年09月29日                         第一作者：Zhaokun Zhou                       [链接](https://arxiv.org/abs/2209.15425).                     
## 摘要：我们考虑两种生物学上合理的结构，脉冲神经网络（SNN）和自我注意机制。前者为深度学习提供了一种能效和事件驱动的范例，而后者能够捕获特征依赖性，使Transformer能够实现良好的性能。直觉上，探索他们之间的婚姻是有希望的。在本文中，我们考虑利用SNN的自我注意能力和生物学特性，并提出了一种新的脉冲自我注意（SSA）以及一个强大的框架，称为脉冲变压器（Spikformer）。Spikformer中的SSA机制通过使用spik-form Query、Key和Value来建模稀疏视觉特征，而不使用softmax。由于它的计算是稀疏的并且避免了乘法，因此SSA是高效的并且具有低的计算能耗。结果表明，具有SSA的Spikformer在图像分类方面，无论是在神经形态分类还是在视觉识别方面，都能优于最先进的SNN类框架
<details>	<summary>英文摘要</summary>	We consider two biologically plausible structures, the Spiking Neural Network (SNN) and the self-attention mechanism. The former offers an energy-efficient and event-driven paradigm for deep learning, while the latter has the ability to capture feature dependencies, enabling Transformer to achieve good performance. It is intuitively promising to explore the marriage between them. In this paper, we consider leveraging both self-attention capability and biological properties of SNNs, and propose a novel Spiking Self Attention (SSA) as well as a powerful framework, named Spiking Transformer (Spikformer). The SSA mechanism in Spikformer models the sparse visual feature by using spike-form Query, Key, and Value without softmax. Since its computation is sparse and avoids multiplication, SSA is efficient and has low computational energy consumption. It is shown that Spikformer with SSA can outperform the state-of-the-art SNNs-like frameworks in image classification on both neuromorphic and static datasets. Spikformer (66.3M parameters) with comparable size to SEW-ResNet-152 (60.2M,69.26%) can achieve 74.81% top1 accuracy on ImageNet using 4 time steps, which is the state-of-the-art in directly trained SNNs models. </details>
<details>	<summary>邮件日期</summary>	2022年10月03日</details>

# 562、用DVS手势链评估神经网络对基于事件的动作识别的时间理解
- [ ] Evaluating the temporal understanding of neural networks on event-based action recognition with DVS-Gesture-Chain 
时间：2022年09月29日                         第一作者：Alex Vicente-Sola                       [链接](https://arxiv.org/abs/2209.14915).                     
## 摘要：为了实现对视频序列的完整感知，使人工神经网络（ANN）在视觉任务中具有时间理解是一项基本要求。当使用传统的基于帧的视频序列时，可以使用广泛的基准数据集来评估这种能力。相比之下，由于缺乏适当的数据集，针对神经形态数据的系统评估它们仍然是一个挑战。在这项工作中，我们为基于事件的视频序列中的动作识别定义了一个新的基准任务DVS手势链（DVS-GC），该任务基于广泛使用的DVS手势数据集中的多个手势的时间组合。这种方法允许创建在时间维度上任意复杂的数据集。使用我们新定义的任务，我们评估了不同前馈卷积神经网络和卷积脉冲神经网络（SNN）的时空理解。我们的研究证明了原始DVS
<details>	<summary>英文摘要</summary>	Enabling artificial neural networks (ANNs) to have temporal understanding in visual tasks is an essential requirement in order to achieve complete perception of video sequences. A wide range of benchmark datasets is available to allow for the evaluation of such capabilities when using conventional frame-based video sequences. In contrast, evaluating them for systems targeting neuromorphic data is still a challenge due to the lack of appropriate datasets. In this work we define a new benchmark task for action recognition in event-based video sequences, DVS-Gesture-Chain (DVS-GC), which is based on the temporal combination of multiple gestures from the widely used DVS-Gesture dataset. This methodology allows to create datasets that are arbitrarily complex in the temporal dimension. Using our newly defined task, we evaluate the spatio-temporal understanding of different feed-forward convolutional ANNs and convolutional Spiking Neural Networks (SNNs). Our study proves how the original DVS Gesture benchmark could be solved by networks without temporal understanding, unlike the new DVS-GC which demands an understanding of the ordering of events. From there, we provide a study showing how certain elements such as spiking neurons or time-dependent weights allow for temporal understanding in feed-forward networks without the need for recurrent connections. Code available at: https://github.com/VicenteAlex/DVS-Gesture-Chain </details>
<details>	<summary>邮件日期</summary>	2022年09月30日</details>

# 561、注意力脉冲神经网络
- [ ] Attention Spiking Neural Networks 
时间：2022年09月28日                         第一作者：Man Yao                       [链接](https://arxiv.org/abs/2209.13929).                     
## 摘要：得益于大脑的事件驱动和稀疏脉冲特性，脉冲神经网络（SNN）正成为人工神经网络（ANN）的节能替代品。然而，长期以来，SNN和ANN之间的性能差距一直是广泛部署SNN的巨大障碍。为了充分利用SNN的潜力，我们研究了SNN中注意机制的影响。我们首先用一种即插即用的工具，即多维注意力（MA）来展示我们的注意力概念。然后，提出了一种新的具有端到端训练的注意力SNN架构，称为“MA-SNN”，该架构分别或同时沿时间、信道和空间维度推断注意力权重。基于现有的神经科学理论，我们利用注意力权重来优化膜电位，从而以数据依赖的方式调节脉冲反应。以可忽略的附加参数为代价，MA促进了普通SNN的交流
<details>	<summary>英文摘要</summary>	Benefiting from the event-driven and sparse spiking characteristics of the brain, spiking neural networks (SNNs) are becoming an energy-efficient alternative to artificial neural networks (ANNs). However, the performance gap between SNNs and ANNs has been a great hindrance to deploying SNNs ubiquitously for a long time. To leverage the full potential of SNNs, we study the effect of attention mechanisms in SNNs. We first present our idea of attention with a plug-and-play kit, termed the Multi-dimensional Attention (MA). Then, a new attention SNN architecture with end-to-end training called "MA-SNN" is proposed, which infers attention weights along the temporal, channel, as well as spatial dimensions separately or simultaneously. Based on the existing neuroscience theories, we exploit the attention weights to optimize membrane potentials, which in turn regulate the spiking response in a data-dependent way. At the cost of negligible additional parameters, MA facilitates vanilla SNNs to achieve sparser spiking activity, better performance, and energy efficiency concurrently. Experiments are conducted in event-based DVS128 Gesture/Gait action recognition and ImageNet-1k image classification. On Gesture/Gait, the spike counts are reduced by 84.9%/81.6%, and the task accuracy and energy efficiency are improved by 5.9%/4.7% and 3.4$\times$/3.2$\times$. On ImageNet-1K, we achieve top-1 accuracy of 75.92% and 77.08% on single/4-step Res-SNN-104, which are state-of-the-art results in SNNs. To our best knowledge, this is for the first time, that the SNN community achieves comparable or even better performance compared with its ANN counterpart in the large-scale dataset. Our work lights up SNN's potential as a general backbone to support various applications for SNNs, with a great balance between effectiveness and efficiency. </details>
<details>	<summary>注释</summary>	18 pages, 8 figures, Under Review </details>
<details>	<summary>邮件日期</summary>	2022年09月29日</details>

# 560、脉冲SiamFC++：用于目标跟踪的深度脉冲神经网络
- [ ] Spiking SiamFC++: Deep Spiking Neural Network for Object Tracking 
时间：2022年09月24日                         第一作者：Shuiying Xiang                       [链接](https://arxiv.org/abs/2209.12010).                     
## 摘要：脉冲神经网络（SNN）是一种生物学上合理的模型，具有高计算能力和低功耗的优点。然而，深度SNN的训练仍然是一个开放的问题，这限制了深度SNN在现实世界中的应用。在这里，我们提出了一种名为Spiking SiamFC++的深度SNN架构，用于通过端到端直接训练进行目标跟踪。具体地，在时域中扩展AlexNet网络以提取特征，并采用替代梯度函数来实现深度SNN的直接监督训练。为了检查脉冲SiamFC++的性能，考虑了几个跟踪基准，包括OTB2013、OTB2015、VOT2015、VOT2016和UAV123。结果表明，与原始SiamFC++相比，精度损失较小。与现有的基于SNN的目标跟踪器（例如SiamSNN）相比，所提出的脉冲SiamFC++的精度（连续性）达到85.24%（64.37%），远高于52.78%(
<details>	<summary>英文摘要</summary>	Spiking neural network (SNN) is a biologically-plausible model and exhibits advantages of high computational capability and low power consumption. While the training of deep SNN is still an open problem, which limits the real-world applications of deep SNN. Here we propose a deep SNN architecture named Spiking SiamFC++ for object tracking with end-to-end direct training. Specifically, the AlexNet network is extended in the time domain to extract the feature, and the surrogate gradient function is adopted to realize direct supervised training of the deep SNN. To examine the performance of the Spiking SiamFC++, several tracking benchmarks including OTB2013, OTB2015, VOT2015, VOT2016, and UAV123 are considered. It is found that, the precision loss is small compared with the original SiamFC++. Compared with the existing SNN-based target tracker, e.g., the SiamSNN, the precision (succession) of the proposed Spiking SiamFC++ reaches 85.24% (64.37%), which is much higher than that of 52.78% (44.32%) achieved by the SiamSNN. To our best knowledge, the performance of the Spiking SiamFC++ outperforms the existing state-of-the-art approaches in SNN-based object tracking, which provides a novel path for SNN application in the field of target tracking. This work may further promote the development of SNN algorithms and neuromorphic chips. </details>
<details>	<summary>邮件日期</summary>	2022年09月27日</details>

# 559、求解神经元模型逆问题的物理约束神经网络
- [ ] Physically constrained neural networks to solve the inverse problem for neuron models 
时间：2022年09月24日                         第一作者：Matteo Ferrante                       [链接](https://arxiv.org/abs/2209.11998).                     
## 摘要：特别是系统生物学和系统神经生理学最近已成为生物医学科学中许多关键应用的有力工具。尽管如此，此类模型通常基于多尺度（以及可能的多物理）策略的复杂组合，这些策略需要特殊的计算策略，并提出极高的计算要求。深度神经网络领域的最新发展表明，与传统模型相比，建立非线性、通用逼近器来估计高度非线性和复杂问题的解具有显著的速度和精度优势。在合成数据验证后，我们使用所谓的物理约束神经网络（PINN）来同时求解生物学上合理的霍奇金-赫胥黎模型，并在可变和恒定电流刺激下从真实数据中推断其参数和隐藏时间过程，证明了脉冲的变异性极低
<details>	<summary>英文摘要</summary>	Systems biology and systems neurophysiology in particular have recently emerged as powerful tools for a number of key applications in the biomedical sciences. Nevertheless, such models are often based on complex combinations of multiscale (and possibly multiphysics) strategies that require ad hoc computational strategies and pose extremely high computational demands. Recent developments in the field of deep neural networks have demonstrated the possibility of formulating nonlinear, universal approximators to estimate solutions to highly nonlinear and complex problems with significant speed and accuracy advantages in comparison with traditional models. After synthetic data validation, we use so-called physically constrained neural networks (PINN) to simultaneously solve the biologically plausible Hodgkin-Huxley model and infer its parameters and hidden time-courses from real data under both variable and constant current stimulation, demonstrating extremely low variability across spikes and faithful signal reconstruction. The parameter ranges we obtain are also compatible with prior knowledge. We demonstrate that detailed biological knowledge can be provided to a neural network, making it able to fit complex dynamics over both simulated and real data. </details>
<details>	<summary>邮件日期</summary>	2022年09月27日</details>

# 558、神经形态综合传感与通信
- [ ] Neuromorphic Integrated Sensing and Communications 
时间：2022年09月24日                         第一作者：Jiechen Chen                       [链接](https://arxiv.org/abs/2209.11891).                     
## 摘要：神经形态计算是一种新兴技术，它为需要高效在线推理和/或控制的应用程序支持事件驱动的数据处理。最近的工作引入了神经形态通信的概念，其中神经形态计算与脉冲无线电（IR）传输集成，以在无线物联网网络中实现低能量和低延迟的远程推断。在本文中，我们介绍了神经形态集成传感和通信（N-ISAC），这是一种能够实现高效在线数据解码和雷达传感的新解决方案。N-ISAC利用公共IR波形来实现传输数字信息和检测雷达目标的存在或不存在的双重目的。在接收器处部署脉冲神经网络（SNN），以解码数字数据并直接使用接收到的信号检测雷达目标。通过平衡数据通信和雷达传感的性能指标，突出协同作用和跟踪能力，优化了SNN操作
<details>	<summary>英文摘要</summary>	Neuromorphic computing is an emerging technology that support event-driven data processing for applications requiring efficient online inference and/or control. Recent work has introduced the concept of neuromorphic communications, whereby neuromorphic computing is integrated with impulse radio (IR) transmission to implement low-energy and low-latency remote inference in wireless IoT networks. In this paper, we introduce neuromorphic integrated sensing and communications (N-ISAC), a novel solution that enables efficient online data decoding and radar sensing. N-ISAC leverages a common IR waveform for the dual purpose of conveying digital information and of detecting the presence or absence of a radar target. A spiking neural network (SNN) is deployed at the receiver to decode digital data and detect the radar target using directly the received signal. The SNN operation is optimized by balancing performance metric for data communications and radar sensing, highlighting synergies and trade-offs between the two applications. </details>
<details>	<summary>注释</summary>	Submitted </details>
<details>	<summary>邮件日期</summary>	2022年09月27日</details>

# 557、Spiking神经网络的空间-通道-时间融合注意
- [ ] A Spatial-channel-temporal-fused Attention for Spiking Neural Networks 
时间：2022年09月22日                         第一作者：Wuque Cai                       [链接](https://arxiv.org/abs/2209.10837).                     
## 摘要：脉冲神经网络（SNN）模仿大脑的计算策略，并在时空信息处理方面表现出强大的能力。视觉注意作为人类感知的一个基本因素，是指生物视觉系统中显著区域的动态选择过程。尽管视觉注意机制在计算机视觉中取得了巨大成功，但很少被引入SNN。受预测性注意力重映射的实验观察启发，我们在此提出了一种新的空间通道-时间融合注意力（SCTFA）模块，该模块可以引导SNN通过利用历史积累的空间通道信息来有效捕获潜在的目标区域。通过对三个事件流数据集（DVS手势、SL动物DVS和MNIST-DVS）的系统评估，我们证明了具有SCTFA模块的SNN（SCTFA-SNN）不仅显著优于基线SNN（BL-SNN）和具有退化注意力模型的其他两个SNN模型
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) mimic brain computational strategies, and exhibit substantial capabilities in spatiotemporal information processing. As an essential factor for human perception, visual attention refers to the dynamic selection process of salient regions in biological vision systems. Although mechanisms of visual attention have achieved great success in computer vision, they are rarely introduced into SNNs. Inspired by experimental observations on predictive attentional remapping, we here propose a new spatial-channel-temporal-fused attention (SCTFA) module that can guide SNNs to efficiently capture underlying target regions by utilizing historically accumulated spatial-channel information. Through a systematic evaluation on three event stream datasets (DVS Gesture, SL-Animals-DVS and MNIST-DVS), we demonstrate that the SNN with the SCTFA module (SCTFA-SNN) not only significantly outperforms the baseline SNN (BL-SNN) and other two SNN models with degenerated attention modules, but also achieves competitive accuracy with existing state-of-the-art methods. Additionally, our detailed analysis shows that the proposed SCTFA-SNN model has strong robustness to noise and outstanding stability to incomplete data, while maintaining acceptable complexity and efficiency. Overall, these findings indicate that appropriately incorporating cognitive mechanisms of the brain may provide a promising approach to elevate the capability of SNNs. </details>
<details>	<summary>注释</summary>	12 pages, 8 figures, 5 tabes; This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible </details>
<details>	<summary>邮件日期</summary>	2022年09月23日</details>

# 556、用于强化学习的群体编码和动态神经元改进的Spiking Actor网络
- [ ] Population-coding and Dynamic-neurons improved Spiking Actor Network for Reinforcement Learning 
时间：2022年09月22日                         第一作者：Duzhen Zhang                       [链接](https://arxiv.org/abs/2106.07854).                     
<details>	<summary>邮件日期</summary>	2022年09月23日</details>

# 555、自适应SpikeNet：使用具有可学习神经动力学的Spiking神经网络进行基于事件的光流估计
- [ ] Adaptive-SpikeNet: Event-based Optical Flow Estimation using Spiking Neural Networks with Learnable Neuronal Dynamics 
时间：2022年09月21日                         第一作者：Adarsh Kumar Kosta                        [链接](https://arxiv.org/abs/2209.11741).                     
## 摘要：基于事件的摄像机由于其异步捕获时间丰富的信息的能力，最近显示出高速运动估计的巨大潜力。脉冲神经网络（SNN）以其神经启发的事件驱动处理可以有效地处理此类异步数据，而泄漏积分和激发（LIF）等神经元模型可以跟踪输入中包含的典型时序信息。SNN通过在神经元记忆中保持动态状态，保留重要信息，同时随着时间的推移忘记冗余数据来实现这一点。因此，我们假设，与类似大小的模拟神经网络（ANN）相比，SNN将允许在序列回归任务中具有更好的性能。然而，深层SNN很难训练，因为后期的脉冲会消失。为此，我们提出了一种具有可学习神经元动力学的自适应完全脉冲框架，以缓解脉冲消失问题。我们使用基于替代梯度的backpro
<details>	<summary>英文摘要</summary>	Event-based cameras have recently shown great potential for high-speed motion estimation owing to their ability to capture temporally rich information asynchronously. Spiking Neural Networks (SNNs), with their neuro-inspired event-driven processing can efficiently handle such asynchronous data, while neuron models such as the leaky-integrate and fire (LIF) can keep track of the quintessential timing information contained in the inputs. SNNs achieve this by maintaining a dynamic state in the neuron memory, retaining important information while forgetting redundant data over time. Thus, we posit that SNNs would allow for better performance on sequential regression tasks compared to similarly sized Analog Neural Networks (ANNs). However, deep SNNs are difficult to train due to vanishing spikes at later layers. To that effect, we propose an adaptive fully-spiking framework with learnable neuronal dynamics to alleviate the spike vanishing problem. We utilize surrogate gradient-based backpropagation through time (BPTT) to train our deep SNNs from scratch. We validate our approach for the task of optical flow estimation on the Multi-Vehicle Stereo Event-Camera (MVSEC) dataset and the DSEC-Flow dataset. Our experiments on these datasets show an average reduction of 13% in average endpoint error (AEE) compared to state-of-the-art ANNs. We also explore several down-scaled models and observe that our SNN models consistently outperform similarly sized ANNs offering 10%-16% lower AEE. These results demonstrate the importance of SNNs for smaller models and their suitability at the edge. In terms of efficiency, our SNNs offer substantial savings in network parameters (48x) and computational energy (51x) while attaining ~10% lower EPE compared to the state-of-the-art ANN implementations. </details>
<details>	<summary>邮件日期</summary>	2022年09月26日</details>

# 554、学习发电机模型的稀疏潜在表示
- [ ] Learning Sparse Latent Representations for Generator Model 
时间：2022年09月20日                         第一作者：Hanao Li                       [链接](https://arxiv.org/abs/2209.09949).                     
## 摘要：稀疏是一个理想的属性。与密集模型相比，它可以导致更高效和更有效的表示。同时，由于稀疏潜在表示的复杂性，学习稀疏潜在表示一直是计算机视觉和机器学习领域的一个挑战性问题。在本文中，我们提出了一种新的无监督学习方法，以增强生成器模型的潜在空间上的稀疏性，并将逐渐稀疏的脉冲和板条分布作为我们的先验。我们的模型仅由一个自上而下的生成器网络组成，它将潜在变量映射到观测数据。使用基于非持久梯度的方法，可以根据生成器后向推断潜在变量。推理步骤中的Spike和Slab正则化可以将非信息潜在维度推向零，从而导致稀疏性。广泛的实验表明，该模型可以保留原始图像中的大部分信息，并具有稀疏表示，同时显示了改进的结果
<details>	<summary>英文摘要</summary>	Sparsity is a desirable attribute. It can lead to more efficient and more effective representations compared to the dense model. Meanwhile, learning sparse latent representations has been a challenging problem in the field of computer vision and machine learning due to its complexity. In this paper, we present a new unsupervised learning method to enforce sparsity on the latent space for the generator model with a gradually sparsified spike and slab distribution as our prior. Our model consists of only one top-down generator network that maps the latent variable to the observed data. Latent variables can be inferred following generator posterior direction using non-persistent gradient based method. Spike and Slab regularization in the inference step can push non-informative latent dimensions towards zero to induce sparsity. Extensive experiments show the model can preserve majority of the information from original images with sparse representations while demonstrating improved results compared to other existing methods. We observe that our model can learn disentangled semantics and increase explainability of the latent codes while boosting the robustness in the task of classification and denoising. </details>
<details>	<summary>邮件日期</summary>	2022年09月22日</details>

# 553、一种学习马尔可夫链的脉冲神经网络
- [ ] A Spiking Neural Network Learning Markov Chain 
时间：2022年09月20日                         第一作者：Mikhail Kiselev                       [链接](https://arxiv.org/abs/2209.09572).                     
## 摘要：本文探讨了脉冲神经网络（SNN）如何在其内部结构中学习和固定外部世界动力学模型的问题。这个问题对于基于模型的强化学习（RL）的实施很重要，这是一种现实的强化学习机制，在该机制中，SNN做出的决策及其在奖惩信号方面的评估可能会被重要的时间间隔和中间评估中立世界状态的序列分开。在目前的工作中，我将世界动力学形式化为具有未知先验状态转移概率的马尔可夫链，这应该由网络学习。为了使这个问题公式更现实，我在连续时间内解决它，这样马尔可夫链中每个状态的持续时间可能是不同的，并且是未知的。它证明了如何通过具有特殊设计结构和局部突触可塑性规则的SNN来完成这一任务。作为一个例子，我们展示了这个网络主题如何在简单但没有
<details>	<summary>英文摘要</summary>	In this paper, the question how spiking neural network (SNN) learns and fixes in its internal structures a model of external world dynamics is explored. This question is important for implementation of the model-based reinforcement learning (RL), the realistic RL regime where the decisions made by SNN and their evaluation in terms of reward/punishment signals may be separated by significant time interval and sequence of intermediate evaluation-neutral world states. In the present work, I formalize world dynamics as a Markov chain with unknown a priori state transition probabilities, which should be learnt by the network. To make this problem formulation more realistic, I solve it in continuous time, so that duration of every state in the Markov chain may be different and is unknown. It is demonstrated how this task can be accomplished by an SNN with specially designed structure and local synaptic plasticity rules. As an example, we show how this network motif works in the simple but non-trivial world where a ball moves inside a square box and bounces from its walls with a random new direction and velocity. </details>
<details>	<summary>邮件日期</summary>	2022年09月21日</details>

# 552、脉冲神经网络的突触阈值协同学习方法
- [ ] A Synapse-Threshold Synergistic Learning Approach for Spiking Neural Networks 
时间：2022年09月20日                         第一作者：Hongze Sun                       [链接](https://arxiv.org/abs/2206.06129).                     
<details>	<summary>注释</summary>	13 pages, 9 figures, submitted for publication </details>
<details>	<summary>邮件日期</summary>	2022年09月21日</details>

# 551、紧凑、区域特定和正则化脉冲神经网络的集成用于可扩展位置识别
- [ ] Ensembles of Compact, Region-specific & Regularized Spiking Neural Networks for Scalable Place Recognition 
时间：2022年09月19日                         第一作者：Somayeh Hussaini                       [链接](https://arxiv.org/abs/2209.08723).                     
## 摘要：脉冲神经网络由于其在专用硬件上的高能效，在机器人技术中具有重要的潜在用途，但概念验证实现通常还没有实现与传统方法相比具有竞争力的性能或能力。在本文中，我们通过引入一种新的模块化集成网络方法来解决可扩展性的关键实际挑战之一，在这种方法中，紧凑的局部脉冲网络每个都学习并仅负责识别环境的局部区域中的位置。这种模块化方法创建了一个高度可扩展的系统。然而，它带来了高性能的代价，在部署时缺乏全局规则化会导致过度活跃的神经元错误地对学习区域以外的地方做出反应。我们的第二个贡献介绍了一种正则化方法，在初始环境学习阶段检测并去除这些有问题的过度活跃神经元。我们评估了这个新的sca
<details>	<summary>英文摘要</summary>	Spiking neural networks have significant potential utility in robotics due to their high energy efficiency on specialized hardware, but proof-of-concept implementations have not yet typically achieved competitive performance or capability with conventional approaches. In this paper, we tackle one of the key practical challenges of scalability by introducing a novel modular ensemble network approach, where compact, localized spiking networks each learn and are solely responsible for recognizing places in a local region of the environment only. This modular approach creates a highly scalable system. However, it comes with a high-performance cost where a lack of global regularization at deployment time leads to hyperactive neurons that erroneously respond to places outside their learned region. Our second contribution introduces a regularization approach that detects and removes these problematic hyperactive neurons during the initial environmental learning phase. We evaluate this new scalable modular system on benchmark localization datasets Nordland and Oxford RobotCar, with comparisons to both standard techniques NetVLAD and SAD, and a previous spiking neural network system. Our system substantially outperforms the previous SNN system on its small dataset, but also maintains performance on 27 times larger benchmark datasets where the operation of the previous system is computationally infeasible, and performs competitively with the conventional localization systems. </details>
<details>	<summary>注释</summary>	8 pages, 6 figures, under review </details>
<details>	<summary>邮件日期</summary>	2022年09月20日</details>

# 550、SpikeSEE:一种用于视网膜修复的能效动态场景处理框架
- [ ] SpikeSEE: An Energy-Efficient Dynamic Scenes Processing Framework for Retinal Prostheses 
时间：2022年09月16日                         第一作者：Chuanqing Wang                       [链接](https://arxiv.org/abs/2209.07898).                     
## 摘要：在这个时代，智能和低功耗的视网膜假体被高度要求，可穿戴和可植入设备被用于许多医疗应用。在本文中，我们提出了一种节能的动态场景处理框架（SpikeSEE），该框架结合了脉冲表示编码技术和仿生脉冲递归神经网络（SRNN）模型，以实现视网膜假体的智能处理和极低功耗计算。脉冲表示编码技术可以解释具有稀疏脉冲序列的动态场景，从而减少数据量。受人类视网膜特殊结构和棘波处理方法启发，采用SRNN模型预测神经节细胞对动态场景的反应。实验结果表明，所提出的SRNN模型的皮尔逊相关系数达到0.93，这优于视网膜假体的现有技术处理框架。由于脉冲表示和SRNN
<details>	<summary>英文摘要</summary>	Intelligent and low-power retinal prostheses are highly demanded in this era, where wearable and implantable devices are used for numerous healthcare applications. In this paper, we propose an energy-efficient dynamic scenes processing framework (SpikeSEE) that combines a spike representation encoding technique and a bio-inspired spiking recurrent neural network (SRNN) model to achieve intelligent processing and extreme low-power computation for retinal prostheses. The spike representation encoding technique could interpret dynamic scenes with sparse spike trains, decreasing the data volume. The SRNN model, inspired by the human retina special structure and spike processing method, is adopted to predict the response of ganglion cells to dynamic scenes. Experimental results show that the Pearson correlation coefficient of the proposed SRNN model achieves 0.93, which outperforms the state of the art processing framework for retinal prostheses. Thanks to the spike representation and SRNN processing, the model can extract visual features in a multiplication-free fashion. The framework achieves 12 times power reduction compared with the convolutional recurrent neural network (CRNN) processing-based framework. Our proposed SpikeSEE predicts the response of ganglion cells more accurately with lower energy consumption, which alleviates the precision and power issues of retinal prostheses and provides a potential solution for wearable or implantable prostheses. </details>
<details>	<summary>邮件日期</summary>	2022年09月19日</details>

# 549、神经形态硬件系统的自修复
- [ ] Astromorphic Self-Repair of Neuromorphic Hardware Systems 
时间：2022年09月15日                         第一作者：Zhuangyu Han                       [链接](https://arxiv.org/abs/2209.07428).                     
## 摘要：尽管基于脉冲神经网络（SNN）的神经形态计算体系结构越来越受到人们的关注，作为一种通往生物似是而非的机器学习的途径，但人们的注意力仍然集中在神经元和突触等计算单元上。从神经突触的角度出发，本文试图探索神经胶质细胞，特别是星形胶质细胞的自我修复作用。这项工作研究了与星形胶质细胞计算神经科学模型的更强相关性，以开发具有更高生物保真度的宏观模型，准确捕捉自我修复过程的动态行为。硬件-软件协同设计分析表明，生物形态星形细胞调节具有自我修复神经形态硬件系统中硬件现实故障的潜力，对于MNIST和F-MNIST数据集上的无监督学习任务，具有显著更好的准确性和修复收敛性。
<details>	<summary>英文摘要</summary>	While neuromorphic computing architectures based on Spiking Neural Networks (SNNs) are increasingly gaining interest as a pathway toward bio-plausible machine learning, attention is still focused on computational units like the neuron and synapse. Shifting from this neuro-synaptic perspective, this paper attempts to explore the self-repair role of glial cells, in particular, astrocytes. The work investigates stronger correlations with astrocyte computational neuroscience models to develop macro-models with a higher degree of bio-fidelity that accurately captures the dynamic behavior of the self-repair process. Hardware-software co-design analysis reveals that bio-morphic astrocytic regulation has the potential to self-repair hardware realistic faults in neuromorphic hardware systems with significantly better accuracy and repair convergence for unsupervised learning tasks on the MNIST and F-MNIST datasets. </details>
<details>	<summary>邮件日期</summary>	2022年09月16日</details>

# 548、神经模型的新方法示意图
- [ ] Sketch of a novel approach to a neural model 
时间：2022年09月14日                         第一作者：Gabriele Scheler                       [链接](https://arxiv.org/abs/2209.06865).                     
## 摘要：在本文中，我们提出了一种新的神经可塑性模型，其形式是神经加工的水平-垂直集成模型。我们相信，一种新的神经建模方法将有利于第三波人工智能。水平面由一个由传输链路连接的自适应神经元网络组成，该网络产生时空脉冲模式。这符合标准的计算神经科学方法。此外，对于每个单独的神经元，都有一个由内部自适应参数组成的垂直部分，该内部自适应参数控制与神经传递有关的外部膜表达参数。每个神经元都有一个垂直的模块化参数系统，这些参数对应于（a）膜层的外部参数，分为隔室（棘、骨）（b）膜下带和细胞质中的内部参数及其蛋白信号网络，以及（c）细胞核中的遗传和表观遗传信息的核心参数。在这种模型中，每个节点（=神经
<details>	<summary>英文摘要</summary>	In this paper, we lay out a novel model of neuroplasticity in the form of a horizontal-vertical integration model of neural processing. We believe a new approach to neural modeling will benefit the 3rd wave of AI. The horizontal plane consists of an adaptive network of neurons connected by transmission links which generates spatio-temporal spike patterns. This fits with standard computational neuroscience approaches. Additionally for each individual neuron there is a vertical part consisting of internal adaptive parameters steering the external membrane-expressed parameters which are involved in neural transmission. Each neuron has a vertical modular system of parameters corresponding to (a) external parameters at the membrane layer, divided into compartments (spines, boutons) (b) internal parameters in the submembrane zone and the cytoplasm with its protein signaling network and (c) core parameters in the nucleus for genetic and epigenetic information. In such models, each node (=neuron) in the horizontal network has its own internal memory. Neural transmission and information storage are systematically separated, an important conceptual advance over synaptic weight models. We discuss the membrane-based (external) filtering and selection of outside signals for processing vs. signal loss by fast fluctuations and the neuron-internal computing strategies from intracellular protein signaling to the nucleus as the core system. We want to show that the individual neuron has an important role in the computation of signals and that many assumptions derived from the synaptic weight adjustment hypothesis of memory may not hold in a real brain. Not every transmission event leaves a trace and the neuron is a self-programming device, rather than passively determined by current input. Ultimately we strive to build a flexible memory system that processes facts and events automatically. </details>
<details>	<summary>邮件日期</summary>	2022年09月16日</details>

# 547、脉冲GATs：通过脉冲神经网络学习图形注意力
- [ ] Spiking GATs: Learning Graph Attentions via Spiking Neural Network 
时间：2022年09月05日                         第一作者：Beibei Wang                        [链接](https://arxiv.org/abs/2209.13539).                     
## 摘要：图形注意力网络（GAT）已被深入研究并广泛应用于图形数据学习任务中。现有的GAT通常采用自注意机制来进行图边缘注意学习，需要昂贵的计算。已知脉冲神经网络（SNN）可以通过将输入信号数据传输成离散脉冲序列来执行廉价的计算，并且还可以返回稀疏输出。受SNN优点的启发，在这项工作中，我们提出了一种用于图数据表示和学习的新型图脉冲注意力网络（GSAT）。与现有GAT中的自我关注机制相比，所提出的GSAT采用了明显节能的SNN模块架构。此外，GSAT可以返回自然的稀疏关注系数，从而可以对选择性邻居执行特征聚合，这使得GSAT能够针对图边缘噪声执行鲁棒性。在多个数据集上的实验结果表明
<details>	<summary>英文摘要</summary>	Graph Attention Networks (GATs) have been intensively studied and widely used in graph data learning tasks. Existing GATs generally adopt the self-attention mechanism to conduct graph edge attention learning, requiring expensive computation. It is known that Spiking Neural Networks (SNNs) can perform inexpensive computation by transmitting the input signal data into discrete spike trains and can also return sparse outputs. Inspired by the merits of SNNs, in this work, we propose a novel Graph Spiking Attention Network (GSAT) for graph data representation and learning. In contrast to self-attention mechanism in existing GATs, the proposed GSAT adopts a SNN module architecture which is obvious energy-efficient. Moreover, GSAT can return sparse attention coefficients in natural and thus can perform feature aggregation on the selective neighbors which makes GSAT perform robustly w.r.t graph edge noises. Experimental results on several datasets demonstrate the effectiveness, energy efficiency and robustness of the proposed GSAT model. </details>
<details>	<summary>邮件日期</summary>	2022年09月28日</details>

# 546、基于事件学习的时域和极域数据增强
- [ ] Data Augmentation in Temporal and Polar Domains for Event-Based Learning 
时间：2022年07月24日                         第一作者：Haibo Shen                       [链接](None).                     
<details>	<summary>注释</summary>	7+2pages, 6figures License: http://creativecommons.org/licenses/by/4.0/ </details>
<details>	<summary>邮件日期</summary>	2022年09月14日</details>

# 545、基于事件学习的时域和极域数据增强
- [ ] Data Augmentation in Temporal and Polar Domains for Event-Based Learning 
时间：2022年07月24日                         第一作者：Haibo Shen                       [链接](None).                     
<details>	<summary>注释</summary>	7+2pages, 6figures License: http://creativecommons.org/licenses/by/4.0/ </details>
<details>	<summary>邮件日期</summary>	2022年09月13日</details>

# 544、确保脉冲：脉冲神经网络对对抗性示例的可转移性和安全性
- [ ] Securing the Spike: On the Transferabilty and Security of Spiking Neural Networks to Adversarial Examples 
时间：2022年09月07日                         第一作者：Nuo Xu                       [链接](https://arxiv.org/abs/2209.03358).                     
## 摘要：脉冲神经网络（SNN）由于其高能量效率和分类性能的最新进展而引起了广泛关注。然而，与传统的深度学习方法不同，SNN对对抗性示例的鲁棒性的分析和研究仍然相对不足。在这项工作中，我们通过实验和分析三个重要的SNN安全属性，推进了对抗性机器学习领域。首先，我们表明，对SNN的成功白盒对抗性攻击高度依赖于潜在的替代梯度技术。其次，我们分析了SNN和其他最先进的架构（如视觉变换器和大传输CNN）生成的对抗性示例的可传输性。我们证明，SNN通常不会被视觉变换器和某些类型的CNN生成的对抗性示例所欺骗。最后，我们开发了一种新的白盒攻击，它可以生成能够愚弄的对手示例
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have attracted much attention for their high energy efficiency and for recent advances in their classification performance. However, unlike traditional deep learning approaches, the analysis and study of the robustness of SNNs to adversarial examples remains relatively underdeveloped. In this work we advance the field of adversarial machine learning through experimentation and analyses of three important SNN security attributes. First, we show that successful white-box adversarial attacks on SNNs are highly dependent on the underlying surrogate gradient technique. Second, we analyze the transferability of adversarial examples generated by SNNs and other state-of-the-art architectures like Vision Transformers and Big Transfer CNNs. We demonstrate that SNNs are not often deceived by adversarial examples generated by Vision Transformers and certain types of CNNs. Lastly, we develop a novel white-box attack that generates adversarial examples capable of fooling both SNN models and non-SNN models simultaneously. Our experiments and analyses are broad and rigorous covering two datasets (CIFAR-10 and CIFAR-100), five different white-box attacks and twelve different classifier models. </details>
<details>	<summary>邮件日期</summary>	2022年09月09日</details>

# 543、制造脉冲网络：健壮的类大脑无监督机器学习
- [ ] Making a Spiking Net Work: Robust brain-like unsupervised machine learning 
时间：2022年09月01日                         第一作者：Peter G. Stratton                       [链接](https://arxiv.org/abs/2208.01204).                     
<details>	<summary>注释</summary>	12 pages (manuscript), 5 figures, 10 pages (appendix), 11 pages (extended data) </details>
<details>	<summary>邮件日期</summary>	2022年09月02日</details>

# 542、基于脉冲神经网络的贝叶斯连续学习
- [ ] Bayesian Continual Learning via Spiking Neural Networks 
时间：2022年08月29日                         第一作者：Nicolas Skatchkovsky                       [链接](https://arxiv.org/abs/2208.13723).                     
## 摘要：生物智能的主要特征包括能源效率、持续适应能力以及通过不确定性量化进行风险管理。迄今为止，神经形态工程主要是由实现节能机器的目标驱动的，这些机器的灵感来自生物大脑的基于时间的计算范式。在本文中，我们采取步骤设计能够适应变化学习任务的神经形态系统，同时产生校准良好的不确定性量化估计。为此，我们在贝叶斯连续学习框架内导出了脉冲神经网络（SNN）的在线学习规则。在该模型中，每个突触权重由参数表示，这些参数量化了由先验知识和观察数据产生的当前认知不确定性。所提出的在线规则在观察到数据时以流方式更新分布参数。我们为实际值和
<details>	<summary>英文摘要</summary>	Among the main features of biological intelligence are energy efficiency, capacity for continual adaptation, and risk management via uncertainty quantification. Neuromorphic engineering has been thus far mostly driven by the goal of implementing energy-efficient machines that take inspiration from the time-based computing paradigm of biological brains. In this paper, we take steps towards the design of neuromorphic systems that are capable of adaptation to changing learning tasks, while producing well-calibrated uncertainty quantification estimates. To this end, we derive online learning rules for spiking neural networks (SNNs) within a Bayesian continual learning framework. In it, each synaptic weight is represented by parameters that quantify the current epistemic uncertainty resulting from prior knowledge and observed data. The proposed online rules update the distribution parameters in a streaming fashion as data are observed. We instantiate the proposed approach for both real-valued and binary synaptic weights. Experimental results using Intel's Lava platform show the merits of Bayesian over frequentist learning in terms of capacity for adaptation and uncertainty quantification. </details>
<details>	<summary>注释</summary>	Submitted for journal publication </details>
<details>	<summary>邮件日期</summary>	2022年08月30日</details>

# 541、Spike摄像机的不确定性引导深度融合
- [ ] Uncertainty Guided Depth Fusion for Spike Camera 
时间：2022年08月29日                         第一作者：Jianing Li                       [链接](https://arxiv.org/abs/2208.12653).                     
<details>	<summary>注释</summary>	18 pages, 11 figures ACM-class: I.2.10 </details>
<details>	<summary>邮件日期</summary>	2022年08月30日</details>

# 540、可伸缩纳米光子电子脉冲神经网络
- [ ] Scalable Nanophotonic-Electronic Spiking Neural Networks 
时间：2022年08月28日                         第一作者：Luis El Srouji                       [链接](https://arxiv.org/abs/2208.13144).                     
## 摘要：脉冲神经网络（SNN）提供了一种能够高度并行化、实时处理的新计算范式。光子器件是设计与SNN计算范式相匹配的高带宽并行架构的理想选择。CMOS和光子元件的共集成允许低损耗光子器件与模拟电子器件相结合，以提高非线性计算元件的灵活性。因此，我们设计并模拟了单片硅光子学（SiPh）工艺上的光电脉冲神经元电路，该工艺复制了漏积分和激发（LIF）之外的有用脉冲行为。此外，我们还探索了两种具有片上学习潜力的学习算法，使用Mach-Zehnder干涉（MZI）网格作为突触互连。随机反向传播（RPB）的变化在芯片上进行了实验演示，并与简单分类任务的标准线性回归的性能相匹配。同时，对比
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNN) provide a new computational paradigm capable of highly parallelized, real-time processing. Photonic devices are ideal for the design of high-bandwidth, parallel architectures matching the SNN computational paradigm. Co-integration of CMOS and photonic elements allow low-loss photonic devices to be combined with analog electronics for greater flexibility of nonlinear computational elements. As such, we designed and simulated an optoelectronic spiking neuron circuit on a monolithic silicon photonics (SiPh) process that replicates useful spiking behaviors beyond the leaky integrate-and-fire (LIF). Additionally, we explored two learning algorithms with the potential for on-chip learning using Mach-Zehnder Interferometric (MZI) meshes as synaptic interconnects. A variation of Random Backpropagation (RPB) was experimentally demonstrated on-chip and matched the performance of a standard linear regression on a simple classification task. Meanwhile, the Contrastive Hebbian Learning (CHL) rule was applied to a simulated neural network composed of MZI meshes for a random input-output mapping task. The CHL-trained MZI network performed better than random guessing but does not match the performance of the ideal neural network (without the constraints imposed by the MZI meshes). Through these efforts, we demonstrate that co-integrated CMOS and SiPh technologies are well-suited to the design of scalable SNN computing architectures. </details>
<details>	<summary>邮件日期</summary>	2022年08月30日</details>

# 539、使用Rockpool和Xylo的Sub-mW神经形态SNN音频处理应用
- [ ] Sub-mW Neuromorphic SNN audio processing applications with Rockpool and Xylo 
时间：2022年08月27日                         第一作者：Hannah Bos                        [链接](https://arxiv.org/abs/2208.12991).                     
## 摘要：脉冲神经网络（SNN）为时间信号处理提供了一种有效的计算机制，特别是当与低功率SNN推理ASIC耦合时。SNN在历史上很难配置，缺乏为任意任务找到解决方案的通用方法。近年来，梯度下降优化方法越来越容易地应用于SNN。因此，SNN和SNN推理处理器为能源受限环境中的商业低功耗信号处理提供了良好的平台，而无需依赖云。然而，到目前为止，行业中的ML工程师还无法使用这些方法，需要研究生级别的培训才能成功配置单个SNN应用程序。在这里，我们演示了一种方便的高级流水线，用于设计、训练和部署任意时间信号处理应用程序到子mW SNN推理硬件。我们采用了一种新的直接SNN架构，用于时间信号处理
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) provide an efficient computational mechanism for temporal signal processing, especially when coupled with low-power SNN inference ASICs. SNNs have been historically difficult to configure, lacking a general method for finding solutions for arbitrary tasks. In recent years, gradient-descent optimization methods have been applied to SNNs with increasing ease. SNNs and SNN inference processors therefore offer a good platform for commercial low-power signal processing in energy constrained environments without cloud dependencies. However, to date these methods have not been accessible to ML engineers in industry, requiring graduate-level training to successfully configure a single SNN application. Here we demonstrate a convenient high-level pipeline to design, train and deploy arbitrary temporal signal processing applications to sub-mW SNN inference hardware. We apply a new straightforward SNN architecture designed for temporal signal processing, using a pyramid of synaptic time constants to extract signal features at a range of temporal scales. We demonstrate this architecture on an ambient audio classification task, deployed to the Xylo SNN inference processor in streaming mode. Our application achieves high accuracy (98%) and low latency (100ms) at low power (<4muW inference power). Our approach makes training and deploying SNN applications available to ML engineers with general NN backgrounds, without requiring specific prior experience with spiking NNs. We intend for our approach to make Neuromorphic hardware and SNNs an attractive choice for commercial low-power and edge signal processing applications. </details>
<details>	<summary>邮件日期</summary>	2022年08月30日</details>

# 538、基于共振网络的神经形态视觉场景理解
- [ ] Neuromorphic Visual Scene Understanding with Resonator Networks 
时间：2022年08月26日                         第一作者：Alpha Renner                       [链接](https://arxiv.org/abs/2208.12880).                     
## 摘要：推断对象的位置及其刚性变换仍然是视觉场景理解中的一个开放问题。在这里，我们提出了一种神经形态解，该解利用基于三个关键概念的有效因子分解网络：（1）基于具有复值向量的向量符号架构（VSA）的计算框架；（2） 设计分层谐振器网络（HRN），以处理视觉场景中平移和旋转的非交换性质，当两者结合使用时；（3） 用于在神经形态硬件上实现复值向量绑定的多室脉冲相量神经元模型的设计。VSA框架使用向量绑定操作生成生成图像模型，其中绑定充当几何变换的等变操作。因此，场景可以被描述为向量乘积的和，而向量乘积又可以被谐振器网络有效地分解以推断对象及其姿态。
<details>	<summary>英文摘要</summary>	Inferring the position of objects and their rigid transformations is still an open problem in visual scene understanding. Here we propose a neuromorphic solution that utilizes an efficient factorization network which is based on three key concepts: (1) a computational framework based on Vector Symbolic Architectures (VSA) with complex-valued vectors; (2) the design of Hierarchical Resonator Networks (HRN) to deal with the non-commutative nature of translation and rotation in visual scenes, when both are used in combination; (3) the design of a multi-compartment spiking phasor neuron model for implementing complex-valued vector binding on neuromorphic hardware. The VSA framework uses vector binding operations to produce generative image models in which binding acts as the equivariant operation for geometric transformations. A scene can therefore be described as a sum of vector products, which in turn can be efficiently factorized by a resonator network to infer objects and their poses. The HRN enables the definition of a partitioned architecture in which vector binding is equivariant for horizontal and vertical translation within one partition, and for rotation and scaling within the other partition. The spiking neuron model allows to map the resonator network onto efficient and low-power neuromorphic hardware. In this work, we demonstrate our approach using synthetic scenes composed of simple 2D shapes undergoing rigid geometric transformations and color changes. A companion paper demonstrates this approach in real-world application scenarios for machine vision and robotics. </details>
<details>	<summary>注释</summary>	15 pages, 6 figures ACM-class: I.4.8 </details>
<details>	<summary>邮件日期</summary>	2022年08月30日</details>

# 537、基于跨模态跨领域知识转移的无监督脉冲深度估计
- [ ] Unsupervised Spike Depth Estimation via Cross-modality Cross-domain Knowledge Transfer 
时间：2022年08月26日                         第一作者：Jiaming Liu                       [链接](https://arxiv.org/abs/2208.12527).                     
## 摘要：神经形态spike摄像机以生物启发的方式生成具有高时间分辨率的数据流，在自动驾驶等现实世界应用中具有巨大潜力。与RGB流相比，脉冲流具有克服运动模糊的固有优势，从而为高速对象提供更精确的深度估计。然而，以有监督的方式训练脉冲深度估计网络几乎是不可能的，因为对于时间密集的脉冲流获得成对的深度标签是极其困难和具有挑战性的。在本文中，我们从开源RGB数据集（如KITTI）转移知识，并以无监督的方式估计脉冲深度，而不是构建具有全深度标签的脉冲流数据集。这类问题的关键挑战在于RGB和棘波模式之间的模态间隙，以及标记源RGB和未标记目标棘波域之间的域间隙。为了克服这些挑战，我们引入了cross-m
<details>	<summary>英文摘要</summary>	The neuromorphic spike camera generates data streams with high temporal resolution in a bio-inspired way, which has vast potential in the real-world applications such as autonomous driving. In contrast to RGB streams, spike streams have an inherent advantage to overcome motion blur, leading to more accurate depth estimation for high-velocity objects. However, training the spike depth estimation network in a supervised manner is almost impossible since it is extremely laborious and challenging to obtain paired depth labels for temporally intensive spike streams. In this paper, instead of building a spike stream dataset with full depth labels, we transfer knowledge from the open-source RGB datasets (e.g., KITTI) and estimate spike depth in an unsupervised manner. The key challenges for such problem lie in the modality gap between RGB and spike modalities, and the domain gap between labeled source RGB and unlabeled target spike domains. To overcome these challenges, we introduce a cross-modality cross-domain (BiCross) framework for unsupervised spike depth estimation. Our method narrows the enormous gap between source RGB and target spike by introducing the mediate simulated source spike domain. To be specific, for the cross-modality phase, we propose a novel Coarse-to-Fine Knowledge Distillation (CFKD), which transfers the image and pixel level knowledge from source RGB to source spike. Such design leverages the abundant semantic and dense temporal information of RGB and spike modalities respectively. For the cross-domain phase, we introduce the Uncertainty Guided Mean-Teacher (UGMT) to generate reliable pseudo labels with uncertainty estimation, alleviating the shift between the source spike and target spike domains. Besides, we propose a Global-Level Feature Alignment method (GLFA) to align the feature between two domains and generate more reliable pseudo labels. </details>
<details>	<summary>邮件日期</summary>	2022年08月29日</details>

# 536、Spike摄像机的不确定性引导深度融合
- [ ] Uncertainty Guided Depth Fusion for Spike Camera 
时间：2022年08月26日                         第一作者：Jianing Li                       [链接](https://arxiv.org/abs/2208.12653).                     
## 摘要：深度估计对于各种重要的实际应用（如自动驾驶）至关重要。然而，由于传统摄像机只能捕获模糊图像，因此在高速场景下，它的性能会严重下降。为了解决这个问题，spike摄像机被设计为在高帧速率下捕获像素级亮度强度。然而，使用基于光度一致性的传统单目或立体深度估计算法，使用spike相机进行深度估计仍然非常具有挑战性。在本文中，我们提出了一种新的不确定性引导深度融合（UGDF）框架，用于融合单目和立体深度估计网络的预测。我们的框架是基于这样一个事实，即立体脉冲深度估计在近距离获得更好的结果，而单目脉冲深度估算在远距离获得更好的效果。因此，我们引入了一种具有联合训练的双任务深度估计架构
<details>	<summary>英文摘要</summary>	Depth estimation is essential for various important real-world applications such as autonomous driving. However, it suffers from severe performance degradation in high-velocity scenario since traditional cameras can only capture blurred images. To deal with this problem, the spike camera is designed to capture the pixel-wise luminance intensity at high frame rate. However, depth estimation with spike camera remains very challenging using traditional monocular or stereo depth estimation algorithms, which are based on the photometric consistency. In this paper, we propose a novel Uncertainty-Guided Depth Fusion (UGDF) framework to fuse the predictions of monocular and stereo depth estimation networks for spike camera. Our framework is motivated by the fact that stereo spike depth estimation achieves better results at close range while monocular spike depth estimation obtains better results at long range. Therefore, we introduce a dual-task depth estimation architecture with a joint training strategy and estimate the distributed uncertainty to fuse the monocular and stereo results. In order to demonstrate the advantage of spike depth estimation over traditional camera depth estimation, we contribute a spike-depth dataset named CitySpike20K, which contains 20K paired samples, for spike depth estimation. UGDF achieves state-of-the-art results on CitySpike20K, surpassing all monocular or stereo spike depth estimation baselines. We conduct extensive experiments to evaluate the effectiveness and generalization of our method on CitySpike20K. To the best of our knowledge, our framework is the first dual-task fusion framework for spike camera depth estimation. Code and dataset will be released. </details>
<details>	<summary>注释</summary>	18 pages, 11 figures, submitted to AAAI 2023 ACM-class: I.2.10 </details>
<details>	<summary>邮件日期</summary>	2022年08月29日</details>

# 535、用于时域模拟脉冲神经网络的基于CMOS的面积和功率高效神经元和突触电路
- [ ] CMOS-based area-and-power-efficient neuron and synapse circuits for time-domain analog spiking neural networks 
时间：2022年08月25日                         第一作者：Xiangyu Chen                       [链接](https://arxiv.org/abs/2208.11881).                     
## 摘要：传统的神经结构倾向于通过模拟量（例如电流或电压）进行通信，然而，随着CMOS器件的缩小和电源电压的降低，电压/电流域模拟电路的动态范围变得更窄，可用裕度变得更小，并且噪声抗扰度降低。除此之外，在传统设计中使用运算放大器（运算放大器）和时钟或异步比较器导致高能耗和大芯片面积，这将不利于建立脉冲神经网络。鉴于此，我们提出了一种用于生成和传输时域信号的神经结构，包括神经元模块、突触模块和两个权重模块。所提出的神经结构由晶体管三极管区域中的漏电流驱动，并且不使用运算放大器和比较器，因此与传统设计相比提供了更高的能量和面积效率。此外，该结构提供了更大的抗噪声能力
<details>	<summary>英文摘要</summary>	Conventional neural structures tend to communicate through analog quantities such as currents or voltages, however, as CMOS devices shrink and supply voltages decrease, the dynamic range of voltage/current-domain analog circuits becomes narrower, the available margin becomes smaller, and noise immunity decreases. More than that, the use of operational amplifiers (op-amps) and clocked or asynchronous comparators in conventional designs leads to high energy consumption and large chip area, which would be detrimental to building spiking neural networks. In view of this, we propose a neural structure for generating and transmitting time-domain signals, including a neuron module, a synapse module, and two weight modules. The proposed neural structure is driven by leakage currents in the transistor triode region and does not use op-amps and comparators, thus providing higher energy and area efficiency compared to conventional designs. In addition, the structure provides greater noise immunity due to internal communication via time-domain signals, which simplifies the wiring between the modules. The proposed neural structure is fabricated using TSMC 65 nm CMOS technology. The proposed neuron and synapse occupy an area of 127 um2 and 231 um2, respectively, while achieving millisecond time constants. Actual chip measurements show that the proposed structure successfully implements the temporal signal communication function with millisecond time constants, which is a critical step toward hardware reservoir computing for human-computer interaction. </details>
<details>	<summary>邮件日期</summary>	2022年08月26日</details>

# 534、通过直接训练的深度脉冲Q网络实现人的水平控制
- [ ] Human-Level Control through Directly-Trained Deep Spiking Q-Networks 
时间：2022年08月25日                         第一作者：Guisong Liu                       [链接](https://arxiv.org/abs/2201.07211).                     
<details>	<summary>邮件日期</summary>	2022年08月26日</details>

# 533、估算：一个28nm亚平方毫米任务不可知脉冲循环神经网络处理器，支持在秒长时间尺度上进行片上学习
- [ ] ReckOn: A 28nm Sub-mm2 Task-Agnostic Spiking Recurrent Neural Network Processor Enabling On-Chip Learning over Second-Long Timescales 
时间：2022年08月20日                         第一作者：Charlotte Frenkel                       [链接](https://arxiv.org/abs/2208.09759).                     
## 摘要：自主边缘设备的强大现实部署需要片上自适应，以适应用户、环境和任务引起的变化。由于芯片内存限制，先前的学习设备仅限于没有时间内容的静态刺激。我们提出了一个0.45毫米$^2$的峰值RNN处理器，使任务无关的在线学习在几秒钟内实现，我们演示了在0.8%的内存开销和<150-$\mu$W的训练功率预算内进行导航、手势识别和关键字识别。
<details>	<summary>英文摘要</summary>	A robust real-world deployment of autonomous edge devices requires on-chip adaptation to user-, environment- and task-induced variability. Due to on-chip memory constraints, prior learning devices were limited to static stimuli with no temporal contents. We propose a 0.45-mm$^2$ spiking RNN processor enabling task-agnostic online learning over seconds, which we demonstrate for navigation, gesture recognition, and keyword spotting within a 0.8-% memory overhead and a <150-$\mu$W training power budget. </details>
<details>	<summary>注释</summary>	Published in the 2022 IEEE International Solid-State Circuits Conference (ISSCC), 2022 DOI: 10.1109/ISSCC42614.2022.9731734 </details>
<details>	<summary>邮件日期</summary>	2022年08月23日</details>

# 532、基于脉冲神经网络的相干伊辛机组合优化求解
- [ ] Combinatorial optimization solving by coherent Ising machines based on spiking neural networks 
时间：2022年08月16日                         第一作者：Bo Lu                       [链接](https://arxiv.org/abs/2208.07502).                     
## 摘要：脉冲神经网络是一种神经形态计算，被认为可以提高智能水平，为量子计算提供优势。在这项工作中，我们通过设计一个光学脉冲神经网络来解决这个问题，并证明它可以用来加快计算速度，特别是在组合优化问题上。这里，脉冲神经网络由反对称耦合简并光学参量振荡器脉冲和耗散脉冲构成。根据脉冲神经元的动态行为，选择非线性传递函数来缓解振幅不均匀性并使产生的局部极小值不稳定。数值结果表明，脉冲神经网络相干伊辛机在组合优化问题上具有优异的性能，有望为神经计算和光学计算提供新的应用。
<details>	<summary>英文摘要</summary>	Spiking neural network is a kind of neuromorphic computing which is believed to improve on the level of intelligence and provide advabtages for quantum computing. In this work, we address this issue by designing an optical spiking neural network and prove that it can be used to accelerate the speed of computation, especially on the combinatorial optimization problems. Here the spiking neural network is constructed by the antisymmetrically coupled degenerate optical parametric oscillator pulses and dissipative pulses. A nonlinear transfer function is chosen to mitigate amplitude inhomogeneities and destabilize the resulting local minima according to the dynamical behavior of spiking neurons. It is numerically proved that the spiking neural network-coherent Ising machines has excellent performance on combinatorial optimization problems, for which is expected to offer a new applications for neural computing and optical computing. </details>
<details>	<summary>注释</summary>	5 pages, 4 figures, comments are welcome </details>
<details>	<summary>邮件日期</summary>	2022年08月17日</details>

# 531、通过脉冲神经网络扩展动态图表示学习
- [ ] Scaling Up Dynamic Graph Representation Learning via Spiking Neural Networks 
时间：2022年08月15日                         第一作者：Jintang Li                       [链接](https://arxiv.org/abs/2208.10364).                     
## 摘要：近年来，动态图表示学习的研究出现了激增，其目的是对动态且随时间不断演化的时态图建模。然而，当前的工作通常使用递归神经网络（RNN）来建模图动态，这使得它们在大型时态图上的计算和内存开销严重。到目前为止，大型时态图的动态图表示学习的可扩展性仍然是主要挑战之一。在本文中，我们提出了一个可扩展的框架，即SpikeNet，以有效地捕获时态图的时态和结构模式。我们探索了一个新的方向，即我们可以用脉冲神经网络（SNN）而不是RNN来捕捉时态图的演化动态。作为RNN的低功耗替代方案，SNN明确地将图动力学建模为神经元种群的脉冲序列，并以有效的方式实现基于脉冲的传播。三个大型实时图数据集的实验
<details>	<summary>英文摘要</summary>	Recent years have seen a surge in research on dynamic graph representation learning, which aims to model temporal graphs that are dynamic and evolving constantly over time. However, current work typically models graph dynamics with recurrent neural networks (RNNs), making them suffer seriously from computation and memory overheads on large temporal graphs. So far, scalability of dynamic graph representation learning on large temporal graphs remains one of the major challenges. In this paper, we present a scalable framework, namely SpikeNet, to efficiently capture the temporal and structural patterns of temporal graphs. We explore a new direction in that we can capture the evolving dynamics of temporal graphs with spiking neural networks (SNNs) instead of RNNs. As a low-power alternative to RNNs, SNNs explicitly model graph dynamics as spike trains of neuron populations and enable spike-based propagation in an efficient way. Experiments on three large real-world temporal graph datasets demonstrate that SpikeNet outperforms strong baselines on the temporal node classification task with lower computational costs. Particularly, SpikeNet generalizes to a large temporal graph (2M nodes and 13M edges) with significantly fewer parameters and computation overheads. Our code is publicly available at https://github.com/EdisonLeeeee/SpikeNet </details>
<details>	<summary>注释</summary>	Preprint; Code available at https://github.com/EdisonLeeeee/SpikeNet </details>
<details>	<summary>邮件日期</summary>	2022年08月23日</details>

# 530、使用虚拟神经元在神经形态计算机上编码整数和有理数
- [ ] Encoding Integers and Rationals on Neuromorphic Computers using Virtual Neuron 
时间：2022年08月15日                         第一作者：Prasanna Date                       [链接](https://arxiv.org/abs/2208.07468).                     
## 摘要：神经形态计算机通过模拟人脑进行计算，并使用极低的功耗。预计它们在未来的节能计算中是不可或缺的。虽然它们主要用于基于脉冲神经网络的机器学习应用，但已知神经形态计算机是图灵完备的，因此能够进行通用计算。然而，为了充分实现其通用、节能计算的潜力，设计高效的数字编码机制非常重要。当前的编码方法具有有限的适用性，并且可能不适合于通用计算。在本文中，我们提出了虚拟神经元作为整数和有理数的编码机制。我们评估了虚拟神经元在物理和模拟神经形态硬件上的性能，并表明它可以使用基于混合信号忆阻器的神经形态过程平均使用23 nJ的能量执行加法运算
<details>	<summary>英文摘要</summary>	Neuromorphic computers perform computations by emulating the human brain, and use extremely low power. They are expected to be indispensable for energy-efficient computing in the future. While they are primarily used in spiking neural network-based machine learning applications, neuromorphic computers are known to be Turing-complete, and thus, capable of general-purpose computation. However, to fully realize their potential for general-purpose, energy-efficient computing, it is important to devise efficient mechanisms for encoding numbers. Current encoding approaches have limited applicability and may not be suitable for general-purpose computation. In this paper, we present the virtual neuron as an encoding mechanism for integers and rational numbers. We evaluate the performance of the virtual neuron on physical and simulated neuromorphic hardware and show that it can perform an addition operation using 23 nJ of energy on average using a mixed-signal memristor-based neuromorphic processor. We also demonstrate its utility by using it in some of the mu-recursive functions, which are the building blocks of general-purpose computation. </details>
<details>	<summary>邮件日期</summary>	2022年08月17日</details>

# 529、利用脑电图检测预期脑电位的卷积脉冲神经网络
- [ ] Convolutional Spiking Neural Networks for Detecting Anticipatory Brain Potentials Using Electroencephalogram 
时间：2022年08月14日                         第一作者：Nathan Lutes                       [链接](https://arxiv.org/abs/2208.06900).                     
## 摘要：脉冲神经网络（SNN）作为一种开发“生物学上合理的”机器学习模型的手段，正受到越来越多的关注。这些网络模拟人脑中的突触连接并产生脉冲序列，可以用二进制值近似，从而避免了浮点运算电路的高计算成本。最近，引入了卷积层，将卷积网络的特征提取能力与SNN的计算效率结合起来。本文研究了使用卷积脉冲神经网络（CSNN）作为分类器，使用脑电图（EEG）检测与人类参与者制动意图相关的预期慢皮层电位的可行性。EEG数据是在一项实验中收集的，其中参与者在设计用于模拟城市环境的试验台上操作遥控车辆。通过音频提醒参与者即将发生的制动事件
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are receiving increased attention as a means to develop "biologically plausible" machine learning models. These networks mimic synaptic connections in the human brain and produce spike trains, which can be approximated by binary values, precluding high computational cost with floating-point arithmetic circuits. Recently, the addition of convolutional layers to combine the feature extraction power of convolutional networks with the computational efficiency of SNNs has been introduced. In this paper, the feasibility of using a convolutional spiking neural network (CSNN) as a classifier to detect anticipatory slow cortical potentials related to braking intention in human participants using an electroencephalogram (EEG) was studied. The EEG data was collected during an experiment wherein participants operated a remote controlled vehicle on a testbed designed to simulate an urban environment. Participants were alerted to an incoming braking event via an audio countdown to elicit anticipatory potentials that were then measured using an EEG. The CSNN's performance was compared to a standard convolutional neural network (CNN) and three graph neural networks (GNNs) via 10-fold cross-validation. The results showed that the CSNN outperformed the other neural networks. </details>
<details>	<summary>注释</summary>	10 pages, 5 figures, IEEE transaction on Neural Networks submission </details>
<details>	<summary>邮件日期</summary>	2022年08月16日</details>

# 528、一种用于能量有效的深度脉冲神经网络处理器设计的时间到第一脉冲编码和转换感知训练
- [ ] A Time-to-first-spike Coding and Conversion Aware Training for Energy-Efficient Deep Spiking Neural Network Processor Design 
时间：2022年08月09日                         第一作者：Dongwoo Lew                       [链接](https://arxiv.org/abs/2208.04494).                     
## 摘要：在本文中，我们提出了一种能量高效的SNN架构，它可以无缝运行深度脉冲神经网络（SNN），并提高精度。首先，我们提出了一种转换感知训练（CAT），以减少ANN到SNN的转换损失，而无需硬件实现开销。在所提出的CAT中，有效地利用了为模拟ANN训练期间的SNN而开发的激活函数，以减少转换后的数据表示误差。基于CAT技术，我们还提出了一种时间到第一脉冲编码，该编码允许利用脉冲时间信息进行轻量级对数计算。支持所提出技术的SNN处理器设计已使用28nm CMOS工艺实现。当运行具有5位对数权重的VGG-16时，处理器分别以486.7uJ、503.6uJ和1426uJ的推理能量处理CIFAR-10、CIFAR-100和Tiny ImageNet，达到了91.7%、67.9%和57.4%的顶级精度。
<details>	<summary>英文摘要</summary>	In this paper, we present an energy-efficient SNN architecture, which can seamlessly run deep spiking neural networks (SNNs) with improved accuracy. First, we propose a conversion aware training (CAT) to reduce ANN-to-SNN conversion loss without hardware implementation overhead. In the proposed CAT, the activation function developed for simulating SNN during ANN training, is efficiently exploited to reduce the data representation error after conversion. Based on the CAT technique, we also present a time-to-first-spike coding that allows lightweight logarithmic computation by utilizing spike time information. The SNN processor design that supports the proposed techniques has been implemented using 28nm CMOS process. The processor achieves the top-1 accuracies of 91.7%, 67.9% and 57.4% with inference energy of 486.7uJ, 503.6uJ, and 1426uJ to process CIFAR-10, CIFAR-100, and Tiny-ImageNet, respectively, when running VGG-16 with 5bit logarithmic weights. </details>
<details>	<summary>注释</summary>	Accepted to Design Automation Conference 2022 DOI: 10.1145/3489517.3530457 </details>
<details>	<summary>邮件日期</summary>	2022年08月10日</details>

# 527、使用自动编码器消除感应电机噪音
- [ ] Denoising Induction Motor Sounds Using an Autoencoder 
时间：2022年08月08日                         第一作者：Thanh Tran                       [链接](https://arxiv.org/abs/2208.04462).                     
## 摘要：去噪是在改善声音信号的质量和充分性的同时从声音信号中去除噪声的过程。声音去噪在语音处理、声音事件分类和机器故障检测系统中有许多应用。本文描述了一种创建自动编码器的方法，用于将有噪声的机器声音映射到干净的声音，以达到去噪目的。声音中有几种类型的噪声，例如环境噪声和信号处理方法产生的频率相关噪声。环境活动产生的噪声为环境噪声。在工厂内，车辆、钻井、在测量区域工作或谈话的人员、风和流水会产生环境噪声。这些噪音在录音中表现为脉冲。在本文的范围内，我们以感应电机的水槽水龙头噪声为例，演示了高斯分布产生的噪声和环境噪声的去除
<details>	<summary>英文摘要</summary>	Denoising is the process of removing noise from sound signals while improving the quality and adequacy of the sound signals. Denoising sound has many applications in speech processing, sound events classification, and machine failure detection systems. This paper describes a method for creating an autoencoder to map noisy machine sounds to clean sounds for denoising purposes. There are several types of noise in sounds, for example, environmental noise and generated frequency-dependent noise from signal processing methods. Noise generated by environmental activities is environmental noise. In the factory, environmental noise can be created by vehicles, drilling, people working or talking in the survey area, wind, and flowing water. Those noises appear as spikes in the sound record. In the scope of this paper, we demonstrate the removal of generated noise with Gaussian distribution and the environmental noise with a specific example of the water sink faucet noise from the induction motor sounds. The proposed method was trained and verified on 49 normal function sounds and 197 horizontal misalignment fault sounds from the Machinery Fault Database (MAFAULDA). The mean square error (MSE) was used as the assessment criteria to evaluate the similarity between denoised sounds using the proposed autoencoder and the original sounds in the test set. The MSE is below or equal to 0.14 when denoise both types of noises on 15 testing sounds of the normal function category. The MSE is below or equal to 0.15 when denoising 60 testing sounds on the horizontal misalignment fault category. The low MSE shows that both the generated Gaussian noise and the environmental noise were almost removed from the original sounds with the proposed trained autoencoder. </details>
<details>	<summary>注释</summary>	9 pages, 10 figures, conference </details>
<details>	<summary>邮件日期</summary>	2022年08月10日</details>

# 526、用于数据流连续学习的脉冲神经预测编码
- [ ] Spiking Neural Predictive Coding for Continual Learning from Data Streams 
时间：2022年08月08日                         第一作者：Alex                       [链接](https://arxiv.org/abs/1908.08655).                     
<details>	<summary>注释</summary>	Newest revised version of manuscript </details>
<details>	<summary>邮件日期</summary>	2022年08月09日</details>

# 525、带脉冲神经网络的神经符号计算
- [ ] Neuro-symbolic computing with spiking neural networks 
时间：2022年08月04日                         第一作者：Dominik Dold                       [链接](https://arxiv.org/abs/2208.02576).                     
## 摘要：知识图是一种表现力强、使用广泛的数据结构，因为它们能够以合理和机器可读的方式集成来自不同领域的数据。因此，它们可以用于模拟各种系统，如分子和社交网络。然而，如何在脉冲系统中实现符号推理，以及如何将脉冲神经网络应用于此类图形数据，仍然是一个悬而未决的问题。在这里，我们通过演示如何使用脉冲神经元对符号和多关系信息进行编码，扩展了先前关于基于脉冲的图算法的工作，允许使用脉冲神经网络对符号结构（如知识图）进行推理。引入的框架是通过结合图嵌入范式和使用误差反向传播训练脉冲神经网络的最新进展而实现的。所提出的方法适用于各种脉冲神经元模型，并可与其他方法相结合进行端到端训练
<details>	<summary>英文摘要</summary>	Knowledge graphs are an expressive and widely used data structure due to their ability to integrate data from different domains in a sensible and machine-readable way. Thus, they can be used to model a variety of systems such as molecules and social networks. However, it still remains an open question how symbolic reasoning could be realized in spiking systems and, therefore, how spiking neural networks could be applied to such graph data. Here, we extend previous work on spike-based graph algorithms by demonstrating how symbolic and multi-relational information can be encoded using spiking neurons, allowing reasoning over symbolic structures like knowledge graphs with spiking neural networks. The introduced framework is enabled by combining the graph embedding paradigm and the recent progress in training spiking neural networks using error backpropagation. The presented methods are applicable to a variety of spiking neuron models and can be trained end-to-end in combination with other differentiable network architectures, which we demonstrate by implementing a spiking relational graph neural network. </details>
<details>	<summary>注释</summary>	Accepted for publication at the International Conference on Neuromorphic Systems (ICONS) 2022 </details>
<details>	<summary>邮件日期</summary>	2022年08月05日</details>

# 524、使用CycleGAN和随机生成的数据集进行黑白轮廓图像的样式转换
- [ ] Style Transfer of Black and White Silhouette Images using CycleGAN and a Randomly Generated Dataset 
时间：2022年08月03日                         第一作者：Worasait Suwannik                       [链接](https://arxiv.org/abs/2208.04140).                     
## 摘要：CycleGAN可用于将艺术风格转换为图像。它不需要成对的源图像和样式化图像来训练模型。利用这一优势，我们建议使用随机生成的数据来训练机器学习模型，该模型可以将传统艺术风格转换为黑白轮廓图像。该结果明显优于先前的神经类型转移方法。然而，还有一些方面需要改进，例如从变换图像中去除伪影和脉冲。
<details>	<summary>英文摘要</summary>	CycleGAN can be used to transfer an artistic style to an image. It does not require pairs of source and stylized images to train a model. Taking this advantage, we propose using randomly generated data to train a machine learning model that can transfer traditional art style to a black and white silhouette image. The result is noticeably better than the previous neural style transfer methods. However, there are some areas for improvement, such as removing artifacts and spikes from the transformed image. </details>
<details>	<summary>邮件日期</summary>	2022年08月09日</details>

# 523、LaneSNNs：用于Loihi神经形态处理器上车道检测的脉冲神经网络
- [ ] LaneSNNs: Spiking Neural Networks for Lane Detection on the Loihi Neuromorphic Processor 
时间：2022年08月03日                         第一作者：Alberto Viale                        [链接](https://arxiv.org/abs/2208.02253).                     
## 摘要：与自动驾驶（AD）相关的功能是下一代移动机器人和自动车辆的重要组成部分，其重点是日益智能、自主和互联的系统。根据定义，涉及使用这些功能的应用程序必须提供实时决策，而这一特性是避免灾难性事故的关键。此外，所有决策过程必须要求低功耗，以增加电池驱动系统的寿命和自主性。这些挑战可以通过在神经形态芯片上有效实现脉冲神经网络（SNN）和使用基于事件的摄像机而不是传统的基于帧的摄像机来解决。在本文中，我们提出了一种新的基于SNN的方法，称为LaneSNN，用于使用基于事件的摄像机输入检测街道上标记的车道。我们开发了四种具有低复杂度和快速响应特性的新型SNN模型，并使用离线监督学习对其进行训练
<details>	<summary>英文摘要</summary>	Autonomous Driving (AD) related features represent important elements for the next generation of mobile robots and autonomous vehicles focused on increasingly intelligent, autonomous, and interconnected systems. The applications involving the use of these features must provide, by definition, real-time decisions, and this property is key to avoid catastrophic accidents. Moreover, all the decision processes must require low power consumption, to increase the lifetime and autonomy of battery-driven systems. These challenges can be addressed through efficient implementations of Spiking Neural Networks (SNNs) on Neuromorphic Chips and the use of event-based cameras instead of traditional frame-based cameras. In this paper, we present a new SNN-based approach, called LaneSNN, for detecting the lanes marked on the streets using the event-based camera input. We develop four novel SNN models characterized by low complexity and fast response, and train them using an offline supervised learning rule. Afterward, we implement and map the learned SNNs models onto the Intel Loihi Neuromorphic Research Chip. For the loss function, we develop a novel method based on the linear composition of Weighted binary Cross Entropy (WCE) and Mean Squared Error (MSE) measures. Our experimental results show a maximum Intersection over Union (IoU) measure of about 0.62 and very low power consumption of about 1 W. The best IoU is achieved with an SNN implementation that occupies only 36 neurocores on the Loihi processor while providing a low latency of less than 8 ms to recognize an image, thereby enabling real-time performance. The IoU measures provided by our networks are comparable with the state-of-the-art, but at a much low power consumption of 1 W. </details>
<details>	<summary>注释</summary>	To appear at the 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2022) </details>
<details>	<summary>邮件日期</summary>	2022年08月05日</details>

# 522、制造脉冲网络：健壮的类大脑无监督机器学习
- [ ] Making a Spiking Net Work: Robust brain-like unsupervised machine learning 
时间：2022年08月02日                         第一作者：Peter G. Stratton                       [链接](https://arxiv.org/abs/2208.01204).                     
## 摘要：在过去十年中，人工智能（AI）的兴趣激增几乎完全是由人工神经网络（ANN）的进步推动的。虽然人工神经网络为许多以前难以解决的问题提供了最先进的性能，但它们需要大量的数据和计算资源用于训练，并且由于它们采用监督学习，它们通常需要知道每个训练示例的正确标记响应，从而限制了它们在现实领域的可扩展性。脉冲神经网络（SNN）是神经网络的一种替代方案，它使用更多类似大脑的人工神经元，并且可以使用无监督学习来发现输入数据中的可识别特征，而无需知道正确的响应。然而，SNN难以保持动态稳定性，无法与ANN的精度相匹配。在这里，我们展示了SNN如何克服文献中确定的许多缺点，包括为消失脉冲问题提供一个原则性解决方案
<details>	<summary>英文摘要</summary>	The surge in interest in Artificial Intelligence (AI) over the past decade has been driven almost exclusively by advances in Artificial Neural Networks (ANNs). While ANNs set state-of-the-art performance for many previously intractable problems, they require large amounts of data and computational resources for training, and since they employ supervised learning they typically need to know the correctly labelled response for every training example, limiting their scalability for real-world domains. Spiking Neural Networks (SNNs) are an alternative to ANNs that use more brain-like artificial neurons and can use unsupervised learning to discover recognizable features in the input data without knowing correct responses. SNNs, however, struggle with dynamical stability and cannot match the accuracy of ANNs. Here we show how an SNN can overcome many of the shortcomings that have been identified in the literature, including offering a principled solution to the vanishing spike problem, to outperform all existing shallow SNNs and equal the performance of an ANN. It accomplishes this while using unsupervised learning with unlabeled data and only 1/50th of the training epochs (labelled data is used only for a final simple linear readout layer). This result makes SNNs a viable new method for fast, accurate, efficient, explainable, and re-deployable machine learning with unlabeled datasets. </details>
<details>	<summary>注释</summary>	12 pages (manuscript), 10 pages (appendix), 10 pages (extended data) </details>
<details>	<summary>邮件日期</summary>	2022年08月03日</details>

# 521、MT-SNN：实现多任务单任务的脉冲神经网络
- [ ] MT-SNN: Spiking Neural Network that Enables Single-Tasking of Multiple Tasks 
时间：2022年08月02日                         第一作者：Paolo G. Cachi                       [链接](https://arxiv.org/abs/2208.01522).                     
## 摘要：在本文中，我们探索了脉冲神经网络在解决多任务分类问题中使用多任务单任务方法的能力。我们设计并实现了一个多任务脉冲神经网络（MT-SNN），它可以在一次执行一个任务的同时学习两个或多个分类任务。通过调节本工作中使用的漏积分和激发神经元的激发阈值来选择要执行的任务。该网络使用Intel的Loihi2神经形态芯片的Lava平台实现。对NMNIST数据的动态多任务分类进行了测试。结果表明，MT-SNN通过修改其动力学，即脉冲神经元的放电阈值，有效地学习多任务。
<details>	<summary>英文摘要</summary>	In this paper we explore capabilities of spiking neural networks in solving multi-task classification problems using the approach of single-tasking of multiple tasks. We designed and implemented a multi-task spiking neural network (MT-SNN) that can learn two or more classification tasks while performing one task at a time. The task to perform is selected by modulating the firing threshold of leaky integrate and fire neurons used in this work. The network is implemented using Intel's Lava platform for the Loihi2 neuromorphic chip. Tests are performed on dynamic multitask classification for NMNIST data. The results show that MT-SNN effectively learns multiple tasks by modifying its dynamics, namely, the spiking neurons' firing threshold. </details>
<details>	<summary>注释</summary>	4 pages, 2 figures </details>
<details>	<summary>邮件日期</summary>	2022年08月03日</details>

# 520、卷积网络的脉冲图
- [ ] Spiking Graph Convolutional Networks 
时间：2022年08月02日                         第一作者：Zulun Zhu                       [链接](https://arxiv.org/abs/2205.02767).                     
<details>	<summary>注释</summary>	Accepted by IJCAI 2022; Code available at https://github.com/ZulunZhu/SpikingGCN </details>
<details>	<summary>邮件日期</summary>	2022年08月03日</details>

# 519、enpheeph：一种用于脉冲和压缩深度神经网络的故障注入框架
- [ ] enpheeph: A Fault Injection Framework for Spiking and Compressed Deep Neural Networks 
时间：2022年07月31日                         第一作者：Alessio Colucci                        [链接](https://arxiv.org/abs/2208.00328).                     
## 摘要：深度神经网络（DNN）的研究侧重于提高实际部署的性能和准确性，从而产生了新的模型，如脉冲神经网络（SNN），以及优化技术，如压缩网络的量化和修剪。然而，这些创新模型和优化技术的部署引入了可能的可靠性问题，这是DNN广泛用于安全关键应用（如自动驾驶）的支柱。此外，扩展技术节点具有同时发生多个故障的相关风险，这一可能性在最先进的弹性分析中没有解决。为了更好地分析DNN的可靠性，我们提出了enpheeph，这是一种用于脉冲和压缩DNN的故障注入框架。enpheeph框架支持在专用硬件设备（如GPU）上优化执行，同时提供完全的可定制性，以调查不同的故障模型，模拟各种可靠性测试
<details>	<summary>英文摘要</summary>	Research on Deep Neural Networks (DNNs) has focused on improving performance and accuracy for real-world deployments, leading to new models, such as Spiking Neural Networks (SNNs), and optimization techniques, e.g., quantization and pruning for compressed networks. However, the deployment of these innovative models and optimization techniques introduces possible reliability issues, which is a pillar for DNNs to be widely used in safety-critical applications, e.g., autonomous driving. Moreover, scaling technology nodes have the associated risk of multiple faults happening at the same time, a possibility not addressed in state-of-the-art resiliency analyses. Towards better reliability analysis for DNNs, we present enpheeph, a Fault Injection Framework for Spiking and Compressed DNNs. The enpheeph framework enables optimized execution on specialized hardware devices, e.g., GPUs, while providing complete customizability to investigate different fault models, emulating various reliability constraints and use-cases. Hence, the faults can be executed on SNNs as well as compressed networks with minimal-to-none modifications to the underlying code, a feat that is not achievable by other state-of-the-art tools. To evaluate our enpheeph framework, we analyze the resiliency of different DNN and SNN models, with different compression techniques. By injecting a random and increasing number of faults, we show that DNNs can show a reduction in accuracy with a fault rate as low as 7 x 10 ^ (-7) faults per parameter, with an accuracy drop higher than 40%. Run-time overhead when executing enpheeph is less than 20% of the baseline execution time when executing 100 000 faults concurrently, at least 10x lower than state-of-the-art frameworks, making enpheeph future-proof for complex fault injection scenarios. We release enpheeph at https://github.com/Alexei95/enpheeph. </details>
<details>	<summary>注释</summary>	Source code: https://github.com/Alexei95/enpheeph To appear at 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), October, 2022 </details>
<details>	<summary>邮件日期</summary>	2022年08月02日</details>

# 518、具有精度损失估计器的超低延迟自适应局部二进制脉冲神经网络
- [ ] Ultra-low Latency Adaptive Local Binary Spiking Neural Network with Accuracy Loss Estimator 
时间：2022年07月31日                         第一作者：Changqing Xu                       [链接](https://arxiv.org/abs/2208.00398).                     
## 摘要：脉冲神经网络（SNN）是一种受大脑启发的模型，具有更强的时空信息处理能力和计算能量效率。然而，随着SNN深度的增加，SNN权重引起的记忆问题逐渐引起关注。受人工神经网络（ANN）量化技术的启发，引入二值化SNN（BSNN）来解决记忆问题。由于缺乏合适的学习算法，BSNN通常通过ANN到SNN的转换获得，其精度将受到训练的ANN的限制。在本文中，我们提出了一种具有精度损失估计器的超低延迟自适应局部二进制脉冲神经网络（ALBSNN），该网络通过评估网络学习过程中二值化权重引起的误差，动态选择要二值化的网络层，以确保网络的精度。实验结果表明，该方法可以在不损失网络的情况下，将存储空间减少20%以上
<details>	<summary>英文摘要</summary>	Spiking neural network (SNN) is a brain-inspired model which has more spatio-temporal information processing capacity and computational energy efficiency. However, with the increasing depth of SNNs, the memory problem caused by the weights of SNNs has gradually attracted attention. Inspired by Artificial Neural Networks (ANNs) quantization technology, binarized SNN (BSNN) is introduced to solve the memory problem. Due to the lack of suitable learning algorithms, BSNN is usually obtained by ANN-to-SNN conversion, whose accuracy will be limited by the trained ANNs. In this paper, we propose an ultra-low latency adaptive local binary spiking neural network (ALBSNN) with accuracy loss estimators, which dynamically selects the network layers to be binarized to ensure the accuracy of the network by evaluating the error caused by the binarized weights during the network learning process. Experimental results show that this method can reduce storage space by more than 20 % without losing network accuracy. At the same time, in order to accelerate the training speed of the network, the global average pooling(GAP) layer is introduced to replace the fully connected layers by the combination of convolution and pooling, so that SNNs can use a small number of time steps to obtain better recognition accuracy. In the extreme case of using only one time step, we still can achieve 92.92 %, 91.63 % ,and 63.54 % testing accuracy on three different datasets, FashionMNIST, CIFAR-10, and CIFAR-100, respectively. </details>
<details>	<summary>邮件日期</summary>	2022年08月02日</details>

# 517、基于忆阻器的脉冲神经网络文本分类
- [ ] Text Classification in Memristor-based Spiking Neural Networks 
时间：2022年07月31日                         第一作者：Jinqi Huang                       [链接](https://arxiv.org/abs/2207.13729).                     
<details>	<summary>注释</summary>	23 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2022年08月02日</details>

# 516、通过代理训练的脉冲神经网络
- [ ] Spiking neural networks trained via proxy 
时间：2022年07月30日                         第一作者：Saeed Reza Kheradpisheh                       [链接](https://arxiv.org/abs/2109.13208).                     
<details>	<summary>邮件日期</summary>	2022年08月02日</details>

# 515、基于忆阻器的脉冲神经网络文本分类
- [ ] Text Classification in Memristor-based Spiking Neural Networks 
时间：2022年07月27日                         第一作者：Jinqi Huang                       [链接](https://arxiv.org/abs/2207.13729).                     
## 摘要：忆阻器是一种新兴的非易失性存储器件，在神经形态硬件设计中显示出了巨大的潜力，特别是在脉冲神经网络（SNN）硬件实现中。基于忆阻器的SNN已成功应用于广泛的各种应用，包括图像分类和模式识别。然而，在文本分类中实现基于记忆的SNN仍在探索中。其中一个主要原因是，由于缺乏有效的学习规则和记忆器的非理想性，训练用于文本分类的基于记忆器的SNN代价高昂。为了解决这些问题并加快在文本分类应用中探索基于忆阻器的脉冲神经网络的研究，我们使用经验忆阻器模型开发了虚拟忆阻器阵列的仿真框架。我们使用这个框架来演示IMDB电影评论数据集中的情感分析任务。我们采用两种方法获得训练的脉冲神经网络
<details>	<summary>英文摘要</summary>	Memristors, emerging non-volatile memory devices, have shown promising potential in neuromorphic hardware designs, especially in spiking neural network (SNN) hardware implementation. Memristor-based SNNs have been successfully applied in a wide range of various applications, including image classification and pattern recognition. However, implementing memristor-based SNNs in text classification is still under exploration. One of the main reasons is that training memristor-based SNNs for text classification is costly due to the lack of efficient learning rules and memristor non-idealities. To address these issues and accelerate the research of exploring memristor-based spiking neural networks in text classification applications, we develop a simulation framework with a virtual memristor array using an empirical memristor model. We use this framework to demonstrate a sentiment analysis task in the IMDB movie reviews dataset. We take two approaches to obtain trained spiking neural networks with memristor models: 1) by converting a pre-trained artificial neural network (ANN) to a memristor-based SNN, or 2) by training a memristor-based SNN directly. These two approaches can be applied in two scenarios: offline classification and online training. We achieve the classification accuracy of 85.88% by converting a pre-trained ANN to a memristor-based SNN and 84.86% by training the memristor-based SNN directly, given that the baseline training accuracy of the equivalent ANN is 86.02%. We conclude that it is possible to achieve similar classification accuracy in simulation from ANNs to SNNs and from non-memristive synapses to data-driven memristive synapses. We also investigate how global parameters such as spike train length, the read noise, and the weight updating stop conditions affect the neural networks in both approaches. </details>
<details>	<summary>注释</summary>	23 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2022年07月29日</details>

# 514、面向低水平人工通用智能的神经进化
- [ ] Towards the Neuroevolution of Low-level Artificial General Intelligence 
时间：2022年07月27日                         第一作者：Sidney Pontes-Filho                       [链接](https://arxiv.org/abs/2207.13583).                     
## 摘要：在这项工作中，我们认为人工通用智能（AGI）的搜索应该从比人类智能低得多的水平开始。自然界中智能行为的环境是由有机体与其周围环境相互作用产生的，随着时间的推移，可能会发生变化，并对有机体施加压力，以允许学习新的行为或环境模型。我们的假设是，当一个主体在一个环境中行动时，学习是通过解释感觉反馈发生的。要做到这一点，需要一个机构和一个反应环境。我们评估了一种进化从环境反应中学习的生物启发人工神经网络的方法，称为人工通用智能（NAGI）的神经进化，这是一种低水平AGI框架。该方法允许具有自适应突触的随机初始化脉冲神经网络的进化复杂化，该神经网络控制在可变环境中实例化的代理。这种结构
<details>	<summary>英文摘要</summary>	In this work, we argue that the search for Artificial General Intelligence (AGI) should start from a much lower level than human-level intelligence. The circumstances of intelligent behavior in nature resulted from an organism interacting with its surrounding environment, which could change over time and exert pressure on the organism to allow for learning of new behaviors or environment models. Our hypothesis is that learning occurs through interpreting sensory feedback when an agent acts in an environment. For that to happen, a body and a reactive environment are needed. We evaluate a method to evolve a biologically-inspired artificial neural network that learns from environment reactions named Neuroevolution of Artificial General Intelligence (NAGI), a framework for low-level AGI. This method allows the evolutionary complexification of a randomly-initialized spiking neural network with adaptive synapses, which controls agents instantiated in mutable environments. Such a configuration allows us to benchmark the adaptivity and generality of the controllers. The chosen tasks in the mutable environments are food foraging, emulation of logic gates, and cart-pole balancing. The three tasks are successfully solved with rather small network topologies and therefore it opens up the possibility of experimenting with more complex tasks and scenarios where curriculum learning is beneficial. </details>
<details>	<summary>注释</summary>	18 pages, 14 figures MSC-class: 68T05 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2022年07月28日</details>

# 513、SPAIC：一个基于Spike的人工智能计算框架
- [ ] SPAIC: A Spike-based Artificial Intelligence Computing Framework 
时间：2022年07月26日                         第一作者：Chaofei Hong                       [链接](https://arxiv.org/abs/2207.12750).                     
## 摘要：神经形态计算是一个新兴的研究领域，旨在通过整合神经科学和深度学习等多学科的理论和技术来开发新的智能系统。目前，已经为相关领域开发了各种软件框架，但缺乏专门用于基于峰值的计算模型和算法的有效框架。在这项工作中，我们提出了一个基于Python的脉冲神经网络（SNN）模拟和训练框架，又名SPAIC，旨在支持结合深度学习和神经科学特征的脑启发模型和算法研究。为了整合这两个压倒性学科的不同方法，平衡灵活性和效率，SPAIC设计了神经科学风格的前端和深度学习后端结构。我们提供了广泛的示例，包括神经电路仿真、深度SNN学习和神经形态应用，演示了
<details>	<summary>英文摘要</summary>	Neuromorphic computing is an emerging research field that aims to develop new intelligent systems by integrating theories and technologies from multi-disciplines such as neuroscience and deep learning. Currently, there have been various software frameworks developed for the related fields, but there is a lack of an efficient framework dedicated for spike-based computing models and algorithms. In this work, we present a Python based spiking neural network (SNN) simulation and training framework, aka SPAIC that aims to support brain-inspired model and algorithm researches integrated with features from both deep learning and neuroscience. To integrate different methodologies from the two overwhelming disciplines, and balance between flexibility and efficiency, SPAIC is designed with neuroscience-style frontend and deep learning backend structure. We provide a wide range of examples including neural circuits Simulation, deep SNN learning and neuromorphic applications, demonstrating the concise coding style and wide usability of our framework. The SPAIC is a dedicated spike-based artificial intelligence computing platform, which will significantly facilitate the design, prototype and validation of new models, theories and applications. Being user-friendly, flexible and high-performance, it will help accelerate the rapid growth and wide applicability of neuromorphic computing research. </details>
<details>	<summary>注释</summary>	This Paper has been submitted to IEEE computational intelligence magazine </details>
<details>	<summary>邮件日期</summary>	2022年07月27日</details>

# 512、基于神经形态硬件的美国手语静态手势识别
- [ ] Static Hand Gesture Recognition for American Sign Language using Neuromorphic Hardware 
时间：2022年07月25日                         第一作者：MohammedReza Mohammadi                       [链接](https://arxiv.org/abs/2207.12559).                     
## 摘要：在本文中，我们为两个静态美国手语（ASL）手势分类任务（即ASL字母表和ASL数字）开发了四个脉冲神经网络（SNN）模型。SNN模型部署在Intel的神经形态平台Loihi上，然后与部署在边缘计算设备Intel神经计算棒2（NCS2）上的等效深度神经网络（DNN）模型进行比较。我们在准确性、延迟、功耗和能量方面对两个系统进行了全面比较。最佳DNN模型在ASL字母表数据集上的准确率达到99.6%，而最佳SNN模型的准确率为99.44%。对于ASL数字数据集，最佳SNN模型以99.52%的准确率优于所有DNN模型。此外，我们获得的实验结果表明，与NCS2相比，Loihi神经形态硬件实现的功耗和能量分别减少了14.67x和4.09x。
<details>	<summary>英文摘要</summary>	In this paper, we develop four spiking neural network (SNN) models for two static American Sign Language (ASL) hand gesture classification tasks, i.e., the ASL Alphabet and ASL Digits. The SNN models are deployed on Intel's neuromorphic platform, Loihi, and then compared against equivalent deep neural network (DNN) models deployed on an edge computing device, the Intel Neural Compute Stick 2 (NCS2). We perform a comprehensive comparison between the two systems in terms of accuracy, latency, power consumption, and energy. The best DNN model achieves an accuracy of 99.6% on the ASL Alphabet dataset, whereas the best performing SNN model has an accuracy of 99.44%. For the ASL-Digits dataset, the best SNN model outperforms all of its DNN counterparts with 99.52% accuracy. Moreover, our obtained experimental results show that the Loihi neuromorphic hardware implementations achieve up to 14.67x and 4.09x reduction in power consumption and energy, respectively, when compared to NCS2. </details>
<details>	<summary>注释</summary>	Authors MohammedReza Mohammadi, and Peyton Chandarana contributed equally </details>
<details>	<summary>邮件日期</summary>	2022年07月27日</details>

# 511、模拟突触间的联想可塑性以增强脉冲神经网络的学习
- [ ] Modeling Associative Plasticity between Synapses to Enhance Learning of Spiking Neural Networks 
时间：2022年07月24日                         第一作者：Haibo Shen                       [链接](https://arxiv.org/abs/2207.11670).                     
## 摘要：脉冲神经网络（SNN）是第三代人工神经网络，能够在神经形态硬件上实现节能。然而，脉冲的离散传输给鲁棒和高性能学习机制带来了重大挑战。现有的大多数工作只关注神经元之间的学习，而忽略了突触之间的影响，导致鲁棒性和准确性的损失。为了解决这个问题，我们提出了一种鲁棒有效的学习机制，通过模拟突触间的联想可塑性（APB），从联想长时程增强（ALTP）的生理现象中观察到。在所提出的APBS方法中，当其他神经元同时刺激时，同一神经元的突触通过共享因子相互作用。此外，我们提出了一种时空裁剪和翻转（STCF）方法来提高网络的泛化能力。大量实验表明，我们的方法实现了
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) are the third generation of artificial neural networks that enable energy-efficient implementation on neuromorphic hardware. However, the discrete transmission of spikes brings significant challenges to the robust and high-performance learning mechanism. Most existing works focus solely on learning between neurons but ignore the influence between synapses, resulting in a loss of robustness and accuracy. To address this problem, we propose a robust and effective learning mechanism by modeling the associative plasticity between synapses (APBS) observed from the physiological phenomenon of associative long-term potentiation (ALTP). With the proposed APBS method, synapses of the same neuron interact through a shared factor when concurrently stimulated by other neurons. In addition, we propose a spatiotemporal cropping and flipping (STCF) method to improve the generalization ability of our network. Extensive experiments demonstrate that our approaches achieve superior performance on static CIFAR-10 datasets and state-of-the-art performance on neuromorphic MNIST-DVS, CIFAR10-DVS datasets by a lightweight convolution network. To our best knowledge, this is the first time to explore a learning method between synapses and an extended approach for neuromorphic data. </details>
<details>	<summary>注释</summary>	Submitted to ijcai2022, rejected </details>
<details>	<summary>邮件日期</summary>	2022年07月26日</details>

# 510、位置刺激神经元的事件驱动触觉学习
- [ ] Event-Driven Tactile Learning with Location Spiking Neurons 
时间：2022年07月23日                         第一作者：Peng Kang                       [链接](https://arxiv.org/abs/2209.01080).                     
## 摘要：触觉对于各种日常任务至关重要。基于事件的触觉传感器和脉冲神经网络（SNN）的新进展推动了事件驱动触觉学习的研究。然而，由于现有脉冲神经元的代表性能力有限以及数据的时空复杂性，SNN激活的事件驱动触觉学习仍处于初级阶段。在本文中，为了提高现有脉冲神经元的代表性能力，我们提出了一种新的神经元模型，称为“位置脉冲神经元”，它使我们能够以新的方式提取基于事件的数据的特征。此外，在经典的时间脉冲响应模型（TSRM）的基础上，我们开发了一种特定的位置脉冲神经元模型-位置脉冲响应（LSRM），作为SNN的新构建块。此外，我们提出了一种混合模型，将SNN与TSRM神经元和SNN与LSRM神经元相结合，以捕获数据中复杂的时空相关性。广阔的
<details>	<summary>英文摘要</summary>	The sense of touch is essential for a variety of daily tasks. New advances in event-based tactile sensors and Spiking Neural Networks (SNNs) spur the research in event-driven tactile learning. However, SNN-enabled event-driven tactile learning is still in its infancy due to the limited representative abilities of existing spiking neurons and high spatio-temporal complexity in the data. In this paper, to improve the representative capabilities of existing spiking neurons, we propose a novel neuron model called "location spiking neuron", which enables us to extract features of event-based data in a novel way. Moreover, based on the classical Time Spike Response Model (TSRM), we develop a specific location spiking neuron model - Location Spike Response Model (LSRM) that serves as a new building block of SNNs. Furthermore, we propose a hybrid model which combines an SNN with TSRM neurons and an SNN with LSRM neurons to capture the complex spatio-temporal dependencies in the data. Extensive experiments demonstrate the significant improvements of our models over other works on event-driven tactile learning and show the superior energy efficiency of our models and location spiking neurons, which may unlock their potential on neuromorphic hardware. </details>
<details>	<summary>注释</summary>	accepted by IJCNN 2022 (oral), the source code is available at https://github.com/pkang2017/TactileLocNeurons </details>
<details>	<summary>邮件日期</summary>	2022年09月05日</details>

# 509、NeuroHSMD：神经形态混合脉冲运动检测器
- [ ] NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector 
时间：2022年07月22日                         第一作者：Pedro Machado                       [链接](https://arxiv.org/abs/2112.06102).                     
<details>	<summary>邮件日期</summary>	2022年07月25日</details>

# 508、脉冲神经网络中彩票假设的探索
- [ ] Exploring Lottery Ticket Hypothesis in Spiking Neural Networks 
时间：2022年07月20日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2207.01382).                     
<details>	<summary>注释</summary>	Accepted to European Conference on Computer Vision (ECCV) 2022 </details>
<details>	<summary>邮件日期</summary>	2022年07月22日</details>

# 507、脉冲神经网络的神经结构搜索
- [ ] Neural Architecture Search for Spiking Neural Networks 
时间：2022年07月20日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2201.10355).                     
<details>	<summary>注释</summary>	Accepted to European Conference on Computer Vision (ECCV) 2022 </details>
<details>	<summary>邮件日期</summary>	2022年07月22日</details>

# 506、一种基于时间和空间局部脉冲的反向传播算法，用于硬件训练
- [ ] A Temporally and Spatially Local Spike-based Backpropagation Algorithm to Enable Training in Hardware 
时间：2022年07月20日                         第一作者：Anmol Biswas                       [链接](https://arxiv.org/abs/2207.09755).                     
## 摘要：脉冲神经网络（SNN）已成为分类任务的硬件高效架构。基于脉冲的编码的缺点是缺乏完全使用脉冲执行的通用训练机制。已经有几次尝试采用非脉冲人工神经网络（ANN）中使用的强大反向传播（BP）技术：（1）SNN可以通过外部计算的数值梯度进行训练。（2） 基于自然脉冲的学习的一个主要进展是使用了近似反向传播，使用脉冲时间相关塑性（STDP）和分阶段的前/后向传递。然而，这些阶段之间的信息传输需要外部存储器和计算访问。这是神经形态硬件实现的挑战。在本文中，我们提出了一种基于随机SNN的反向支持（SSNN-BP）算法，该算法利用一个复合神经元同时计算前向通过激活和后向通过梯度
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have emerged as a hardware efficient architecture for classification tasks. The penalty of spikes-based encoding has been the lack of a universal training mechanism performed entirely using spikes. There have been several attempts to adopt the powerful backpropagation (BP) technique used in non-spiking artificial neural networks (ANN): (1) SNNs can be trained by externally computed numerical gradients. (2) A major advancement toward native spike-based learning has been the use of approximate Backpropagation using spike-time-dependent plasticity (STDP) with phased forward/backward passes. However, the transfer of information between such phases necessitates external memory and computational access. This is a challenge for neuromorphic hardware implementations. In this paper, we propose a stochastic SNN-based Back-Prop (SSNN-BP) algorithm that utilizes a composite neuron to simultaneously compute the forward pass activations and backward pass gradients explicitly with spikes. Although signed gradient values are a challenge for spike-based representation, we tackle this by splitting the gradient signal into positive and negative streams. The composite neuron encodes information in the form of stochastic spike-trains and converts Backpropagation weight updates into temporally and spatially local discrete STDP-like spike coincidence updates compatible with hardware-friendly Resistive Processing Units (RPUs). Furthermore, our method approaches BP ANN baseline with sufficiently long spike-trains. Finally, we show that softmax cross-entropy loss function can be implemented through inhibitory lateral connections enforcing a Winner Take All (WTA) rule. Our SNN shows excellent generalization through comparable performance to ANNs on the MNIST, Fashion-MNIST and Extended MNIST datasets. Thus, SSNN-BP enables BP compatible with purely spike-based neuromorphic hardware. </details>
<details>	<summary>邮件日期</summary>	2022年07月21日</details>

# 505、用于训练脉冲神经网络的神经形态数据扩充
- [ ] Neuromorphic Data Augmentation for Training Spiking Neural Networks 
时间：2022年07月20日                         第一作者：Yuhang Li                       [链接](https://arxiv.org/abs/2203.06145).                     
<details>	<summary>注释</summary>	Accepted to the 17th European Conference on Computer Vision (ECCV 2022) </details>
<details>	<summary>邮件日期</summary>	2022年07月21日</details>

# 504、用于自然视觉图像重建的脑激励解码器
- [ ] The Brain-Inspired Decoder for Natural Visual Image Reconstruction 
时间：2022年07月18日                         第一作者：Wenyi Li                       [链接](https://arxiv.org/abs/2207.08591).                     
## 摘要：从大脑活动中解码图像一直是一个挑战。由于深度学习的发展，有可用的工具来解决这个问题。解码图像，其目的是将神经脉冲训练映射到低级视觉特征和高级语义信息空间。最近，有一些关于从棘波序列解码的研究，然而，这些研究较少关注神经科学的基础，并且很少有研究将感受野合并到视觉图像重建中。在本文中，我们提出了一种具有生物学特性的深度学习神经网络结构，用于从脉冲序列重建视觉图像。据我们所知，我们第一次实现了将接收场特性矩阵整合到损失函数中的方法。我们的模型是从神经脉冲序列到图像的端到端解码器。我们不仅将Gabor滤波器合并到用于生成图像的自动编码器中，还提出了一种具有感受野特性的损失函数。
<details>	<summary>英文摘要</summary>	Decoding images from brain activity has been a challenge. Owing to the development of deep learning, there are available tools to solve this problem. The decoded image, which aims to map neural spike trains to low-level visual features and high-level semantic information space. Recently, there are a few studies of decoding from spike trains, however, these studies pay less attention to the foundations of neuroscience and there are few studies that merged receptive field into visual image reconstruction. In this paper, we propose a deep learning neural network architecture with biological properties to reconstruct visual image from spike trains. As far as we know, we implemented a method that integrated receptive field property matrix into loss function at the first time. Our model is an end-to-end decoder from neural spike trains to images. We not only merged Gabor filter into auto-encoder which used to generate images but also proposed a loss function with receptive field properties. We evaluated our decoder on two datasets which contain macaque primary visual cortex neural spikes and salamander retina ganglion cells (RGCs) spikes. Our results show that our method can effectively combine receptive field features to reconstruct images, providing a new approach to visual reconstruction based on neural information. </details>
<details>	<summary>邮件日期</summary>	2022年07月19日</details>

# 503、BrainCog：一个基于脉冲神经网络的脑启发认知智能引擎，用于脑启发人工智能和脑模拟
- [ ] BrainCog: A Spiking Neural Network based Brain-inspired Cognitive Intelligence Engine for Brain-inspired AI and Brain Simulation 
时间：2022年07月18日                         第一作者：Yi Zeng                       [链接](https://arxiv.org/abs/2207.08533).                     
## 摘要：脉冲神经网络（SNN）在脑启发人工智能和计算神经科学中引起了广泛关注。它们可以用于在多个尺度上模拟大脑中的生物信息处理。更重要的是，SNN作为一个适当的抽象层次，将大脑和认知的灵感引入人工智能。在本文中，我们提出了脑启发认知智能引擎（BrainCog），用于创建脑启发人工智能和脑模拟模型。BrainCog整合了不同类型的脉冲神经元模型、学习规则、大脑区域等，作为平台提供的基本模块。基于这些易于使用的模块，BrainCog支持各种大脑启发的认知功能，包括感知和学习、决策、知识表示和推理、运动控制和社会认知。这些受大脑启发的人工智能模型已在各种有监督、无监督、有监督和无监督的情况下得到有效验证
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have attracted extensive attentions in Brain-inspired Artificial Intelligence and computational neuroscience. They can be used to simulate biological information processing in the brain at multiple scales. More importantly, SNNs serve as an appropriate level of abstraction to bring inspirations from brain and cognition to Artificial Intelligence. In this paper, we present the Brain-inspired Cognitive Intelligence Engine (BrainCog) for creating brain-inspired AI and brain simulation models. BrainCog incorporates different types of spiking neuron models, learning rules, brain areas, etc., as essential modules provided by the platform. Based on these easy-to-use modules, BrainCog supports various brain-inspired cognitive functions, including Perception and Learning, Decision Making, Knowledge Representation and Reasoning, Motor Control, and Social Cognition. These brain-inspired AI models have been effectively validated on various supervised, unsupervised, and reinforcement learning tasks, and they can be used to enable AI models to be with multiple brain-inspired cognitive functions. For brain simulation, BrainCog realizes the function simulation of decision-making, working memory, the structure simulation of the Neural Circuit, and whole brain structure simulation of Mouse brain, Macaque brain, and Human brain. An AI engine named BORN is developed based on BrainCog, and it demonstrates how the components of BrainCog can be integrated and used to build AI models and applications. To enable the scientific quest to decode the nature of biological intelligence and create AI, BrainCog aims to provide essential and easy-to-use building blocks, and infrastructural support to develop brain-inspired spiking neural network based AI, and to simulate the cognitive brains at multiple scales. The online repository of BrainCog can be found at https://github.com/braincog-x. </details>
<details>	<summary>邮件日期</summary>	2022年07月19日</details>

# 502、神经形态语音识别的有效脉冲编码算法
- [ ] Efficient spike encoding algorithms for neuromorphic speech recognition 
时间：2022年07月14日                         第一作者：Sidi Yaya Arnaud Yarga                       [链接](https://arxiv.org/abs/2207.07073).                     
## 摘要：已知脉冲神经网络（SNN）对于神经形态处理器的实现非常有效，与传统深度学习方法相比，在能量效率和计算延迟方面实现了数量级的改进。最近，随着监督训练算法适应SNN环境，可比较的算法性能也成为可能。然而，包括音频、视频和其他传感器衍生数据的信息通常被编码为不适合SNN的实值信号，从而防止网络利用脉冲定时信息。因此，从实值信号到脉冲的有效编码是关键的，并且显著影响整个系统的性能。为了有效地将信号编码成脉冲，必须考虑与手头任务相关的信息的保存以及编码脉冲的密度。在本文中，我们研究了说话人背景下的四种脉冲编码方法
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNN) are known to be very effective for neuromorphic processor implementations, achieving orders of magnitude improvements in energy efficiency and computational latency over traditional deep learning approaches. Comparable algorithmic performance was recently made possible as well with the adaptation of supervised training algorithms to the context of SNN. However, information including audio, video, and other sensor-derived data are typically encoded as real-valued signals that are not well-suited to SNN, preventing the network from leveraging spike timing information. Efficient encoding from real-valued signals to spikes is therefore critical and significantly impacts the performance of the overall system. To efficiently encode signals into spikes, both the preservation of information relevant to the task at hand as well as the density of the encoded spikes must be considered. In this paper, we study four spike encoding methods in the context of a speaker independent digit classification system: Send on Delta, Time to First Spike, Leaky Integrate and Fire Neuron and Bens Spiker Algorithm. We first show that all encoding methods yield higher classification accuracy using significantly fewer spikes when encoding a bio-inspired cochleagram as opposed to a traditional short-time Fourier transform. We then show that two Send On Delta variants result in classification results comparable with a state of the art deep convolutional neural network baseline, while simultaneously reducing the encoded bit rate. Finally, we show that several encoding methods result in improved performance over the conventional deep learning baseline in certain cases, further demonstrating the power of spike encoding algorithms in the encoding of real-valued signals and that neuromorphic implementation has the potential to outperform state of the art techniques. </details>
<details>	<summary>注释</summary>	Accepted to International Conference on Neuromorphic Systems (ICONS 2022) DOI: 10.1145/3546790.3546803 </details>
<details>	<summary>邮件日期</summary>	2022年07月15日</details>

# 501、用时间（脉冲）神经元实现的宏列结构
- [ ] A Macrocolumn Architecture Implemented with Temporal (Spiking) Neurons 
时间：2022年07月11日                         第一作者：James E. Smith                       [链接](https://arxiv.org/abs/2207.05081).                     
## 摘要：由于长期目标是自下而上逆向架构计算大脑，因此本文的重点是宏列抽象层。通过首先用状态机模型描述其操作，开发了基本的宏列架构。然后用支持时间计算的脉冲神经元实现状态机函数。神经元模型基于活跃的脉冲树突，反映了Hawkins/Numenta神经元模型。该架构通过一个研究基准进行了演示，其中代理使用宏列首先学习，然后导航包含随机放置特征的二维环境。环境在宏列中表示为带标签的有向图，其中边连接特征，标签表示它们之间的相对位移。
<details>	<summary>英文摘要</summary>	With the long-term goal of reverse-architecting the computational brain from the bottom up, the focus of this document is the macrocolumn abstraction layer. A basic macrocolumn architecture is developed by first describing its operation with a state machine model. Then state machine functions are implemented with spiking neurons that support temporal computation. The neuron model is based on active spiking dendrites and mirrors the Hawkins/Numenta neuron model. The architecture is demonstrated with a research benchmark in which an agent uses a macrocolumn to first learn and then navigate 2-d environments containing randomly placed features. Environments are represented in the macrocolumn as labeled directed graphs where edges connect features and labels indicate the relative displacements between them. </details>
<details>	<summary>邮件日期</summary>	2022年07月13日</details>

# 500、用于常识知识表示和推理的脑激励图形脉冲神经网络
- [ ] Brain-inspired Graph Spiking Neural Networks for Commonsense Knowledge Representation and Reasoning 
时间：2022年07月11日                         第一作者：Hongjian Fang                       [链接](https://arxiv.org/abs/2207.05561).                     
## 摘要：人脑中的神经网络如何代表常识知识，并完成相关推理任务，是神经科学、认知科学、心理学和人工智能领域的重要研究课题。尽管使用固定长度向量表示符号的传统人工神经网络在某些特定任务中取得了良好的性能，但它仍然是一个缺乏可解释性的黑盒子，与人类如何感知世界相去甚远。受神经科学中祖母细胞假说的启发，这项工作研究了群体编码和脉冲时间依赖性可塑性（STDP）机制如何整合到脉冲神经网络的学习中，以及神经元群体如何通过引导不同神经元群体之间的顺序放电完成来表示符号。不同群体的神经元群体共同构成了整个常识知识图，形成了一个巨大的图脉冲神经网络。此外，我们和我
<details>	<summary>英文摘要</summary>	How neural networks in the human brain represent commonsense knowledge, and complete related reasoning tasks is an important research topic in neuroscience, cognitive science, psychology, and artificial intelligence. Although the traditional artificial neural network using fixed-length vectors to represent symbols has gained good performance in some specific tasks, it is still a black box that lacks interpretability, far from how humans perceive the world. Inspired by the grandmother-cell hypothesis in neuroscience, this work investigates how population encoding and spiking timing-dependent plasticity (STDP) mechanisms can be integrated into the learning of spiking neural networks, and how a population of neurons can represent a symbol via guiding the completion of sequential firing between different neuron populations. The neuron populations of different communities together constitute the entire commonsense knowledge graph, forming a giant graph spiking neural network. Moreover, we introduced the Reward-modulated spiking timing-dependent plasticity (R-STDP) mechanism to simulate the biological reinforcement learning process and completed the related reasoning tasks accordingly, achieving comparable accuracy and faster convergence speed than the graph convolutional artificial neural networks. For the fields of neuroscience and cognitive science, the work in this paper provided the foundation of computational modeling for further exploration of the way the human brain represents commonsense knowledge. For the field of artificial intelligence, this paper indicated the exploration direction for realizing a more robust and interpretable neural network by constructing a commonsense knowledge representation and reasoning spiking neural networks with solid biological plausibility. </details>
<details>	<summary>邮件日期</summary>	2022年07月13日</details>

# 499、BioLCNet:奖励调制局部连接脉冲神经网络
- [ ] BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks 
时间：2022年07月07日                         第一作者：Hafez Ghaemi                       [链接](https://arxiv.org/abs/2109.05539).                     
<details>	<summary>注释</summary>	15 pages, 6 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2022年07月08日</details>

# 498、脉冲校准：用于目标检测和分割的脉冲神经网络的快速准确转换
- [ ] Spike Calibration: Fast and Accurate Conversion of Spiking Neural Network for Object Detection and Segmentation 
时间：2022年07月06日                         第一作者：Yang Li                       [链接](https://arxiv.org/abs/2207.02702).                     
## 摘要：脉冲神经网络（SNN）由于在神经形态硬件上具有高生物似然性和低能量消耗的特性而受到高度重视。作为获得深度SNN的有效方法，该转换方法在各种大规模数据集上表现出了高性能。然而，它通常遭受严重的性能降级和高时间延迟。特别是，以前的大多数工作集中于简单的分类任务，而忽略了神经网络输出的精确近似。在本文中，我们首先从理论上分析了转换误差，并推导了时变极值对突触电流的有害影响。我们提出了脉冲校准（SpiCalib）以消除离散脉冲对输出分布的损害，并修改Lipoooling以允许任意最大池层的无损转换。此外，提出了最佳归一化参数的贝叶斯优化，以避免经验设置。经验
<details>	<summary>英文摘要</summary>	Spiking neural network (SNN) has been attached to great importance due to the properties of high biological plausibility and low energy consumption on neuromorphic hardware. As an efficient method to obtain deep SNN, the conversion method has exhibited high performance on various large-scale datasets. However, it typically suffers from severe performance degradation and high time delays. In particular, most of the previous work focuses on simple classification tasks while ignoring the precise approximation to ANN output. In this paper, we first theoretically analyze the conversion errors and derive the harmful effects of time-varying extremes on synaptic currents. We propose the Spike Calibration (SpiCalib) to eliminate the damage of discrete spikes to the output distribution and modify the LIPooling to allow conversion of the arbitrary MaxPooling layer losslessly. Moreover, Bayesian optimization for optimal normalization parameters is proposed to avoid empirical settings. The experimental results demonstrate the state-of-the-art performance on classification, object detection, and segmentation tasks. To the best of our knowledge, this is the first time to obtain SNN comparable to ANN on these tasks simultaneously. Moreover, we only need 1/50 inference time of the previous work on the detection task and can achieve the same performance under 0.492$\times$ energy consumption of ANN on the segmentation task. </details>
<details>	<summary>邮件日期</summary>	2022年07月07日</details>

# 497、一种受生物上合理的学习规则和连接启发的无监督脉冲神经网络
- [ ] An Unsupervised Spiking Neural Network Inspired By Biologically Plausible Learning Rules and Connections 
时间：2022年07月06日                         第一作者：Yiting Dong                       [链接](https://arxiv.org/abs/2207.02727).                     
## 摘要：反向传播算法促进了深度学习的快速发展，但它依赖于大量的标记数据，与人类的学习方式还有很大差距。人脑可以以自组织和无监督的方式快速学习各种概念知识，这是通过协调人脑中的多个学习规则和结构来实现的。脉冲时间依赖性可塑性（STDP）是大脑中广泛存在的学习规则，但单独使用STDP训练的脉冲神经网络效率低且性能差。本文受短期突触可塑性的启发，设计了一种自适应突触滤波器，并引入自适应阈值平衡作为神经元可塑性，以丰富SNN的表达能力。我们还引入了自适应横向抑制连接来动态调整脉冲平衡，以帮助网络学习更丰富的特征。加快和稳定
<details>	<summary>英文摘要</summary>	The backpropagation algorithm has promoted the rapid development of deep learning, but it relies on a large amount of labeled data, and there is still a large gap with the way the human learns. The human brain can rapidly learn various concept knowledge in a self-organized and unsupervised way, which is accomplished through the coordination of multiple learning rules and structures in the human brain. Spike-timing-dependent plasticity (STDP) is a widespread learning rule in the brain, but spiking neural network trained using STDP alone are inefficient and performs poorly. In this paper, taking inspiration from the short-term synaptic plasticity, we design an adaptive synaptic filter, and we introduce the adaptive threshold balance as the neuron plasticity to enrich the representation ability of SNNs. We also introduce an adaptive lateral inhibitory connection to dynamically adjust the spikes balance to help the network learn richer features. To accelerate and stabilize the training of the unsupervised spiking neural network, we design a sample temporal batch STDP which update the weight based on multiple samples and multiple moments. We have conducted experiments on MNIST and FashionMNIST, and have achieved state-of-the-art performance of the current unsupervised spiking neural network based on STDP. And our model also shows strong superiority in small samples learning. </details>
<details>	<summary>邮件日期</summary>	2022年07月07日</details>

# 496、神经网络中的彩票假设
- [ ] Lottery Ticket Hypothesis for Spiking Neural Networks 
时间：2022年07月04日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2207.01382).                     
## 摘要：脉冲神经网络（SNN）最近作为新一代低功耗深度神经网络出现，其中二进制脉冲在多个时间步长上传递信息。当SNN部署在资源受限的移动/边缘设备上时，对SNN的修剪非常重要。以前的SNN修剪工作集中于浅SNN（2~6层），然而，最先进的SNN工作提出了更深的SNN（>16层），这很难与当前的修剪工作兼容。为了向深度SNN扩展剪枝技术，我们研究了彩票假设（LTH），该假设指出，密集网络包含较小的子网络（即中奖彩票），其性能与密集网络相当。我们对LTH的研究表明，中奖彩票始终存在于各种数据集和架构的深度SNN中，提供了高达97%的稀疏性，而不会出现巨大的性能下降。然而，LTH的迭代搜索过程带来了巨大的训练成本
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have recently emerged as a new generation of low-power deep neural networks where binary spikes convey information across multiple timesteps. Pruning for SNNs is highly important as they become deployed on a resource-constraint mobile/edge device. The previous SNN pruning works focus on shallow SNNs (2~6 layers), however, deeper SNNs (>16 layers) are proposed by state-of-the-art SNN works, which is difficult to be compatible with the current pruning work. To scale up a pruning technique toward deep SNNs, we investigate Lottery Ticket Hypothesis (LTH) which states that dense networks contain smaller subnetworks (i.e., winning tickets) that achieve comparable performance to the dense networks. Our studies on LTH reveal that the winning tickets consistently exist in deep SNNs across various datasets and architectures, providing up to 97% sparsity without huge performance degradation. However, the iterative searching process of LTH brings a huge training computational cost when combined with the multiple timesteps of SNNs. To alleviate such heavy searching cost, we propose Early-Time (ET) ticket where we find the important weight connectivity from a smaller number of timesteps. The proposed ET ticket can be seamlessly combined with common pruning techniques for finding winning tickets, such as Iterative Magnitude Pruning (IMP) and Early-Bird (EB) tickets. Our experiment results show that the proposed ET ticket reduces search time by up to 38% compared to IMP or EB methods. </details>
<details>	<summary>注释</summary>	Accepted to European Conference on Computer Vision (ECCV) 2022 </details>
<details>	<summary>邮件日期</summary>	2022年07月05日</details>

# 495、简单和复杂的脉冲神经元：简单STDP场景中的透视和分析
- [ ] Simple and complex spiking neurons: perspectives and analysis in a simple STDP scenario 
时间：2022年06月28日                         第一作者：Davide Liberato Manna                       [链接](https://arxiv.org/abs/2207.04881).                     
## 摘要：脉冲神经网络（SNN）在很大程度上受到生物学和神经科学的启发，并利用思想和理论来创建快速高效的学习系统。脉冲神经元模型被用作神经形态系统的核心处理单元，因为它们支持基于事件的处理。通常采用集成和火灾（I&F）模型，其中最常用的是简单泄漏I&F（LIF）。采用这种模型的原因是它们的效率和/或生物学合理性。然而，在人工学习系统中采用LIF优于其他神经元模型的严格理由尚未研究。这项工作考虑了文献中的各种神经元模型，然后选择单变量、高效且显示不同类型复杂性的计算神经元模型。从这一选择中，我们对三种简单的I&F神经元模型，即LIF、二次I&F（QIF）和指数I&F，进行了比较研究，以了解是否使用
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are largely inspired by biology and neuroscience and leverage ideas and theories to create fast and efficient learning systems. Spiking neuron models are adopted as core processing units in neuromorphic systems because they enable event-based processing. The integrate-and-fire (I&F) models are often adopted, with the simple Leaky I&F (LIF) being the most used. The reason for adopting such models is their efficiency and/or biological plausibility. Nevertheless, rigorous justification for adopting LIF over other neuron models for use in artificial learning systems has not yet been studied. This work considers various neuron models in the literature and then selects computational neuron models that are single-variable, efficient, and display different types of complexities. From this selection, we make a comparative study of three simple I&F neuron models, namely the LIF, the Quadratic I&F (QIF) and the Exponential I&F (EIF), to understand whether the use of more complex models increases the performance of the system and whether the choice of a neuron model can be directed by the task to be completed. Neuron models are tested within an SNN trained with Spike-Timing Dependent Plasticity (STDP) on a classification task on the N-MNIST and DVS Gestures datasets. Experimental results reveal that more complex neurons manifest the same ability as simpler ones to achieve high levels of accuracy on a simple dataset (N-MNIST), albeit requiring comparably more hyper-parameter tuning. However, when the data possess richer Spatio-temporal features, the QIF and EIF neuron models steadily achieve better results. This suggests that accurately selecting the model based on the richness of the feature spectrum of the data could improve the whole system's performance. Finally, the code implementing the spiking neurons in the SpykeTorch framework is made publicly available. </details>
<details>	<summary>邮件日期</summary>	2022年07月12日</details>

# 494、脉冲神经网络的结构稳定性
- [ ] Structural Stability of Spiking Neural Networks 
时间：2022年06月21日                         第一作者：G. Zhang                        [链接](https://arxiv.org/abs/2207.04876).                     
## 摘要：在过去的几十年中，由于对时间相关数据建模的巨大潜力，人们对脉冲神经网络（SNN）越来越感兴趣。已经开发了许多算法和技术；然而，对脉冲神经网络的许多方面的理论理解仍然模糊。最近的一项研究[Zhang等人，2021]揭示，由于其分叉动力学，典型SNN很难承受内部和外部扰动，并建议必须添加自连接。在本文中，我们研究了具有自连接的SNN的理论性质，并通过指定最大分岔解数的下界和上界来深入分析结构稳定性。在模拟和实际任务上进行的数值实验证明了所提出结果的有效性。
<details>	<summary>英文摘要</summary>	The past decades have witnessed an increasing interest in spiking neural networks (SNNs) due to their great potential of modeling time-dependent data. Many algorithms and techniques have been developed; however, theoretical understandings of many aspects of spiking neural networks are still cloudy. A recent work [Zhang et al. 2021] disclosed that typical SNNs could hardly withstand both internal and external perturbations due to their bifurcation dynamics and suggested that self-connection has to be added. In this paper, we investigate the theoretical properties of SNNs with self-connection, and develop an in-depth analysis on structural stability by specifying the lower and upper bounds of the maximum number of bifurcation solutions. Numerical experiments conducted on simulation and practical tasks demonstrate the effectiveness of the proposed results. </details>
<details>	<summary>邮件日期</summary>	2022年07月12日</details>

# 493、基于线性泄漏积分和激发神经元模型的脉冲神经网络及其与深度神经网络的映射关系
- [ ] Linear Leaky-Integrate-and-Fire Neuron Model Based Spiking Neural Networks and Its Mapping Relationship to Deep Neural Networks 
时间：2022年05月31日                         第一作者：Sijia Lu                        [链接](https://arxiv.org/abs/2207.04889).                     
## 摘要：脉冲神经网络（SNN）是一种受大脑启发的机器学习算法，具有生物学合理性和无监督学习能力等优点。先前的工作已经表明，将人工神经网络（ANN）转换为SNN是实现SNN的实用和有效的方法。然而，缺乏训练非精确损失SNN的基本原理和理论基础。本文建立了线性泄漏积分和火灾模型（LIF）/SNN的生物参数与ReLU AN/Deep神经网络（DNN）参数之间的精确数学映射。这种映射关系在一定条件下得到了分析证明，并通过模拟和实际数据实验进行了验证。它可以作为两类神经网络各自优点的潜在组合的理论基础。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are brain-inspired machine learning algorithms with merits such as biological plausibility and unsupervised learning capability. Previous works have shown that converting Artificial Neural Networks (ANNs) into SNNs is a practical and efficient approach for implementing an SNN. However, the basic principle and theoretical groundwork are lacking for training a non-accuracy-loss SNN. This paper establishes a precise mathematical mapping between the biological parameters of the Linear Leaky-Integrate-and-Fire model (LIF)/SNNs and the parameters of ReLU-AN/Deep Neural Networks (DNNs). Such mapping relationship is analytically proven under certain conditions and demonstrated by simulation and real data experiments. It can serve as the theoretical basis for the potential combination of the respective merits of the two categories of neural networks. </details>
<details>	<summary>邮件日期</summary>	2022年07月12日</details>

# 492、为什么医疗保健需要可解释的人工智能
- [ ] Why we do need Explainable AI for Healthcare 
时间：2022年06月30日                         第一作者：Giovanni Cin\`a                       [链接](https://arxiv.org/abs/2206.15363).                     
## 摘要：最近，用于医疗保健的人工智能（AI）认证工具激增，重新引发了关于采用这项技术的辩论。这种争论的一个线索涉及可解释的人工智能及其使人工智能设备更透明和更可信的承诺。一些活跃在医学人工智能领域的声音对可解释人工智能技术的可靠性表示担忧，质疑其使用和纳入指南和标准。回顾这些批评，本文就可解释人工智能的效用提供了一个平衡和全面的视角，重点关注人工智能临床应用的特殊性，并将其放在医疗干预的背景下。针对其批评者，尽管存在合理的担忧，我们认为可解释的人工智能研究项目仍然是人机交互的核心，最终是我们防止失控的主要工具，这种危险仅靠严格的临床验证是无法预防的。
<details>	<summary>英文摘要</summary>	The recent spike in certified Artificial Intelligence (AI) tools for healthcare has renewed the debate around adoption of this technology. One thread of such debate concerns Explainable AI and its promise to render AI devices more transparent and trustworthy. A few voices active in the medical AI space have expressed concerns on the reliability of Explainable AI techniques, questioning their use and inclusion in guidelines and standards. Revisiting such criticisms, this article offers a balanced and comprehensive perspective on the utility of Explainable AI, focusing on the specificity of clinical applications of AI and placing them in the context of healthcare interventions. Against its detractors and despite valid concerns, we argue that the Explainable AI research program is still central to human-machine interaction and ultimately our main tool against loss of control, a danger that cannot be prevented by rigorous clinical validation alone. </details>
<details>	<summary>邮件日期</summary>	2022年07月01日</details>

# 491、CIRDataset：用于临床可解释的肺结节放射组学和恶性肿瘤预测的大规模数据集
- [ ] CIRDataset: A large-scale Dataset for Clinically-Interpretable lung nodule Radiomics and malignancy prediction 
时间：2022年06月29日                         第一作者：Wookjin Choi                       [链接](https://arxiv.org/abs/2206.14903).                     
## 摘要：针状/分叶状、肺结节表面尖锐/弯曲的脉冲是肺癌恶性的良好预测因子，因此，放射科医生定期评估和报告，作为标准化肺RADS临床评分标准的一部分。考虑到结节的三维几何形状和放射科医生逐层二维评估，手动针状/分叶注释是一项繁琐的任务，因此目前还没有公共数据集用于探讨这些临床报告特征在SOTA恶性肿瘤预测算法中的重要性。作为本文的一部分，我们发布了一个大规模临床可解释的放射组学数据集CIRDataset，其中包含来自两个公共数据集LIDC-IDRI（N=883）和LUNGx（N=73）的956个放射科医生对分割肺结节的QA/QC'ed针状/分叶注释。我们还提出了一种基于多类体素网格扩展的端到端深度学习模型，用于分割结节（同时保留脉冲），分类脉冲（尖锐/脉冲和弯曲/分叶状），并进行恶性肿瘤预测。以前的方法已经对LIDC和LUNGx数据集进行了恶性肿瘤预测，但没有对任何临床报告/可操作的特征进行可靠归因（由于一般归因方案存在已知的超参数敏感性问题）。随着这一全面注释的CIRDataset和端到端深度学习基线的发布，我们希望恶性肿瘤预测方法能够验证其解释，对照我们的基线进行基准测试，并提供临床可操作的见解。数据集、代码、预训练模型和docker容器可在https://github.com/nadeemlab/CIR.
<details>	<summary>英文摘要</summary>	Spiculations/lobulations, sharp/curved spikes on the surface of lung nodules, are good predictors of lung cancer malignancy and hence, are routinely assessed and reported by radiologists as part of the standardized Lung-RADS clinical scoring criteria. Given the 3D geometry of the nodule and 2D slice-by-slice assessment by radiologists, manual spiculation/lobulation annotation is a tedious task and thus no public datasets exist to date for probing the importance of these clinically-reported features in the SOTA malignancy prediction algorithms. As part of this paper, we release a large-scale Clinically-Interpretable Radiomics Dataset, CIRDataset, containing 956 radiologist QA/QC'ed spiculation/lobulation annotations on segmented lung nodules from two public datasets, LIDC-IDRI (N=883) and LUNGx (N=73). We also present an end-to-end deep learning model based on multi-class Voxel2Mesh extension to segment nodules (while preserving spikes), classify spikes (sharp/spiculation and curved/lobulation), and perform malignancy prediction. Previous methods have performed malignancy prediction for LIDC and LUNGx datasets but without robust attribution to any clinically reported/actionable features (due to known hyperparameter sensitivity issues with general attribution schemes). With the release of this comprehensively-annotated CIRDataset and end-to-end deep learning baseline, we hope that malignancy prediction methods can validate their explanations, benchmark against our baseline, and provide clinically-actionable insights. Dataset, code, pretrained models, and docker containers are available at https://github.com/nadeemlab/CIR. </details>
<details>	<summary>注释</summary>	MICCAI 2022 </details>
<details>	<summary>邮件日期</summary>	2022年07月01日</details>

# 490、RISP的情况：减少指令脉冲处理器
- [ ] The Case for RISP: A Reduced Instruction Spiking Processor 
时间：2022年06月28日                         第一作者：James S. Plank                       [链接](https://arxiv.org/abs/2206.14016).                     
## 摘要：本文介绍了精简指令脉冲处理器RISP。虽然大多数脉冲神经处理器基于大脑，或来自大脑的概念，但我们提出了一种简化而非复杂的脉冲处理器。因此，它具有离散集成周期、可配置泄漏等特点。我们提出了RISP的计算模型，并强调了其简单性的优点。我们演示了它如何帮助开发用于简单计算任务的手工构建的神经网络，详细介绍了如何使用它来简化使用更复杂的机器学习技术构建的神经网络，并演示了它的性能如何与其他脉冲神经处理器类似。
<details>	<summary>英文摘要</summary>	In this paper, we introduce RISP, a reduced instruction spiking processor. While most spiking neuroprocessors are based on the brain, or notions from the brain, we present the case for a spiking processor that simplifies rather than complicates. As such, it features discrete integration cycles, configurable leak, and little else. We present the computing model of RISP and highlight the benefits of its simplicity. We demonstrate how it aids in developing hand built neural networks for simple computational tasks, detail how it may be employed to simplify neural networks built with more complicated machine learning techniques, and demonstrate how it performs similarly to other spiking neurprocessors. </details>
<details>	<summary>注释</summary>	5 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2022年06月29日</details>

# 489、短时可塑性神经元学习和遗忘
- [ ] Short-Term Plasticity Neurons Learning to Learn and Forget 
时间：2022年06月28日                         第一作者：Hector Garcia Rodriguez                       [链接](https://arxiv.org/abs/2206.14048).                     
## 摘要：短时可塑性（STP）是一种在大脑皮层突触中储存衰退记忆的机制。在计算实践中，虽然理论预测STP是某些动态任务的最佳解决方案，但它主要用于脉冲神经元的小生境。在这里，我们提出了一种新型的递归神经单元，STP神经元（STPN），它确实非常强大。其关键机制是突触具有一种状态，通过突触内的自循环连接在时间中传播。这种公式可以通过时间反向传播来训练可塑性，从而形成一种短期内学会学习和忘记的形式。STPN优于所有测试的替代方案，即RNN、LSTM、其他具有快速权重和可微塑性的模型。我们在监督学习和强化学习（RL）以及联想检索、迷宫探索、雅达利视频游戏和MuJoCo机器人等任务中都证实了这一点。此外，我们计算出，在神经形态或生物电路中，STPN最大限度地减少了跨模型的能量消耗，因为它动态抑制了单个突触。基于这些，生物STP可能是一个强大的进化吸引子，可以最大限度地提高效率和计算能力。STPN现在也为广泛的机器学习实践带来了这些神经形态的优势。代码位于https://github.com/NeuromorphicComputing/stpn
<details>	<summary>英文摘要</summary>	Short-term plasticity (STP) is a mechanism that stores decaying memories in synapses of the cerebral cortex. In computing practice, STP has been used, but mostly in the niche of spiking neurons, even though theory predicts that it is the optimal solution to certain dynamic tasks. Here we present a new type of recurrent neural unit, the STP Neuron (STPN), which indeed turns out strikingly powerful. Its key mechanism is that synapses have a state, propagated through time by a self-recurrent connection-within-the-synapse. This formulation enables training the plasticity with backpropagation through time, resulting in a form of learning to learn and forget in the short term. The STPN outperforms all tested alternatives, i.e. RNNs, LSTMs, other models with fast weights, and differentiable plasticity. We confirm this in both supervised and reinforcement learning (RL), and in tasks such as Associative Retrieval, Maze Exploration, Atari video games, and MuJoCo robotics. Moreover, we calculate that, in neuromorphic or biological circuits, the STPN minimizes energy consumption across models, as it depresses individual synapses dynamically. Based on these, biological STP may have been a strong evolutionary attractor that maximizes both efficiency and computational power. The STPN now brings these neuromorphic advantages also to a broad spectrum of machine learning practice. Code is available at https://github.com/NeuromorphicComputing/stpn </details>
<details>	<summary>注释</summary>	Accepted at ICML 2022 Journal-ref: Proceedings of the 39th International Conference on Machine Learning, Baltimore, Maryland, USA, PMLR 162, 2022 </details>
<details>	<summary>邮件日期</summary>	2022年06月29日</details>

# 488、利用脉冲神经网络中的神经调制突触可塑性学习在线学习
- [ ] Learning to learn online with neuromodulated synaptic plasticity in spiking neural networks 
时间：2022年06月28日                         第一作者：Samuel Schmidgall                       [链接](https://arxiv.org/abs/2206.12520).                     
<details>	<summary>邮件日期</summary>	2022年06月29日</details>

# 487、脉冲神经网络的能量有效知识提取
- [ ] Energy-efficient Knowledge Distillation for Spiking Neural Networks 
时间：2022年06月27日                         第一作者：Dongjin Lee                       [链接](https://arxiv.org/abs/2106.07172).                     
<details>	<summary>注释</summary>	The manuscript was withdrawn because it contains inappropriate content for posting </details>
<details>	<summary>邮件日期</summary>	2022年06月28日</details>

# 486、动态RRAM阵列上基于梯度的神经形态学习
- [ ] Gradient-based Neuromorphic Learning on Dynamical RRAM Arrays 
时间：2022年06月26日                         第一作者：Peng Zhou                       [链接](https://arxiv.org/abs/2206.12992).                     
## 摘要：我们提出了MEMprop，即采用基于梯度的学习来训练全记忆脉冲神经网络（MSNN）。我们的方法利用固有的器件动态来触发自然产生的电压脉冲。忆阻动力学发出的这些脉冲本质上是模拟的，因此是完全可微的，这消除了对脉冲神经网络（SNN）文献中流行的替代梯度方法的需要。忆阻神经网络通常要么将忆阻器集成为映射离线训练网络的突触，要么依赖联想学习机制来训练忆阻神经元网络。相反，我们将时间反向传播（BPTT）训练算法直接应用于记忆神经元和突触的模拟SPICE模型。我们的实现是完全记忆的，因为突触权重和脉冲神经元都集成在电阻RAM（RRAM）阵列上，而不需要额外的电路来实现脉冲动态，例如模数转换器（ADC）或阈值比较器。因此，高阶电生理效应被充分利用，以在运行时使用记忆神经元的状态驱动动力学。通过转向基于非近似梯度的学习，我们在之前报告的轻量级密集完全多通道神经网络中获得了在多个基准上具有高度竞争力的准确性。
<details>	<summary>英文摘要</summary>	We present MEMprop, the adoption of gradient-based learning to train fully memristive spiking neural networks (MSNNs). Our approach harnesses intrinsic device dynamics to trigger naturally arising voltage spikes. These spikes emitted by memristive dynamics are analog in nature, and thus fully differentiable, which eliminates the need for surrogate gradient methods that are prevalent in the spiking neural network (SNN) literature. Memristive neural networks typically either integrate memristors as synapses that map offline-trained networks, or otherwise rely on associative learning mechanisms to train networks of memristive neurons. We instead apply the backpropagation through time (BPTT) training algorithm directly on analog SPICE models of memristive neurons and synapses. Our implementation is fully memristive, in that synaptic weights and spiking neurons are both integrated on resistive RAM (RRAM) arrays without the need for additional circuits to implement spiking dynamics, e.g., analog-to-digital converters (ADCs) or thresholded comparators. As a result, higher-order electrophysical effects are fully exploited to use the state-driven dynamics of memristive neurons at run time. By moving towards non-approximate gradient-based learning, we obtain highly competitive accuracy amongst previously reported lightweight dense fully MSNNs on several benchmarks. </details>
<details>	<summary>邮件日期</summary>	2022年06月28日</details>

# 485、利用脉冲神经网络中的神经调制突触可塑性学习在线学习
- [ ] Learning to learn online with neuromodulated synaptic plasticity in spiking neural networks 
时间：2022年06月25日                         第一作者：Samuel Schmidgall                       [链接](https://arxiv.org/abs/2206.12520).                     
## 摘要：我们提出，为了利用我们对神经科学的理解来进行机器学习，我们必须首先拥有强大的工具来训练类似大脑的学习模型。虽然在理解大脑学习动态方面取得了实质性进展，但神经科学衍生的学习模型尚未证明与梯度下降等深度学习方法具有相同的性能。受使用梯度下降的机器学习成功的启发，我们证明了神经科学中的神经调制突触可塑性模型可以在脉冲神经网络（SNN）中训练，其框架是通过梯度下降学习，以解决具有挑战性的在线学习问题。该框架为开发神经科学启发的在线学习算法开辟了一条新途径。
<details>	<summary>英文摘要</summary>	We propose that in order to harness our understanding of neuroscience toward machine learning, we must first have powerful tools for training brain-like models of learning. Although substantial progress has been made toward understanding the dynamics of learning in the brain, neuroscience-derived models of learning have yet to demonstrate the same performance capabilities as methods in deep learning such as gradient descent. Inspired by the successes of machine learning using gradient descent, we demonstrate that models of neuromodulated synaptic plasticity from neuroscience can be trained in Spiking Neural Networks (SNNs) with a framework of learning to learn through gradient descent to address challenging online learning problems. This framework opens a new path toward developing neuroscience inspired online learning algorithms. </details>
<details>	<summary>邮件日期</summary>	2022年06月28日</details>

# 484、使用剩余脉冲神经网络进行精确特征提取的关键
- [ ] Keys to Accurate Feature Extraction Using Residual Spiking Neural Networks 
时间：2022年06月23日                         第一作者：Alex Vicente-Sola                       [链接](https://arxiv.org/abs/2111.05955).                     
<details>	<summary>注释</summary>	17 pages, 6 figures, 17 tables ACM-class: I.2.6; I.2.10; I.4.8; I.5.2; D.2.13 </details>
<details>	<summary>邮件日期</summary>	2022年06月24日</details>

# 483、基于垂直腔面发射激光器耦合的共振隧道二极管的人工光电脉冲神经元
- [ ] Artificial optoelectronic spiking neuron based on a resonant tunnelling diode coupled to a vertical cavity surface emitting laser 
时间：2022年06月22日                         第一作者：Mat\v{e}j Hejda                       [链接](https://arxiv.org/abs/2206.11044).                     
## 摘要：可激发光电子器件是在神经形态（脑激励）光子系统中实现人工脉冲神经元的关键构件之一。本文介绍并实验研究了一种光电（O/E/O）人工神经元，该神经元由耦合到光电探测器的谐振隧道二极管（RTD）作为接收器和垂直腔面发射激光器作为发射器构建。我们证明了一个定义良好的兴奋性阈值，在该阈值以上，该神经元产生100 ns的光学脉冲反应，具有典型的神经样不应期。我们利用其扇入功能执行设备内符合检测（逻辑AND）和排他逻辑OR（XOR）任务。这些结果首次对具有输入和输出光学（I/O）终端的基于RTD的脉冲光电神经元中的确定性触发和任务进行了实验验证。此外，我们还从理论上研究了所提出的系统在结合纳米RTD元件和纳米激光器的单片设计中实现纳米光子的前景；因此，证明了基于RTD的集成可激发节点在未来神经形态光子硬件中用于低占地面积、高速光电脉冲神经元的潜力。
<details>	<summary>英文摘要</summary>	Excitable optoelectronic devices represent one of the key building blocks for implementation of artificial spiking neurons in neuromorphic (brain-inspired) photonic systems. This work introduces and experimentally investigates an opto-electro-optical (O/E/O) artificial neuron built with a resonant tunnelling diode (RTD) coupled to a photodetector as a receiver and a vertical cavity surface emitting laser as a the transmitter. We demonstrate a well defined excitability threshold, above which this neuron produces 100 ns optical spiking responses with characteristic neural-like refractory period. We utilise its fan-in capability to perform in-device coincidence detection (logical AND) and exclusive logical OR (XOR) tasks. These results provide first experimental validation of deterministic triggering and tasks in an RTD-based spiking optoelectronic neuron with both input and output optical (I/O) terminals. Furthermore, we also investigate in theory the prospects of the proposed system for its nanophotonic implementation with a monolithic design combining a nanoscale RTD element and a nanolaser; therefore demonstrating the potential of integrated RTD-based excitable nodes for low footprint, high-speed optoelectronic spiking neurons in future neuromorphic photonic hardware. </details>
<details>	<summary>注释</summary>	5 figures </details>
<details>	<summary>邮件日期</summary>	2022年06月23日</details>

# 482、使用生物学上合理的脉冲潜伏期代码和赢家通吃抑制的有效视觉对象表示
- [ ] Efficient visual object representation using a biologically plausible spike-latency code and winner-take-all inhibition 
时间：2022年06月22日                         第一作者：Melani Sanchez-Garcia                       [链接](https://arxiv.org/abs/2205.10338).                     
<details>	<summary>邮件日期</summary>	2022年06月23日</details>

# 481、TCJA-SNN：脉冲神经网络的时间通道联合注意
- [ ] TCJA-SNN: Temporal-Channel Joint Attention for Spiking Neural Networks 
时间：2022年06月21日                         第一作者：Rui-Jie Zhu                       [链接](https://arxiv.org/abs/2206.10177).                     
## 摘要：脉冲神经网络（SNN）是一种通过模拟神经元利用时间信息实现更高效数据深度学习的实用方法。在本文中，我们提出了时间通道联合注意（TCJA）架构单元，这是一种高效的SNN技术，它依赖于注意机制，通过在空间和时间维度上有效地增强脉冲序列的相关性。我们的主要技术贡献在于：1）通过压缩操作将脉冲流压缩为平均矩阵，然后使用两种局部注意机制和有效的一维卷积，以灵活的方式建立用于特征提取的时间和通道关系。2） 利用交叉卷积融合（CCF）层建模时间范围和通道范围之间的相互依赖关系，打破了两个维度的独立性，实现了特征之间的交互。通过联合探索和重新校准数据流，我们的方法在所有测试的主流静态和神经形态数据集（包括Fashion MNIST、CIFAR10-DVS、N-Caltech 101和DVS128手势）上的分类精度最高，超过最先进的（SOTA）15.7%。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) is a practical approach toward more data-efficient deep learning by simulating neurons leverage on temporal information. In this paper, we propose the Temporal-Channel Joint Attention (TCJA) architectural unit, an efficient SNN technique that depends on attention mechanisms, by effectively enforcing the relevance of spike sequence along both spatial and temporal dimensions. Our essential technical contribution lies on: 1) compressing the spike stream into an average matrix by employing the squeeze operation, then using two local attention mechanisms with an efficient 1-D convolution to establish temporal-wise and channel-wise relations for feature extraction in a flexible fashion. 2) utilizing the Cross Convolutional Fusion (CCF) layer for modeling inter-dependencies between temporal and channel scope, which breaks the independence of the two dimensions and realizes the interaction between features. By virtue of jointly exploring and recalibrating data stream, our method outperforms the state-of-the-art (SOTA) by up to 15.7% in terms of top-1 classification accuracy on all tested mainstream static and neuromorphic datasets, including Fashion-MNIST, CIFAR10-DVS, N-Caltech 101, and DVS128 Gesture. </details>
<details>	<summary>邮件日期</summary>	2022年06月22日</details>

# 480、波动驱动的脉冲神经网络训练初始化
- [ ] Fluctuation-driven initialization for spiking neural network training 
时间：2022年06月21日                         第一作者：Julian Rossbroich                       [链接](https://arxiv.org/abs/2206.10226).                     
## 摘要：脉冲神经网络（SNN）是大脑中低功耗、容错信息处理的基础，当在合适的神经形态硬件加速器上实现时，可以构成传统深度神经网络的高效替代方案。然而，实例化SNN以解决硅片中的复杂计算任务仍然是一个重大挑战。替代梯度（SG）技术已成为端到端训练SNN的标准解决方案。然而，它们的成功取决于突触权重初始化，类似于传统的人工神经网络（ANN）。然而，与人工神经网络的情况不同，SNN的良好初始状态是由什么组成的仍然是难以捉摸的。在这里，受大脑中常见的波动驱动机制的启发，我们开发了SNN的一般初始化策略。具体来说，我们推导了与数据相关的权重初始化的实用解决方案，以确保广泛使用的泄漏积分和激发（LIF）神经元中的波动驱动激发。我们的经验表明，当使用SGs训练时，按照我们的策略初始化的SNN表现出优越的学习性能。这些发现概括了几个数据集和SNN架构，包括完全连接、深度卷积、周期性和更符合Dale定律的生物学合理SNN。因此，波动驱动初始化提供了一种实用、通用且易于实现的策略，用于改善神经形态工程和计算神经科学中不同任务的SNN训练性能。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) underlie low-power, fault-tolerant information processing in the brain and could constitute a power-efficient alternative to conventional deep neural networks when implemented on suitable neuromorphic hardware accelerators. However, instantiating SNNs that solve complex computational tasks in-silico remains a significant challenge. Surrogate gradient (SG) techniques have emerged as a standard solution for training SNNs end-to-end. Still, their success depends on synaptic weight initialization, similar to conventional artificial neural networks (ANNs). Yet, unlike in the case of ANNs, it remains elusive what constitutes a good initial state for an SNN. Here, we develop a general initialization strategy for SNNs inspired by the fluctuation-driven regime commonly observed in the brain. Specifically, we derive practical solutions for data-dependent weight initialization that ensure fluctuation-driven firing in the widely used leaky integrate-and-fire (LIF) neurons. We empirically show that SNNs initialized following our strategy exhibit superior learning performance when trained with SGs. These findings generalize across several datasets and SNN architectures, including fully connected, deep convolutional, recurrent, and more biologically plausible SNNs obeying Dale's law. Thus fluctuation-driven initialization provides a practical, versatile, and easy-to-implement strategy for improving SNN training performance on diverse tasks in neuromorphic engineering and computational neuroscience. </details>
<details>	<summary>注释</summary>	30 pages, 7 figures, plus supplementary material </details>
<details>	<summary>邮件日期</summary>	2022年06月22日</details>

# 479、检验脉冲神经网络对非理想忆阻交叉的鲁棒性
- [ ] Examining the Robustness of Spiking Neural Networks on Non-ideal Memristive Crossbars 
时间：2022年06月20日                         第一作者：Abhiroop Bhattacharjee                       [链接](https://arxiv.org/abs/2206.09599).                     
## 摘要：由于其异步、稀疏和二进制信息处理，脉冲神经网络（SNN）最近成为人工神经网络（ANN）的低功耗替代方案。为了提高能量效率和吞吐量，SNN可以在忆阻式交叉杆上实现，其中乘法和累加（MAC）操作在模拟域中使用新兴的非易失性存储器（NVM）设备实现。尽管SNN与忆阻交叉码兼容，但很少有人关注内在交叉码的非理想性和随机性对SNN性能的影响。在本文中，我们对SNN在非理想交叉上的鲁棒性进行了全面分析。我们研究了通过学习算法（如代理梯度和ANN-SNN转换）训练的SNN。我们的结果表明，跨多个时间步长的重复交叉计算会导致错误累积，导致SNN推理期间的性能大幅下降。我们进一步表明，当部署在忆阻交叉上时，使用较少时间步长训练的SNN可以获得更好的准确性。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have recently emerged as the low-power alternative to Artificial Neural Networks (ANNs) owing to their asynchronous, sparse, and binary information processing. To improve the energy-efficiency and throughput, SNNs can be implemented on memristive crossbars where Multiply-and-Accumulate (MAC) operations are realized in the analog domain using emerging Non-Volatile-Memory (NVM) devices. Despite the compatibility of SNNs with memristive crossbars, there is little attention to study on the effect of intrinsic crossbar non-idealities and stochasticity on the performance of SNNs. In this paper, we conduct a comprehensive analysis of the robustness of SNNs on non-ideal crossbars. We examine SNNs trained via learning algorithms such as, surrogate gradient and ANN-SNN conversion. Our results show that repetitive crossbar computations across multiple time-steps induce error accumulation, resulting in a huge performance drop during SNN inference. We further show that SNNs trained with a smaller number of time-steps achieve better accuracy when deployed on memristive crossbars. </details>
<details>	<summary>注释</summary>	Accepted in ACM/IEEE International Symposium on Low Power Electronics and Design (ISLPED), 2022 DOI: 10.1145/3531437.3539729 </details>
<details>	<summary>邮件日期</summary>	2022年06月22日</details>

# 478、SNN2ANN：一种快速高效的脉冲神经网络训练框架
- [ ] SNN2ANN: A Fast and Memory-Efficient Training Framework for Spiking Neural Networks 
时间：2022年06月19日                         第一作者：Jianxiong Tang                       [链接](https://arxiv.org/abs/2206.09449).                     
## 摘要：脉冲神经网络是低功耗环境下的有效计算模型。基于峰值的BP算法和神经网络到SNN（ANN2SNN）的转换是SNN训练的成功技术。然而，基于峰值的BP训练速度较慢，需要大量内存。尽管ANN2NN提供了一种低成本的SNN训练方法，但它需要许多推理步骤来模拟经过良好训练的ANN以获得良好的性能。在本文中，我们提出了一个SNN-to-ANN（SNN2ANN）框架，以快速高效地训练SNN。SNN2ANN由两部分组成：a）ANN和SNN之间的权重共享架构和b）脉冲映射单元。首先，该架构在ANN分支上训练权重共享参数，从而实现SNN的快速训练和低内存开销。其次，脉冲映射单元确保神经网络的激活值是脉冲特征。因此，可以通过训练神经网络分支来优化SNN的分类误差。此外，我们设计了一种自适应阈值调整（ATA）算法来解决噪声脉冲问题。实验结果表明，基于SNN2ANN的模型在基准数据集（CIFAR10、CIFAR100和Tiny ImageNet）上表现良好。此外，SNN2ANN在基于峰值的BP模型的0.625x时间步长、0.377x训练时间、0.27x GPU内存成本和0.33x峰值活动下可以达到相当的精度。
<details>	<summary>英文摘要</summary>	Spiking neural networks are efficient computation models for low-power environments. Spike-based BP algorithms and ANN-to-SNN (ANN2SNN) conversions are successful techniques for SNN training. Nevertheless, the spike-base BP training is slow and requires large memory costs. Though ANN2NN provides a low-cost way to train SNNs, it requires many inference steps to mimic the well-trained ANN for good performance. In this paper, we propose a SNN-to-ANN (SNN2ANN) framework to train the SNN in a fast and memory-efficient way. The SNN2ANN consists of 2 components: a) a weight sharing architecture between ANN and SNN and b) spiking mapping units. Firstly, the architecture trains the weight-sharing parameters on the ANN branch, resulting in fast training and low memory costs for SNN. Secondly, the spiking mapping units ensure that the activation values of the ANN are the spiking features. As a result, the classification error of the SNN can be optimized by training the ANN branch. Besides, we design an adaptive threshold adjustment (ATA) algorithm to address the noisy spike problem. Experiment results show that our SNN2ANN-based models perform well on the benchmark datasets (CIFAR10, CIFAR100, and Tiny-ImageNet). Moreover, the SNN2ANN can achieve comparable accuracy under 0.625x time steps, 0.377x training time, 0.27x GPU memory costs, and 0.33x spike activities of the Spike-based BP model. </details>
<details>	<summary>邮件日期</summary>	2022年06月22日</details>

# 477、tinySNN：迈向记忆和能量高效的脉冲神经网络
- [ ] tinySNN: Towards Memory- and Energy-Efficient Spiking Neural Networks 
时间：2022年06月17日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2206.08656).                     
## 摘要：较大的脉冲神经网络（SNN）模型通常是有利的，因为它们可以提供更高的精度。然而，在资源和能源受限的嵌入式平台上使用此类模型效率低下。为此，我们提出了一个tinySNN框架，该框架在训练和推理阶段优化了SNN处理的内存和能量需求，同时保持了较高的准确性。它是通过减少SNN操作、提高学习质量、量化SNN参数和选择适当的SNN模型来实现的。此外，我们的tinySNN对不同的SNN参数（即权重和神经元参数）进行量化，以最大限度地压缩，同时探索量化方案、精度水平和舍入方案的不同组合，以找到提供可接受精度的模型。实验结果表明，与基线网络相比，我们的tinySNN在不损失准确性的情况下显著减少了snn的内存占用和能耗。因此，我们的tinySNN有效地压缩了给定的SNN模型，以节省内存和能源的方式实现高精度，从而使SNN能够用于资源和能源受限的嵌入式应用程序。
<details>	<summary>英文摘要</summary>	Larger Spiking Neural Network (SNN) models are typically favorable as they can offer higher accuracy. However, employing such models on the resource- and energy-constrained embedded platforms is inefficient. Towards this, we present a tinySNN framework that optimizes the memory and energy requirements of SNN processing in both the training and inference phases, while keeping the accuracy high. It is achieved by reducing the SNN operations, improving the learning quality, quantizing the SNN parameters, and selecting the appropriate SNN model. Furthermore, our tinySNN quantizes different SNN parameters (i.e., weights and neuron parameters) to maximize the compression while exploring different combinations of quantization schemes, precision levels, and rounding schemes to find the model that provides acceptable accuracy. The experimental results demonstrate that our tinySNN significantly reduces the memory footprint and the energy consumption of SNNs without accuracy loss as compared to the baseline network. Therefore, our tinySNN effectively compresses the given SNN model to achieve high accuracy in a memory- and energy-efficient manner, hence enabling the employment of SNNs for the resource- and energy-constrained embedded applications. </details>
<details>	<summary>注释</summary>	9 figures </details>
<details>	<summary>邮件日期</summary>	2022年06月20日</details>

# 476、基于帧和基于事件的单目标定位的脉冲神经网络
- [ ] Spiking Neural Networks for Frame-based and Event-based Single Object Localization 
时间：2022年06月13日                         第一作者：Sami Barchid                       [链接](https://arxiv.org/abs/2206.06506).                     
## 摘要：作为人工神经网络的节能替代品，脉冲神经网络显示出了很大的前景。然而，使用常见的神经形态视觉基线（如分类）仍然难以理解传感器噪声和输入编码对网络活动和性能的影响。因此，针对基于帧和事件的传感器，我们提出了一种使用代理梯度下降法训练的单目标定位的脉冲神经网络方法。我们将我们的方法与类似的人工神经网络进行了比较，结果表明，我们的模型在准确性、对各种腐蚀的鲁棒性和较低的能耗方面具有竞争力/更好的性能。此外，我们还研究了神经编码方案对静态图像准确性、鲁棒性和能量效率的影响。我们的观察结果与之前关于生物似真学习规则的研究有很大不同，这有助于设计替代梯度训练架构，并为未来神经形态技术在噪声特性和数据编码方法方面的设计优先级提供了见解。
<details>	<summary>英文摘要</summary>	Spiking neural networks have shown much promise as an energy-efficient alternative to artificial neural networks. However, understanding the impacts of sensor noises and input encodings on the network activity and performance remains difficult with common neuromorphic vision baselines like classification. Therefore, we propose a spiking neural network approach for single object localization trained using surrogate gradient descent, for frame- and event-based sensors. We compare our method with similar artificial neural networks and show that our model has competitive/better performance in accuracy, robustness against various corruptions, and has lower energy consumption. Moreover, we study the impact of neural coding schemes for static images in accuracy, robustness, and energy efficiency. Our observations differ importantly from previous studies on bio-plausible learning rules, which helps in the design of surrogate gradient trained architectures, and offers insight to design priorities in future neuromorphic technologies in terms of noise characteristics and data encoding methods. </details>
<details>	<summary>注释</summary>	21 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2022年06月15日</details>

# 475、神经形态无线认知：用于远程推理的事件驱动语义通信
- [ ] Neuromorphic Wireless Cognition: Event-Driven Semantic Communications for Remote Inference 
时间：2022年06月13日                         第一作者：Jiechen Chen                       [链接](https://arxiv.org/abs/2206.06047).                     
## 摘要：神经形态计算是一种新兴的计算范式，它从批量处理转向在线、事件驱动的流数据处理。当与基于脉冲的传感器结合时，神经形态芯片可以通过仅在脉冲时间记录相关事件时消耗能量，并通过证明对环境中变化条件的低延迟响应，内在地适应数据分布的“语义”。本文提出了一种端到端的神经形态无线物联网系统设计，该系统集成了基于脉冲的传感、处理和通信。在拟议的神经通信系统中，每个传感设备都配备了一个神经形态传感器、一个脉冲神经网络（SNN）和一个具有多个天线的冲击无线电发射器。传输在共享衰落信道上发生，传输到配备多天线冲击无线电接收器和SNN的接收器。为了使接收器能够适应衰落信道条件，我们引入了一个超网络来使用导频控制解码SNN的权重。导频、编码SNN、解码SNN和超网络在多个信道实现中联合训练。结果表明，与传统的基于帧的数字解决方案以及替代的非自适应训练方法相比，该系统在时间精度和能耗指标方面有显著改进。
<details>	<summary>英文摘要</summary>	Neuromorphic computing is an emerging computing paradigm that moves away from batched processing towards the online, event-driven, processing of streaming data. Neuromorphic chips, when coupled with spike-based sensors, can inherently adapt to the "semantics" of the data distribution by consuming energy only when relevant events are recorded in the timing of spikes and by proving a low-latency response to changing conditions in the environment. This paper proposes an end-to-end design for a neuromorphic wireless Internet-of-Things system that integrates spike-based sensing, processing, and communication. In the proposed NeuroComm system, each sensing device is equipped with a neuromorphic sensor, a spiking neural network (SNN), and an impulse radio transmitter with multiple antennas. Transmission takes place over a shared fading channel to a receiver equipped with a multi-antenna impulse radio receiver and with an SNN. In order to enable adaptation of the receiver to the fading channel conditions, we introduce a hypernetwork to control the weights of the decoding SNN using pilots. Pilots, encoding SNNs, decoding SNN, and hypernetwork are jointly trained across multiple channel realizations. The proposed system is shown to significantly improve over conventional frame-based digital solutions, as well as over alternative non-adaptive training methods, in terms of time-to-accuracy and energy consumption metrics. </details>
<details>	<summary>注释</summary>	submitted </details>
<details>	<summary>邮件日期</summary>	2022年06月14日</details>

# 474、自动神经网络：走向节能脉冲神经网络
- [ ] AutoSNN: Towards Energy-Efficient Spiking Neural Networks 
时间：2022年06月13日                         第一作者：Byunggook Na                       [链接](https://arxiv.org/abs/2201.12738).                     
<details>	<summary>注释</summary>	Accepted in ICML22 </details>
<details>	<summary>邮件日期</summary>	2022年06月14日</details>

# 473、一种用于脉冲神经网络的突触阈值协同学习方法
- [ ] A Synapse-Threshold Synergistic Learning Approach for Spiking Neural Networks 
时间：2022年06月10日                         第一作者：Hongze Sun                       [链接](https://arxiv.org/abs/2206.06129).                     
## 摘要：脉冲神经网络（SNN）在各种智能场景中表现出了出色的性能。大多数现有的SNN训练方法都是基于突触可塑性的概念；然而，现实大脑中的学习也利用了神经元固有的非突触机制。生物神经元的脉冲阈值是一个关键的内在神经元特征，在毫秒时间尺度上表现出丰富的动态，被认为是促进神经信息处理的潜在机制。在本研究中，我们开发了一种新的协同学习方法，可以同时训练SNN中的突触权重和棘波阈值。使用突触阈值协同学习（STL SNN）训练的SNN在各种静态和神经形态数据集上的准确率显著高于使用突触学习（SL）和阈值学习（TL）两种单一学习模型训练的SNN。在训练期间，协同学习方法优化神经阈值，通过适当的触发频率为网络提供稳定的信号传输。进一步分析表明，STL SNN对噪声数据具有鲁棒性，并且对于深度网络结构具有低能耗。此外，通过引入广义联合决策框架（JDF），可以进一步提高STL-SNN的性能。总的来说，我们的研究结果表明，突触和内在非突触机制之间的生物学上合理的协同作用可能为开发高效的SNN学习方法提供了一种有希望的方法。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have demonstrated excellent capabilities in various intelligent scenarios. Most existing methods for training SNNs are based on the concept of synaptic plasticity; however, learning in the realistic brain also utilizes intrinsic non-synaptic mechanisms of neurons. The spike threshold of biological neurons is a critical intrinsic neuronal feature that exhibits rich dynamics on a millisecond timescale and has been proposed as an underlying mechanism that facilitates neural information processing. In this study, we develop a novel synergistic learning approach that simultaneously trains synaptic weights and spike thresholds in SNNs. SNNs trained with synapse-threshold synergistic learning (STL-SNNs) achieve significantly higher accuracies on various static and neuromorphic datasets than SNNs trained with two single-learning models of the synaptic learning (SL) and the threshold learning (TL). During training, the synergistic learning approach optimizes neural thresholds, providing the network with stable signal transmission via appropriate firing rates. Further analysis indicates that STL-SNNs are robust to noisy data and exhibit low energy consumption for deep network structures. Additionally, the performance of STL-SNN can be further improved by introducing a generalized joint decision framework (JDF). Overall, our findings indicate that biologically plausible synergies between synaptic and intrinsic non-synaptic mechanisms may provide a promising approach for developing highly efficient SNN learning methods. </details>
<details>	<summary>注释</summary>	13 pages, 9 figures, submitted to the IEEE Transactions on Neural Networks and Learning Systems (TNNLS) </details>
<details>	<summary>邮件日期</summary>	2022年06月14日</details>

# 472、基于稀疏学习脉冲的海马记忆模型的仿生实现
- [ ] A bio-inspired implementation of a sparse-learning spike-based hippocampus memory model 
时间：2022年06月10日                         第一作者：Daniel Casanueva-Morato                       [链接](https://arxiv.org/abs/2206.04924).                     
## 摘要：神经系统，更具体地说，大脑，能够简单有效地解决复杂问题，远远超过现代计算机。在这方面，神经形态工程是一个研究领域，其重点是模仿控制大脑的基本原理，以开发实现此类计算能力的系统。在这个领域中，生物启发学习和记忆系统仍然是一个有待解决的挑战，而这正是海马体所涉及的领域。它是大脑中起短期记忆作用的区域，允许学习和非结构化快速存储来自大脑皮层所有感觉核的信息及其随后的回忆。在这项工作中，我们提出了一种新的基于海马体的仿生记忆模型，该模型能够学习记忆，从线索（与其余内容相关的记忆的一部分）中回忆记忆，甚至在尝试使用相同线索学习其他人时忘记记忆。该模型已使用脉冲神经网络在SpiNNaker硬件平台上实现，并进行了一系列实验和测试，以证明其正确和预期的操作。提出的基于脉冲的记忆模型仅在接收到输入时才会产生脉冲，这是节能的，学习步骤需要7个时间步，调用先前存储的记忆需要6个时间步。这项工作首次提出了一个全功能的基于生物刺激棘波的海马记忆模型的硬件实现，为未来更复杂的神经形态系统的开发铺平了道路。
<details>	<summary>英文摘要</summary>	The nervous system, more specifically, the brain, is capable of solving complex problems simply and efficiently, far surpassing modern computers. In this regard, neuromorphic engineering is a research field that focuses on mimicking the basic principles that govern the brain in order to develop systems that achieve such computational capabilities. Within this field, bio-inspired learning and memory systems are still a challenge to be solved, and this is where the hippocampus is involved. It is the region of the brain that acts as a short-term memory, allowing the learning and unstructured and rapid storage of information from all the sensory nuclei of the cerebral cortex and its subsequent recall. In this work, we propose a novel bio-inspired memory model based on the hippocampus with the ability to learn memories, recall them from a cue (a part of the memory associated with the rest of the content) and even forget memories when trying to learn others with the same cue. This model has been implemented on the SpiNNaker hardware platform using Spiking Neural Networks, and a set of experiments and tests were performed to demonstrate its correct and expected operation. The proposed spike-based memory model generates spikes only when it receives an input, being energy efficient, and it needs 7 timesteps for the learning step and 6 timesteps for recalling a previously-stored memory. This work presents the first hardware implementation of a fully functional bio-inspired spike-based hippocampus memory model, paving the road for the development of future more complex neuromorphic systems. </details>
<details>	<summary>注释</summary>	15 pages, 7 figures, 3 tables, journal, Neural Networks </details>
<details>	<summary>邮件日期</summary>	2022年06月13日</details>

# 471、用基于电势的归一化方法解决脉冲深度Q网络中脉冲特征信息消失问题
- [ ] Solving the Spike Feature Information Vanishing Problem in Spiking Deep Q Network with Potential Based Normalization 
时间：2022年06月08日                         第一作者：Yinqian Sun                       [链接](https://arxiv.org/abs/2206.03654).                     
## 摘要：脑激励脉冲神经网络（SNN）已成功应用于许多模式识别领域。基于SNN的深度结构在图像分类、目标检测等感知任务中取得了显著的成果。然而，深度SNN在强化学习任务中的应用仍然是一个有待探索的问题。虽然之前已有关于SNN和RL结合的研究，但大多数研究集中在具有浅层网络的机器人控制问题或使用ANN-SNN转换方法实现脉冲深度Q网络（SDQN）。在这项工作中，我们从数学上分析了半定量化网络中脉冲信号特征消失的问题，并提出了一种基于电势的层归一化（pbLN）方法来直接训练脉冲深度Q网络。实验表明，与最先进的ANN-SNN转换方法和其他SDQN工作相比，提出的pbLN脉冲深度Q网络（PL-SDQN）在Atari游戏任务中取得了更好的性能。
<details>	<summary>英文摘要</summary>	Brain inspired spiking neural networks (SNNs) have been successfully applied to many pattern recognition domains. The SNNs based deep structure have achieved considerable results in perceptual tasks, such as image classification, target detection. However, the application of deep SNNs in reinforcement learning (RL) tasks is still a problem to be explored. Although there have been previous studies on the combination of SNNs and RL, most of them focus on robotic control problems with shallow networks or using ANN-SNN conversion method to implement spiking deep Q Network (SDQN). In this work, we mathematically analyzed the problem of the disappearance of spiking signal features in SDQN and proposed a potential based layer normalization(pbLN) method to directly train spiking deep Q networks. Experiment shows that compared with state-of-art ANN-SNN conversion method and other SDQN works, the proposed pbLN spiking deep Q networks (PL-SDQN) achieved better performance on Atari game tasks. </details>
<details>	<summary>邮件日期</summary>	2022年06月09日</details>

# 470、基于SpiNNaker上的脉冲神经网络，使用类神经逻辑门构建基于脉冲的存储器
- [ ] Construction of a spike-based memory using neural-like logic gates based on Spiking Neural Networks on SpiNNaker 
时间：2022年06月08日                         第一作者：Alvaro Ayuso-Martinez                       [链接](https://arxiv.org/abs/2206.03957).                     
## 摘要：由于神经形态工程作为一个研究领域的巨大潜力，它集中了大量研究人员的努力，以寻求利用生物神经系统和大脑作为一个整体的优势，设计更高效和实时的应用程序。为了开发尽可能接近生物学的应用，人们使用了脉冲神经网络（SNN），认为其在生物学上是合理的，并形成了第三代人工神经网络（ANN）。由于一些基于SNN的应用程序可能需要存储数据以便以后使用，因此需要在数字电路中以及以某种形式在生物学中存在的数据，即脉冲存储器。这项工作介绍了内存的脉冲实现，它是计算机体系结构中最重要的组件之一，在设计全脉冲计算机时可能是必不可少的。在设计这种脉冲存储器的过程中，还实现并测试了不同的中间组件。测试在SpiNNaker神经形态平台上进行，并允许验证用于构建所述块的方法。此外，本文还深入研究了如何使用这种方法构建脉冲块，并将其与其他类似工作中使用的脉冲组件进行了比较，这些类似工作重点关注脉冲组件的设计，包括脉冲逻辑门和脉冲内存。所有实现的块和开发的测试都可以在公共存储库中获得。
<details>	<summary>英文摘要</summary>	Neuromorphic engineering concentrates the efforts of a large number of researchers due to its great potential as a field of research, in a search for the exploitation of the advantages of the biological nervous system and the brain as a whole for the design of more efficient and real-time capable applications. For the development of applications as close to biology as possible, Spiking Neural Networks (SNNs) are used, considered biologically-plausible and that form the third generation of Artificial Neural Networks (ANNs). Since some SNN-based applications may need to store data in order to use it later, something that is present both in digital circuits and, in some form, in biology, a spiking memory is needed. This work presents a spiking implementation of a memory, which is one of the most important components in the computer architecture, and which could be essential in the design of a fully spiking computer. In the process of designing this spiking memory, different intermediate components were also implemented and tested. The tests were carried out on the SpiNNaker neuromorphic platform and allow to validate the approach used for the construction of the presented blocks. In addition, this work studies in depth how to build spiking blocks using this approach and includes a comparison between it and those used in other similar works focused on the design of spiking components, which include both spiking logic gates and spiking memory. All implemented blocks and developed tests are available in a public repository. </details>
<details>	<summary>注释</summary>	15 pages, 9 figures, 8 tables, journal paper, Neural Networks </details>
<details>	<summary>邮件日期</summary>	2022年06月09日</details>

# 469、脉冲选通流：一种用于在线手势识别的基于层次结构的脉冲神经网络
- [ ] The Spike Gating Flow: A Hierarchical Structure Based Spiking Neural Network for Online Gesture Recognition 
时间：2022年06月07日                         第一作者：Zihao Zhao                       [链接](https://arxiv.org/abs/2206.01910).                     
<details>	<summary>邮件日期</summary>	2022年06月08日</details>

# 468、SpikiLi：基于激光雷达的自动驾驶实时目标检测的脉冲模拟
- [ ] SpikiLi: A Spiking Simulation of LiDAR based Real-time Object Detection for Autonomous Driving 
时间：2022年06月06日                         第一作者：Sambit Mohapatra                       [链接](https://arxiv.org/abs/2206.02876).                     
## 摘要：脉冲神经网络是一种新的神经网络设计方法，有望极大地提高功率效率、计算效率和处理延迟。他们通过使用异步基于脉冲的数据流、基于事件的信号生成、处理和修改神经元模型来实现这一点，使其与生物神经元非常相似。虽然一些初步研究已经显示出对常见深度学习任务适用性的重要初步证据，但它们在复杂现实任务中的应用相对较低。在这项工作中，我们首先说明了脉冲神经网络对复杂深度学习任务的适用性，即基于激光雷达的自动驾驶三维物体检测。其次，我们使用预训练的卷积神经网络逐步演示了模拟脉冲行为。我们在仿真中对脉冲神经网络的关键方面进行了密切建模，并在GPU上实现了等效的运行时间和精度。当该模型在神经形态硬件上实现时，我们希望能够显著提高功率效率。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks are a recent and new neural network design approach that promises tremendous improvements in power efficiency, computation efficiency, and processing latency. They do so by using asynchronous spike-based data flow, event-based signal generation, processing, and modifying the neuron model to resemble biological neurons closely. While some initial works have shown significant initial evidence of applicability to common deep learning tasks, their applications in complex real-world tasks has been relatively low. In this work, we first illustrate the applicability of spiking neural networks to a complex deep learning task namely Lidar based 3D object detection for automated driving. Secondly, we make a step-by-step demonstration of simulating spiking behavior using a pre-trained convolutional neural network. We closely model essential aspects of spiking neural networks in simulation and achieve equivalent run-time and accuracy on a GPU. When the model is realized on a neuromorphic hardware, we expect to have significantly improved power efficiency. </details>
<details>	<summary>注释</summary>	Accepted at Workshop on Event Sensing and Neuromorphic Engineering - 8th International Conference on Event-based Control, Communication, and Signal Processing </details>
<details>	<summary>邮件日期</summary>	2022年06月08日</details>

# 467、支持新兴神经编码的资源高效脉冲神经网络加速器
- [ ] A Resource-efficient Spiking Neural Network Accelerator Supporting Emerging Neural Encoding 
时间：2022年06月06日                         第一作者：Daniel Gerlinghoff                       [链接](https://arxiv.org/abs/2206.02495).                     
## 摘要：脉冲神经网络（SNN）由于其低功耗无乘法计算和与人类神经系统中的生物过程更为相似，最近获得了发展势头。然而，SNN需要很长的脉冲序列（高达1000）才能达到与大型模型的人工神经网络（ANN）类似的精度，这抵消了效率，并阻碍了其在现实世界用例的低功率系统中的应用。为了缓解这个问题，提出了新的神经编码方案来缩短脉冲序列，同时保持高精度。然而，目前的SNN加速器不能很好地支持新兴的编码方案。在这项工作中，我们提出了一种新的硬件架构，可以通过新兴的神经编码有效地支持SNN。我们的实现具有节能和面积效率高的处理单元，提高了并行性，减少了内存访问。我们在FPGA上验证了加速器，在功耗和延迟方面分别比以前的工作提高了25%和90%。同时，高面积效率允许我们扩展大型神经网络模型。据我们所知，这是首次将大型神经网络模型VGG部署在基于FPGA的物理神经形态硬件上。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) recently gained momentum due to their low-power multiplication-free computing and the closer resemblance of biological processes in the nervous system of humans. However, SNNs require very long spike trains (up to 1000) to reach an accuracy similar to their artificial neural network (ANN) counterparts for large models, which offsets efficiency and inhibits its application to low-power systems for real-world use cases. To alleviate this problem, emerging neural encoding schemes are proposed to shorten the spike train while maintaining the high accuracy. However, current accelerators for SNN cannot well support the emerging encoding schemes. In this work, we present a novel hardware architecture that can efficiently support SNN with emerging neural encoding. Our implementation features energy and area efficient processing units with increased parallelism and reduced memory accesses. We verified the accelerator on FPGA and achieve 25% and 90% improvement over previous work in power consumption and latency, respectively. At the same time, high area efficiency allows us to scale for large neural network models. To the best of our knowledge, this is the first work to deploy the large neural network model VGG on physical FPGA-based neuromorphic hardware. </details>
<details>	<summary>邮件日期</summary>	2022年06月07日</details>

# 466、低功率神经形态肌电手势分类
- [ ] Low Power Neuromorphic EMG Gesture Classification 
时间：2022年06月04日                         第一作者：Sai Sukruth Bezugam                       [链接](https://arxiv.org/abs/2206.02061).                     
## 摘要：基于肌电图信号的手势识别对于智能穿戴设备和生物医学神经修复控制等应用至关重要。由于其固有的脉冲/事件驱动的时空动力学，脉冲神经网络（SNN）在低功耗、实时肌电手势识别方面具有广阔的前景。在文献中，用于肌电手势分类的神经形态硬件实现（全芯片/板/系统规模）的演示有限。此外，大多数文献尝试利用基于LIF（漏积分和激发）神经元的原始SNN。在这项工作中，我们通过以下关键贡献来解决上述差距：（1）使用神经形态递归脉冲神经网络（RSNN）低功耗、高精度演示基于肌电信号的手势识别。特别是，我们提出了一种基于特殊双指数自适应阈值（DEXAT）神经元的多时间尺度递归神经形态系统。我们的网络实现了最先进的分类精度（90%），同时在Roshambo肌电图数据集上使用的神经元比最佳报告的现有技术少约53%。（2） 一种新的多通道脉冲编码器方案，用于在神经形态系统上有效处理实值肌电数据。（3） 本文展示了在英特尔专用神经形态Loihi芯片上实现复杂自适应神经元的独特多室方法。（4） 在Loihi（Nahuku 32）上实现的RSNN在批处理大小为50的GPU上实现了约983X/19X的显著能量/延迟优势。
<details>	<summary>英文摘要</summary>	EMG (Electromyograph) signal based gesture recognition can prove vital for applications such as smart wearables and bio-medical neuro-prosthetic control. Spiking Neural Networks (SNNs) are promising for low-power, real-time EMG gesture recognition, owing to their inherent spike/event driven spatio-temporal dynamics. In literature, there are limited demonstrations of neuromorphic hardware implementation (at full chip/board/system scale) for EMG gesture classification. Moreover, most literature attempts exploit primitive SNNs based on LIF (Leaky Integrate and Fire) neurons. In this work, we address the aforementioned gaps with following key contributions: (1) Low-power, high accuracy demonstration of EMG-signal based gesture recognition using neuromorphic Recurrent Spiking Neural Networks (RSNN). In particular, we propose a multi-time scale recurrent neuromorphic system based on special double-exponential adaptive threshold (DEXAT) neurons. Our network achieves state-of-the-art classification accuracy (90%) while using ~53% lesser neurons than best reported prior art on Roshambo EMG dataset. (2) A new multi-channel spike encoder scheme for efficient processing of real-valued EMG data on neuromorphic systems. (3) Unique multi-compartment methodology to implement complex adaptive neurons on Intel's dedicated neuromorphic Loihi chip is shown. (4) RSNN implementation on Loihi (Nahuku 32) achieves significant energy/latency benefits of ~983X/19X compared to GPU for batch size as 50. </details>
<details>	<summary>注释</summary>	3 Pages, 5 figures, 1 table </details>
<details>	<summary>邮件日期</summary>	2022年06月08日</details>

# 465、脉冲选通流：一种用于在线手势识别的基于层次结构的脉冲神经网络
- [ ] The Spike Gating Flow: A Hierarchical Structure Based Spiking Neural Network for Online Gesture Recognition 
时间：2022年06月04日                         第一作者：Zihao Zhao                       [链接](https://arxiv.org/abs/2206.01910).                     
## 摘要：动作识别是人工智能的一个令人兴奋的研究途径，因为它可能在机器人视觉和汽车等新兴工业领域改变游戏规则。然而，由于计算成本巨大和学习效率低下，当前的深度学习在此类应用中面临着重大挑战。因此，我们开发了一种新的基于脑激励的脉冲神经网络（SNN）系统，名为脉冲门控流（SGF），用于在线动作学习。所开发的系统由多个以分层方式组装的SGF单元组成。单个SGF单元包括三层：特征提取层、事件驱动层和基于直方图的训练层。为了演示开发的系统功能，我们采用标准的动态视觉传感器（DVS）手势分类作为基准。结果表明，我们可以达到87.5%的准确率，与深度学习（DL）相当，但在较小的训练/推理数据比1.5:1下。在学习过程中只需要一个训练时段。同时，据我们所知，这是基于非反向传播算法的SNN中精度最高的。最后，我们总结了所开发网络的少镜头学习范式：1）基于层次结构的网络设计涉及人类先验知识；2） SNN用于基于内容的全局动态特征检测。
<details>	<summary>英文摘要</summary>	Action recognition is an exciting research avenue for artificial intelligence since it may be a game changer in the emerging industrial fields such as robotic visions and automobiles. However, current deep learning faces major challenges for such applications because of the huge computational cost and the inefficient learning. Hence, we develop a novel brain-inspired Spiking Neural Network (SNN) based system titled Spiking Gating Flow (SGF) for online action learning. The developed system consists of multiple SGF units which assembled in a hierarchical manner. A single SGF unit involves three layers: a feature extraction layer, an event-driven layer and a histogram-based training layer. To demonstrate the developed system capabilities, we employ a standard Dynamic Vision Sensor (DVS) gesture classification as a benchmark. The results indicate that we can achieve 87.5% accuracy which is comparable with Deep Learning (DL), but at smaller training/inference data number ratio 1.5:1. And only a single training epoch is required during the learning process. Meanwhile, to the best of our knowledge, this is the highest accuracy among the non-backpropagation algorithm based SNNs. At last, we conclude the few-shot learning paradigm of the developed network: 1) a hierarchical structure-based network design involves human prior knowledge; 2) SNNs for content based global dynamic feature detection. </details>
<details>	<summary>邮件日期</summary>	2022年06月07日</details>

# 464、aSTDP：一种更具生物学合理性的学习
- [ ] aSTDP: A More Biologically Plausible Learning 
时间：2022年05月22日                         第一作者：Shiyuan Li                       [链接](https://arxiv.org/abs/2206.14137).                     
## 摘要：生物神经网络中的脉冲时间依赖可塑性在生物学习过程中被证明是重要的。另一方面，人工神经网络使用不同的学习方式，如反向传播或对比赫布学习。在这项工作中，我们介绍了近似STDP，一种新的神经网络学习框架，更类似于生物学习过程。它只使用STDP规则进行监督和非监督学习，每个神经元分布学习模式，不需要全局损失或其他监督信息。我们还使用数值方法来近似每个神经元的导数，以便更好地使用SDTP学习，并使用导数为神经元设定目标，以加速训练和测试过程。该框架可以在一个模型中进行预测或生成模式，而无需额外配置。最后，我们在MNIST数据集上验证了我们的分类和生成任务框架。
<details>	<summary>英文摘要</summary>	Spike-timing dependent plasticity in biological neural networks has been proven to be important during biological learning process. On the other hand, artificial neural networks use a different way to learn, such as Back-Propagation or Contrastive Hebbian Learning. In this work we introduce approximate STDP, a new neural networks learning framework more similar to the biological learning process. It uses only STDP rules for supervised and unsupervised learning, every neuron distributed learn patterns and don' t need a global loss or other supervised information. We also use a numerical way to approximate the derivatives of each neuron in order to better use SDTP learning and use the derivatives to set a target for neurons to accelerate training and testing process. The framework can make predictions or generate patterns in one model without additional configuration. Finally, we verified our framework on MNIST dataset for classification and generation tasks. </details>
<details>	<summary>注释</summary>	17 pages, 6 figures. arXiv admin note: text overlap with arXiv:1912.00009 </details>
<details>	<summary>邮件日期</summary>	2022年06月29日</details>

# 463、编程分子系统以模拟学习脉冲神经元
- [ ] Programming molecular systems to emulate a learning spiking neuron 
时间：2022年05月09日                         第一作者：Jakub Fil                       [链接](https://arxiv.org/abs/2206.02519).                     
## 摘要：赫布理论试图解释大脑中的神经元如何适应刺激，从而实现学习。Hebbian学习的一个有趣特征是，它是一种无监督的方法，因此不需要反馈，适用于系统必须自主学习的环境。本文探讨了如何设计分子系统来显示这种原始智能行为，并提出了第一个化学反应网络（CRN），它可以跨任意多个输入通道显示自主赫布学习。该系统模拟了一个脉冲神经元，我们证明了它可以学习输入的统计偏差。基本CRN是一组最小的、热力学上合理的微观可逆化学方程，可以根据其能量需求进行分析。然而，为了探索这种化学系统如何从头构建，我们还提出了一种基于酶驱动的分区反应的扩展版本。最后，我们还展示了基于DNA链置换范式的纯DNA系统如何实现神经元动力学。我们的分析为探索生物环境中的自主学习提供了一个引人注目的蓝图，使我们更接近于实现真正的合成生物智能。
<details>	<summary>英文摘要</summary>	Hebbian theory seeks to explain how the neurons in the brain adapt to stimuli, to enable learning. An interesting feature of Hebbian learning is that it is an unsupervised method and as such, does not require feedback, making it suitable in contexts where systems have to learn autonomously. This paper explores how molecular systems can be designed to show such proto-intelligent behaviours, and proposes the first chemical reaction network (CRN) that can exhibit autonomous Hebbian learning across arbitrarily many input channels. The system emulates a spiking neuron, and we demonstrate that it can learn statistical biases of incoming inputs. The basic CRN is a minimal, thermodynamically plausible set of micro-reversible chemical equations that can be analysed with respect to their energy requirements. However, to explore how such chemical systems might be engineered de novo, we also propose an extended version based on enzyme-driven compartmentalised reactions. Finally, we also show how a purely DNA system, built upon the paradigm of DNA strand displacement, can realise neuronal dynamics. Our analysis provides a compelling blueprint for exploring autonomous learning in biological settings, bringing us closer to realising real synthetic biological intelligence. </details>
<details>	<summary>注释</summary>	Submitted to ACS Synthetic Biology. arXiv admin note: substantial text overlap with arXiv:2009.13207 </details>
<details>	<summary>邮件日期</summary>	2022年06月07日</details>

# 462、IM／DD光通信中的脉冲神经网络均衡
- [ ] Spiking Neural Network Equalization for IM/DD Optical Communication 
时间：2022年06月01日                         第一作者：Elias Arnold                       [链接](https://arxiv.org/abs/2205.04263).                     
<details>	<summary>邮件日期</summary>	2022年06月02日</details>

# 461、基于累加器神经元的词典学习
- [ ] Dictionary Learning with Accumulator Neurons 
时间：2022年05月30日                         第一作者：Gavin Parpart                       [链接](https://arxiv.org/abs/2205.15386).                     
## 摘要：局部竞争算法（LCA）使用非脉冲泄漏积分器神经元之间的局部竞争来推断稀疏表示，允许在大规模并行神经形态架构（如Intel的Loihi处理器）上实时执行。在这里，我们关注的是使用时空特征字典从流视频中推断稀疏表示的问题，该字典以无监督的方式优化稀疏重建。无脉冲LCA以前被用于实现由原始未标记视频中的卷积核组成的时空词典的无监督学习。我们演示了如何使用累加器神经元有效地实现带脉冲LCA（\hbox{S-LCA}）的无监督字典学习，累加器神经元将传统的漏积分和激发（\hbox{LIF}）脉冲发生器与用于最小化积分输入和脉冲输出之间差异的附加状态变量相结合。我们展示了从分级到间歇脉冲的一系列动态模式的字典学习，用于推断从CIFAR数据库提取的静态图像以及从DVS相机捕获的视频帧的稀疏表示。在一项分类任务中，需要从DVS摄像头快速翻阅的一副卡片中识别套件，我们发现，由于用于推断稀疏时空表示的LCA模型从渐变到脉冲，性能基本上没有下降。我们的结论是，累加器神经元可能为未来的神经形态硬件提供一个强大的使能组件，用于实现时空词典的在线无监督学习，该词典针对基于事件的DVS摄像机的流视频稀疏重建进行了优化。
<details>	<summary>英文摘要</summary>	The Locally Competitive Algorithm (LCA) uses local competition between non-spiking leaky integrator neurons to infer sparse representations, allowing for potentially real-time execution on massively parallel neuromorphic architectures such as Intel's Loihi processor. Here, we focus on the problem of inferring sparse representations from streaming video using dictionaries of spatiotemporal features optimized in an unsupervised manner for sparse reconstruction. Non-spiking LCA has previously been used to achieve unsupervised learning of spatiotemporal dictionaries composed of convolutional kernels from raw, unlabeled video. We demonstrate how unsupervised dictionary learning with spiking LCA (\hbox{S-LCA}) can be efficiently implemented using accumulator neurons, which combine a conventional leaky-integrate-and-fire (\hbox{LIF}) spike generator with an additional state variable that is used to minimize the difference between the integrated input and the spiking output. We demonstrate dictionary learning across a wide range of dynamical regimes, from graded to intermittent spiking, for inferring sparse representations of both static images drawn from the CIFAR database as well as video frames captured from a DVS camera. On a classification task that requires identification of the suite from a deck of cards being rapidly flipped through as viewed by a DVS camera, we find essentially no degradation in performance as the LCA model used to infer sparse spatiotemporal representations migrates from graded to spiking. We conclude that accumulator neurons are likely to provide a powerful enabling component of future neuromorphic hardware for implementing online unsupervised learning of spatiotemporal dictionaries optimized for sparse reconstruction of streaming video from event based DVS cameras. </details>
<details>	<summary>邮件日期</summary>	2022年06月01日</details>

# 460、盲文字母阅读：神经形态硬件时空模式识别的基准
- [ ] Braille Letter Reading: A Benchmark for Spatio-Temporal Pattern Recognition on Neuromorphic Hardware 
时间：2022年05月30日                         第一作者：Simon F Muller-Cleve                       [链接](https://arxiv.org/abs/2205.15864).                     
## 摘要：时空模式识别是大脑的一种基本能力，在许多实际应用中都需要这种能力。最近的深度学习方法在这类任务中已经达到了非常高的精度，但它们在传统嵌入式解决方案上的实现在计算和能源方面仍然非常昂贵。机器人应用中的触觉传感是一个需要实时处理和能效的典型例子。根据大脑启发的计算方法，我们提出了一种通过盲文字母阅读在边缘进行时空触觉模式识别的新基准。我们基于iCub机器人的电容触觉传感器/指尖记录了一个新的盲文字母数据集，然后研究了时间信息的重要性以及基于事件编码对基于脉冲/基于事件计算的影响。之后，我们使用带代理梯度的时间反向传播离线训练和比较前馈和递归脉冲神经网络（SNN），然后将其部署在Intel Loihi神经形态芯片上，以实现快速高效的推理。在分类精度、功耗/能耗和计算延迟方面，我们面对的是标准分类器，尤其是部署在嵌入式Nvidia Jetson GPU上的长-短期内存（LSTM）。我们的结果表明，LSTM在准确性方面优于递归SNN 14%。然而，Loihi上的循环SNN比Jetson上的LSTM能效高237倍，平均功率仅为31mW。这项工作为触觉感知提出了一个新的基准，并强调了基于事件编码、神经形态硬件和基于脉冲的边缘时空模式识别计算的挑战和机遇。
<details>	<summary>英文摘要</summary>	Spatio-temporal pattern recognition is a fundamental ability of the brain which is required for numerous real-world applications. Recent deep learning approaches have reached outstanding accuracy in such tasks, but their implementation on conventional embedded solutions is still very computationally and energy expensive. Tactile sensing in robotic applications is a representative example where real-time processing and energy-efficiency are required. Following a brain-inspired computing approach, we propose a new benchmark for spatio-temporal tactile pattern recognition at the edge through braille letters reading. We recorded a new braille letters dataset based on the capacitive tactile sensors/fingertip of the iCub robot, then we investigated the importance of temporal information and the impact of event-based encoding for spike-based/event-based computation. Afterwards, we trained and compared feed-forward and recurrent spiking neural networks (SNNs) offline using back-propagation through time with surrogate gradients, then we deployed them on the Intel Loihi neuromorphic chip for fast and efficient inference. We confronted our approach to standard classifiers, in particular to a Long Short-Term Memory (LSTM) deployed on the embedded Nvidia Jetson GPU in terms of classification accuracy, power/energy consumption and computational delay. Our results show that the LSTM outperforms the recurrent SNN in terms of accuracy by 14%. However, the recurrent SNN on Loihi is 237 times more energy-efficient than the LSTM on Jetson, requiring an average power of only 31mW. This work proposes a new benchmark for tactile sensing and highlights the challenges and opportunities of event-based encoding, neuromorphic hardware and spike-based computing for spatio-temporal pattern recognition at the edge. </details>
<details>	<summary>注释</summary>	20 pages, submitted to Frontiers in Neuroscience - Neuromorphic Engineering </details>
<details>	<summary>邮件日期</summary>	2022年06月01日</details>

# 459、加速脉冲神经网络训练
- [ ] Accelerating spiking neural network training 
时间：2022年05月30日                         第一作者：Luke Taylor                       [链接](https://arxiv.org/abs/2205.15286).                     
## 摘要：脉冲神经网络（SNN）是一种人工网络，其灵感来源于大脑中动作电位的使用。在神经形态计算机上模拟这些网络的兴趣越来越大，因为它们的能耗和速度都有所提高，这是其对应的人工神经网络（ANN）的主要缩放问题。在直接培训SNN以在准确性方面与ANN媲美方面取得了重大进展。然而，这些方法由于其顺序性而速度较慢，导致训练时间较长。我们提出了一种新的直接训练单棘波神经元SNN的技术，它消除了所有的顺序计算，并且完全依赖于矢量化操作。我们在中低时空复杂度的真实数据集（时尚MNIST和神经性MNIST）上演示了在训练中超过10美元的加速，并具有强大的分类性能。我们提出的解决方案能够解决某些任务，与传统训练的SNN相比，脉冲计数减少了95.68\%，这可以显著降低部署在神经形态计算机上的能量需求。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNN) are a type of artificial network inspired by the use of action potentials in the brain. There is a growing interest in emulating these networks on neuromorphic computers due to their improved energy consumption and speed, which are the main scaling issues of their counterpart the artificial neural network (ANN). Significant progress has been made in directly training SNNs to perform on par with ANNs in terms of accuracy. These methods are however slow due to their sequential nature, leading to long training times. We propose a new technique for directly training single-spike-per-neuron SNNs which eliminates all sequential computation and relies exclusively on vectorised operations. We demonstrate over a $\times 10$ speedup in training with robust classification performance on real datasets of low to medium spatio-temporal complexity (Fashion-MNIST and Neuromophic-MNIST). Our proposed solution manages to solve certain tasks with over a $95.68 \%$ reduction in spike counts relative to a conventionally trained SNN, which could significantly reduce energy requirements when deployed on neuromorphic computers. </details>
<details>	<summary>注释</summary>	18 pages, 5 figures, under review at NeurIPS 2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月31日</details>

# 458、基于TNN的神经形态感觉处理单元的设计框架
- [ ] Towards a Design Framework for TNN-Based Neuromorphic Sensory Processing Units 
时间：2022年05月27日                         第一作者：Prabhu Vellaisamy                        [链接](https://arxiv.org/abs/2205.14248).                     
## 摘要：时间神经网络（TNN）是一种具有高能量效率的神经网络，具有类似大脑的感觉处理能力。这项工作介绍了正在进行的研究，旨在开发一个定制设计框架，用于设计高效的基于特定应用TNN的神经形态感觉处理单元（NSPU）。本文研究了先前针对UCR时间序列聚类和MNIST图像分类应用程序的NSPU设计工作。描述了定制设计框架和工具的当前想法，这些框架和工具能够实现高效的软硬件设计流，以快速探索特定于应用程序的NSPU的设计空间，同时利用EDA工具获取布局后网络表和电源性能区域（PPA）指标。展望了未来的研究方向。
<details>	<summary>英文摘要</summary>	Temporal Neural Networks (TNNs) are spiking neural networks that exhibit brain-like sensory processing with high energy efficiency. This work presents the ongoing research towards developing a custom design framework for designing efficient application-specific TNN-based Neuromorphic Sensory Processing Units (NSPUs). This paper examines previous works on NSPU designs for UCR time-series clustering and MNIST image classification applications. Current ideas for a custom design framework and tools that enable efficient software-to-hardware design flow for rapid design space exploration of application-specific NSPUs while leveraging EDA tools to obtain post-layout netlist and power-performance-area (PPA) metrics are described. Future research directions are also outlined. </details>
<details>	<summary>邮件日期</summary>	2022年05月31日</details>

# 457、基于推土机距离的暹罗脉冲神经网络监督训练
- [ ] Supervised Training of Siamese Spiking Neural Networks with Earth Mover's Distance 
时间：2022年05月27日                         第一作者：Mateusz Pabian                       [链接](https://arxiv.org/abs/2203.13207).                     
<details>	<summary>注释</summary>	Revised paper accepted for presentation at 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) DOI: 10.1109/ICASSP43922.2022.9746630 </details>
<details>	<summary>邮件日期</summary>	2022年05月30日</details>

# 456、基于全力训练的反馈驱动递归脉冲神经网络学习
- [ ] Learning in Feedback-driven Recurrent Spiking Neural Networks using full-FORCE Training 
时间：2022年05月26日                         第一作者：Ankita Paul                       [链接](https://arxiv.org/abs/2205.13585).                     
## 摘要：反馈驱动的递归脉冲神经网络（RSNN）是一种能够模拟动态系统的强大计算模型。然而，从读出到重现层的反馈回路的存在降低了学习机制的稳定性并防止其收敛。在这里，我们提出了一种RSNNs的监督训练过程，其中第二个网络仅在训练期间引入，以提供目标动态的提示。拟议的培训程序包括为重现层和读出层生成目标（即，对于完整的RSNN系统）。它使用基于递归最小二乘法的一阶和减少控制误差（FORCE）算法，使每一层的活动与其目标相适应。建议的全员训练程序减少了使输出和目标之间的误差接近于零所需的修改量。这些修改控制反馈回路，从而使训练收敛。我们使用带有泄漏积分和激发（LIF）神经元和速率编码的RSNN对8个动态系统进行建模，证明了所提出的全力以赴训练方法的改进性能和噪声鲁棒性。为了实现高效节能的硬件实现，在全员训练过程中实现了一种替代的首次脉冲时间（TTFS）编码。与速率编码相比，TTFS编码的全力产生更少的脉冲，并有助于更快地收敛到目标动态。
<details>	<summary>英文摘要</summary>	Feedback-driven recurrent spiking neural networks (RSNNs) are powerful computational models that can mimic dynamical systems. However, the presence of a feedback loop from the readout to the recurrent layer de-stabilizes the learning mechanism and prevents it from converging. Here, we propose a supervised training procedure for RSNNs, where a second network is introduced only during the training, to provide hint for the target dynamics. The proposed training procedure consists of generating targets for both recurrent and readout layers (i.e., for a full RSNN system). It uses the recursive least square-based First-Order and Reduced Control Error (FORCE) algorithm to fit the activity of each layer to its target. The proposed full-FORCE training procedure reduces the amount of modifications needed to keep the error between the output and target close to zero. These modifications control the feedback loop, which causes the training to converge. We demonstrate the improved performance and noise robustness of the proposed full-FORCE training procedure to model 8 dynamical systems using RSNNs with leaky integrate and fire (LIF) neurons and rate coding. For energy-efficient hardware implementation, an alternative time-to-first-spike (TTFS) coding is implemented for the full- FORCE training procedure. Compared to rate coding, full-FORCE with TTFS coding generates fewer spikes and facilitates faster convergence to the target dynamics. </details>
<details>	<summary>注释</summary>	Accepted at IJCNN 2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月30日</details>

# 455、无监督STDP训练的二维与三维卷积脉冲神经网络用于人体动作识别
- [ ] 2D versus 3D Convolutional Spiking Neural Networks Trained with Unsupervised STDP for Human Action Recognition 
时间：2022年05月26日                         第一作者：Mireille El-Assal                       [链接](https://arxiv.org/abs/2205.13474).                     
## 摘要：当前技术的进步突出了视频分析在计算机视觉领域的重要性。然而，视频分析与传统的人工神经网络（ANN）相比，具有相当高的计算成本。脉冲神经网络（SNN）是第三代生物学上合理的模型，以脉冲的形式处理信息。使用脉冲时间依赖可塑性（STDP）规则的SNN无监督学习有可能克服常规人工神经网络的一些瓶颈，但基于STDP的SNN仍不成熟，其性能远远落后于ANN。在这项工作中，我们研究了SNN在接受人类行为识别任务挑战时的性能，因为这项任务在计算机视觉中有许多实时应用，例如视频监控。本文介绍了一种用无监督STDP训练的多层三维卷积SNN模型。当使用KTH和魏茨曼数据集进行挑战时，我们将该模型的性能与基于2D STDP的SNN的性能进行了比较。我们还比较了这些模型的单层和多层版本，以便对其性能进行准确评估。我们表明，基于STDP的卷积snn可以使用3D核学习运动模式，从而实现基于运动的视频识别。最后，我们证明了基于STDP的snn的3D卷积优于2D卷积，尤其是在处理长视频序列时。
<details>	<summary>英文摘要</summary>	Current advances in technology have highlighted the importance of video analysis in the domain of computer vision. However, video analysis has considerably high computational costs with traditional artificial neural networks (ANNs). Spiking neural networks (SNNs) are third generation biologically plausible models that process the information in the form of spikes. Unsupervised learning with SNNs using the spike timing dependent plasticity (STDP) rule has the potential to overcome some bottlenecks of regular artificial neural networks, but STDP-based SNNs are still immature and their performance is far behind that of ANNs. In this work, we study the performance of SNNs when challenged with the task of human action recognition, because this task has many real-time applications in computer vision, such as video surveillance. In this paper we introduce a multi-layered 3D convolutional SNN model trained with unsupervised STDP. We compare the performance of this model to those of a 2D STDP-based SNN when challenged with the KTH and Weizmann datasets. We also compare single-layer and multi-layer versions of these models in order to get an accurate assessment of their performance. We show that STDP-based convolutional SNNs can learn motion patterns using 3D kernels, thus enabling motion-based recognition from videos. Finally, we give evidence that 3D convolution is superior to 2D convolution with STDP-based SNNs, especially when dealing with long video sequences. </details>
<details>	<summary>注释</summary>	arXiv admin note: text overlap with arXiv:2105.14740 by other authors </details>
<details>	<summary>邮件日期</summary>	2022年05月27日</details>

# 454、一点能量可以走很长的路：从卷积神经网络构建一个高效、准确的脉冲神经网络
- [ ] A Little Energy Goes a Long Way: Build an Energy-Efficient, Accurate Spiking Neural Network from Convolutional Neural Network 
时间：2022年05月26日                         第一作者：Dengyu Wu                       [链接](https://arxiv.org/abs/2103.00944).                     
<details>	<summary>邮件日期</summary>	2022年05月27日</details>

# 453、Spiker：一种用于脉冲神经网络的FPGA优化硬件加速
- [ ] Spiker: an FPGA-optimized Hardware acceleration for Spiking Neural Networks 
时间：2022年05月26日                         第一作者：Alessio Carpegna                       [链接](https://arxiv.org/abs/2201.06993).                     
<details>	<summary>注释</summary>	6 pages, 3 figures, 4 tables </details>
<details>	<summary>邮件日期</summary>	2022年05月27日</details>

# 452、神经符号大脑
- [ ] The Neuro-Symbolic Brain 
时间：2022年05月13日                         第一作者：Robert Liz\'ee                       [链接](https://arxiv.org/abs/2205.13440).                     
## 摘要：神经网络促进了符号没有明确位置的分布式表示。尽管如此，我们提出，符号的制造只是通过在反馈脉冲神经网络中将稀疏随机噪声训练为自持吸引子。这样，我们就可以生成许多我们称之为素吸引子的东西，而支持它们的网络就像持有符号值的寄存器，我们称之为寄存器。与符号一样，素吸引子是原子的，没有任何内部结构。此外，由脉冲神经元自然实现的赢家通吃机制使寄存器能够恢复噪声信号中的主吸引子。利用这种能力，当考虑两个相连的寄存器（输入寄存器和输出寄存器）时，可以使用Hebbian规则将输出上的吸引子绑定到输入上的吸引子。因此，每当吸引子在输入端处于活动状态时，它就会在输出端诱导其约束吸引子；即使绑定越多，信号越模糊，赢家通吃过滤功能也可以恢复绑定的素吸引子。然而，容量仍然有限。也可以一次性解除绑定，恢复该绑定所占用的容量。这种机制作为工作记忆的基础，将素数吸引子转化为变量。此外，我们还使用一个随机二阶网络合并两个寄存器所持有的素吸引子，以一次性将第三个寄存器所持有的素吸引子绑定到它们，实际上实现了一个哈希表。此外，我们还介绍了由寄存器组成的寄存器开关盒，用于将一个寄存器的内容移动到另一个寄存器。在此基础上，我们利用脉冲神经元构建了一个玩具符号计算机。所使用的技术提出了以结构先验为代价设计外推、可重用、样本有效的深度学习网络的方法。
<details>	<summary>英文摘要</summary>	Neural networks promote a distributed representation with no clear place for symbols. Despite this, we propose that symbols are manufactured simply by training a sparse random noise as a self-sustaining attractor in a feedback spiking neural network. This way, we can generate many of what we shall call prime attractors, and the networks that support them are like registers holding a symbolic value, and we call them registers. Like symbols, prime attractors are atomic and devoid of any internal structure. Moreover, the winner-take-all mechanism naturally implemented by spiking neurons enables registers to recover a prime attractor within a noisy signal. Using this faculty, when considering two connected registers, an input one and an output one, it is possible to bind in one shot using a Hebbian rule the attractor active on the output to the attractor active on the input. Thus, whenever an attractor is active on the input, it induces its bound attractor on the output; even though the signal gets blurrier with more bindings, the winner-take-all filtering faculty can recover the bound prime attractor. However, the capacity is still limited. It is also possible to unbind in one shot, restoring the capacity taken by that binding. This mechanism serves as a basis for working memory, turning prime attractors into variables. Also, we use a random second-order network to amalgamate the prime attractors held by two registers to bind the prime attractor held by a third register to them in one shot, de facto implementing a hash table. Furthermore, we introduce the register switch box composed of registers to move the content of one register to another. Then, we use spiking neurons to build a toy symbolic computer based on the above. The technics used suggest ways to design extrapolating, reusable, sample-efficient deep learning networks at the cost of structural priors. </details>
<details>	<summary>注释</summary>	32 pages, 11 figures ACM-class: I.2.0; I.2.6 </details>
<details>	<summary>邮件日期</summary>	2022年05月27日</details>

# 451、lpSpikeCon：支持低精度脉冲神经网络处理，实现对自治代理的高效无监督连续学习
- [ ] lpSpikeCon: Enabling Low-Precision Spiking Neural Network Processing for Efficient Unsupervised Continual Learning on Autonomous Agents 
时间：2022年05月24日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2205.12295).                     
## 摘要：最近的研究表明，基于SNN的系统可以有效地执行无监督的连续学习，这是由于其生物似有理的学习规则，例如脉冲时间依赖性可塑性（STDP）。这种学习能力对于需要不断适应动态变化的场景/环境的自治代理（如机器人和无人机）等用例尤其有益，其中直接从环境中收集的新数据可能具有应在线学习的新特性。当前最先进的工作在训练和推理阶段都采用了高精度权重（即32位），这造成了较高的内存和能源成本，从而阻碍了电池驱动移动自治系统中此类系统的高效嵌入式实现。另一方面，由于信息丢失，精度降低可能会危及无监督连续学习的质量。为此，我们提出了lpSpikeCon，这是一种新的方法，可以实现低精度SNN处理，从而在资源受限的自治代理/系统上实现高效的无监督连续学习。我们的lpSpikeCon方法采用了以下关键步骤：（1）分析在无监督的连续学习环境下，在降低权重精度的情况下训练SNN模型对推理精度的影响；（2） 利用本研究确定对推理准确性有重大影响的SNN参数；（3）开发一种算法，用于搜索相应的SNN参数值，以提高无监督连续学习的质量。实验结果表明，我们的lpSpikeCon可以将SNN模型的权重记忆减少8倍（即，通过明智地使用4位权重），以便在无监督连续学习的情况下进行在线训练，并且与在不同网络大小上使用32位权重的基线模型相比，在推理阶段不会出现精度损失。
<details>	<summary>英文摘要</summary>	Recent advances have shown that SNN-based systems can efficiently perform unsupervised continual learning due to their bio-plausible learning rule, e.g., Spike-Timing-Dependent Plasticity (STDP). Such learning capabilities are especially beneficial for use cases like autonomous agents (e.g., robots and UAVs) that need to continuously adapt to dynamically changing scenarios/environments, where new data gathered directly from the environment may have novel features that should be learned online. Current state-of-the-art works employ high-precision weights (i.e., 32 bit) for both training and inference phases, which pose high memory and energy costs thereby hindering efficient embedded implementations of such systems for battery-driven mobile autonomous systems. On the other hand, precision reduction may jeopardize the quality of unsupervised continual learning due to information loss. Towards this, we propose lpSpikeCon, a novel methodology to enable low-precision SNN processing for efficient unsupervised continual learning on resource-constrained autonomous agents/systems. Our lpSpikeCon methodology employs the following key steps: (1) analyzing the impacts of training the SNN model under unsupervised continual learning settings with reduced weight precision on the inference accuracy; (2) leveraging this study to identify SNN parameters that have a significant impact on the inference accuracy; and (3) developing an algorithm for searching the respective SNN parameter values that improve the quality of unsupervised continual learning. The experimental results show that our lpSpikeCon can reduce weight memory of the SNN model by 8x (i.e., by judiciously employing 4-bit weights) for performing online training with unsupervised continual learning and achieve no accuracy loss in the inference phase, as compared to the baseline model with 32-bit weights across different network sizes. </details>
<details>	<summary>注释</summary>	To appear at the 2022 International Joint Conference on Neural Networks (IJCNN), the 2022 IEEE World Congress on Computational Intelligence (WCCI), July 2022, Padova, Italy </details>
<details>	<summary>邮件日期</summary>	2022年05月26日</details>

# 450、DPSNN：一种差分私有脉冲神经网络
- [ ] DPSNN: A Differentially Private Spiking Neural Network 
时间：2022年05月24日                         第一作者：Jihang Wang                       [链接](https://arxiv.org/abs/2205.12718).                     
## 摘要：隐私保护是机器学习算法的一个关键问题。脉冲神经网络（SNN）在图像分类、目标检测、语音识别等领域发挥着重要作用，但对SNN隐私保护的研究却十分迫切。本研究将差分隐私（DP）算法与SNN相结合，提出了差分私有脉冲神经网络（DPSNN）。DP向梯度中注入噪声，SNN以离散的脉冲序列传输信息，因此我们的差异私有SNN可以在确保高精度的同时保持强大的隐私保护。我们在MNIST、Fashion MNIST和人脸识别数据集Extended YaleB上进行了实验。当隐私保护得到改善时，人工神经网络（ANN）的准确性显著下降，但我们的算法在性能上几乎没有变化。同时，分析了影响SNN隐私保护的不同因素。首先，代理梯度越不精确，SNN的隐私保护越好。其次，整合与激发（IF）神经元的表现优于泄漏整合与激发（LIF）神经元。第三，较大的时间窗口更有助于隐私保护和性能。
<details>	<summary>英文摘要</summary>	Privacy-preserving is a key problem for the machine learning algorithm. Spiking neural network (SNN) plays an important role in many domains, such as image classification, object detection, and speech recognition, but the study on the privacy protection of SNN is urgently needed. This study combines the differential privacy (DP) algorithm and SNN and proposes differentially private spiking neural network (DPSNN). DP injects noise into the gradient, and SNN transmits information in discrete spike trains so that our differentially private SNN can maintain strong privacy protection while still ensuring high accuracy. We conducted experiments on MNIST, Fashion-MNIST, and the face recognition dataset Extended YaleB. When the privacy protection is improved, the accuracy of the artificial neural network(ANN) drops significantly, but our algorithm shows little change in performance. Meanwhile, we analyzed different factors that affect the privacy protection of SNN. Firstly, the less precise the surrogate gradient is, the better the privacy protection of the SNN. Secondly, the Integrate-And-Fire (IF) neurons perform better than leaky Integrate-And-Fire (LIF) neurons. Thirdly, a large time window contributes more to privacy protection and performance. </details>
<details>	<summary>注释</summary>	12 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2022年05月26日</details>

# 449、一种自适应的脉冲分类对比学习模型
- [ ] An Adaptive Contrastive Learning Model for Spike Sorting 
时间：2022年05月24日                         第一作者：Lang Qian                       [链接](https://arxiv.org/abs/2205.11914).                     
## 摘要：脑-机接口（BCI）是电子设备与大脑直接通信的方式。对于大多数医学类型的脑-机接口任务，多个神经元单元的活动或局部场电位足以进行解码。但对于用于神经科学研究的BCI来说，分离出单个神经元的活动是很重要的。随着大规模硅技术的发展和探针通道数量的增加，人工解释和标记脉冲变得越来越不切实际。本文提出了一种新的建模框架：自适应对比学习模型，该模型以最大互信息损失函数为理论基础，通过对比学习从脉冲中学习表征。基于具有相似特征的数据共享相同标签这一事实，无论是多重分类还是二进制分类。在这一理论支持下，我们将多分类问题简化为多个二进制分类，提高了准确性和运行效率。此外，我们还对脉冲进行了一系列增强，同时解决了脉冲重叠影响分类效果的问题。
<details>	<summary>英文摘要</summary>	Brain-computer interfaces (BCIs), is ways for electronic devices to communicate directly with the brain. For most medical-type brain-computer interface tasks, the activity of multiple units of neurons or local field potentials is sufficient for decoding. But for BCIs used in neuroscience research, it is important to separate out the activity of individual neurons. With the development of large-scale silicon technology and the increasing number of probe channels, artificially interpreting and labeling spikes is becoming increasingly impractical. In this paper, we propose a novel modeling framework: Adaptive Contrastive Learning Model that learns representations from spikes through contrastive learning based on the maximizing mutual information loss function as a theoretical basis. Based on the fact that data with similar features share the same labels whether they are multi-classified or binary-classified. With this theoretical support, we simplify the multi-classification problem into multiple binary-classification, improving both the accuracy and the runtime efficiency. Moreover, we also introduce a series of enhancements for the spikes, while solving the problem that the classification effect is affected because of the overlapping spikes. </details>
<details>	<summary>邮件日期</summary>	2022年05月25日</details>

# 448、基于Hebbian可塑性的脉冲神经网络的记忆丰富计算与学习
- [ ] Memory-enriched computation and learning in spiking neural networks through Hebbian plasticity 
时间：2022年05月23日                         第一作者：Thomas Limbacher                       [链接](https://arxiv.org/abs/2205.11276).                     
## 摘要：记忆是生物神经系统的一个关键组成部分，它能够在从数百毫秒到数年的巨大时间尺度上保留信息。虽然Hebbian可塑性被认为在生物记忆中起着关键作用，但迄今为止，它主要是在模式完成和无监督学习的背景下进行分析的。在这里，我们提出Hebbian可塑性是生物神经系统计算的基础。我们介绍了一种新的脉冲神经网络结构，该结构丰富了Hebbian突触可塑性。我们表明，Hebbian丰富使脉冲神经网络在计算和学习能力方面具有惊人的通用性。它提高了他们的非分布泛化、一次性学习、跨模态生成关联、语言处理和基于奖励的学习能力。由于脉冲神经网络是高效节能的神经形态硬件的基础，这也表明可以基于此原理构建强大的认知神经形态系统。
<details>	<summary>英文摘要</summary>	Memory is a key component of biological neural systems that enables the retention of information over a huge range of temporal scales, ranging from hundreds of milliseconds up to years. While Hebbian plasticity is believed to play a pivotal role in biological memory, it has so far been analyzed mostly in the context of pattern completion and unsupervised learning. Here, we propose that Hebbian plasticity is fundamental for computations in biological neural systems. We introduce a novel spiking neural network architecture that is enriched by Hebbian synaptic plasticity. We show that Hebbian enrichment renders spiking neural networks surprisingly versatile in terms of their computational as well as learning capabilities. It improves their abilities for out-of-distribution generalization, one-shot learning, cross-modal generative association, language processing, and reward-based learning. As spiking neural networks are the basis for energy-efficient neuromorphic hardware, this also suggests that powerful cognitive neuromorphic systems can be build based on this principle. </details>
<details>	<summary>邮件日期</summary>	2022年05月24日</details>

# 447、PrivateSNN：保护隐私的脉冲神经网络
- [ ] PrivateSNN: Privacy-Preserving Spiking Neural Networks 
时间：2022年05月21日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2104.03414).                     
<details>	<summary>注释</summary>	Accepted to AAAI2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月24日</details>

# 446、脉冲神经网络的最新进展和新前沿
- [ ] Recent Advances and New Frontiers in Spiking Neural Networks 
时间：2022年05月21日                         第一作者：Duzhen Zhang                       [链接](https://arxiv.org/abs/2204.07050).                     
<details>	<summary>注释</summary>	Accepted at IJCAI2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月24日</details>

# 445、使用生物学上合理的脉冲潜伏期代码和赢家通吃抑制有效地表示视觉对象
- [ ] Efficient visual object representation using a biologically plausible spike-latency code and winner-take-all inhibition 
时间：2022年05月20日                         第一作者：Melani Sanchez-Garcia                        [链接](https://arxiv.org/abs/2205.10338).                     
## 摘要：深度神经网络在诸如物体识别等关键视觉挑战方面已经超过了人类的表现，但需要大量的能量、计算和记忆。相比之下，脉冲神经网络（SNN）有潜力提高目标识别系统的效率和生物合理性。在这里，我们提出了一个SNN模型，该模型使用脉冲潜伏期编码和赢家通吃抑制（WTA-I）有效地表示来自时尚MNIST数据集的视觉刺激。刺激用中央周围的感受野进行预处理，然后馈送给一层棘突神经元，其突触重量通过棘突时间依赖性可塑性（STDP）进行更新。我们研究了在不同的WTA-I方案下被表示对象的质量如何变化，并证明了由150个脉冲神经元组成的网络可以有效地表示只有40个脉冲的对象。研究如何在SNN中使用生物学上合理的学习规则来实现核心对象识别，不仅可以加深我们对大脑的理解，还可以开发出新颖高效的人工视觉系统。
<details>	<summary>英文摘要</summary>	Deep neural networks have surpassed human performance in key visual challenges such as object recognition, but require a large amount of energy, computation, and memory. In contrast, spiking neural networks (SNNs) have the potential to improve both the efficiency and biological plausibility of object recognition systems. Here we present a SNN model that uses spike-latency coding and winner-take-all inhibition (WTA-I) to efficiently represent visual stimuli from the Fashion MNIST dataset. Stimuli were preprocessed with center-surround receptive fields and then fed to a layer of spiking neurons whose synaptic weights were updated using spike-timing-dependent-plasticity (STDP). We investigate how the quality of the represented objects changes under different WTA-I schemes and demonstrate that a network of 150 spiking neurons can efficiently represent objects with as little as 40 spikes. Studying how core object recognition may be implemented using biologically plausible learning rules in SNNs may not only further our understanding of the brain, but also lead to novel and efficient artificial vision systems. </details>
<details>	<summary>邮件日期</summary>	2022年05月23日</details>

# 444、出埃及记：稳定有效的脉冲神经网络训练
- [ ] EXODUS: Stable and Efficient Training of Spiking Neural Networks 
时间：2022年05月20日                         第一作者：Felix Christian Bauer (1)                       [链接](https://arxiv.org/abs/2205.10242).                     
## 摘要：在能量效率至关重要的机器学习任务中，脉冲神经网络（SNN）正在获得巨大的吸引力。然而，使用最先进的时间反向传播（BPTT）来训练此类网络非常耗时。Shrestha和Orchard（2018）之前的工作采用了一种高效的GPU加速反向传播算法，称为SLAYER，该算法大大加快了训练速度。然而，SLAYER在计算梯度时没有考虑神经元重置机制，我们认为梯度是数值不稳定性的来源。为了解决这个问题，SLAYER引入了跨层的渐变比例超参数，这需要手动调整。在本文中，（i）我们修改了SLAYER并设计了一种称为EXODUS的算法，该算法考虑了神经元重置机制，并应用隐式函数定理（IFT）计算正确的梯度（相当于BPTT计算的梯度），（ii）我们消除了对梯度进行特殊缩放的需要，从而大大降低了训练复杂性，（iii）我们证明，通过计算机模拟，出埃及记在数值上是稳定的，并且取得了与SLAYER相当或更好的性能，尤其是在使用依赖于时间特征的SNN的各种任务中。我们的代码位于https://github.com/synsense/sinabs-exodus.
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) are gaining significant traction in machine learning tasks where energy-efficiency is of utmost importance. Training such networks using the state-of-the-art back-propagation through time (BPTT) is, however, very time-consuming. Previous work by Shrestha and Orchard [2018] employs an efficient GPU-accelerated back-propagation algorithm called SLAYER, which speeds up training considerably. SLAYER, however, does not take into account the neuron reset mechanism while computing the gradients, which we argue to be the source of numerical instability. To counteract this, SLAYER introduces a gradient scale hyperparameter across layers, which needs manual tuning. In this paper, (i) we modify SLAYER and design an algorithm called EXODUS, that accounts for the neuron reset mechanism and applies the Implicit Function Theorem (IFT) to calculate the correct gradients (equivalent to those computed by BPTT), (ii) we eliminate the need for ad-hoc scaling of gradients, thus, reducing the training complexity tremendously, (iii) we demonstrate, via computer simulations, that EXODUS is numerically stable and achieves a comparable or better performance than SLAYER especially in various tasks with SNNs that rely on temporal features. Our code is available at https://github.com/synsense/sinabs-exodus. </details>
<details>	<summary>邮件日期</summary>	2022年05月23日</details>

# 443、Spikemax：基于峰值的分类损失方法
- [ ] Spikemax: Spike-based Loss Methods for Classification 
时间：2022年05月19日                         第一作者：Sumit Bam Shrestha                        [链接](https://arxiv.org/abs/2205.09845).                     
## 摘要：脉冲神经网络是低功耗边缘计算的一种很有前景的研究范式。最近在SNN反向传播方面的工作使SNN能够针对实际任务进行培训。然而，由于峰值是时间上的二元事件，标准损失公式与峰值输出不直接兼容。因此，目前的工作仅限于使用峰值计数的均方损失。在本文中，我们从脉冲计数度量推导出输出概率解释，并引入基于脉冲的负对数似然度量，它更适合于分类任务，尤其是在能量效率和推理延迟方面。我们将我们的损失度量与其他现有备选方案进行比较，并在三个神经形态基准数据集上使用分类性能进行评估：NMNIST、DVS手势和N-TIDIGITS18。此外，我们在这些数据集上展示了最先进的性能，实现了更快的推理速度和更少的能耗。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks~(SNNs) are a promising research paradigm for low power edge-based computing. Recent works in SNN backpropagation has enabled training of SNNs for practical tasks. However, since spikes are binary events in time, standard loss formulations are not directly compatible with spike output. As a result, current works are limited to using mean-squared loss of spike count. In this paper, we formulate the output probability interpretation from the spike count measure and introduce spike-based negative log-likelihood measure which are more suited for classification tasks especially in terms of the energy efficiency and inference latency. We compare our loss measures with other existing alternatives and evaluate using classification performances on three neuromorphic benchmark datasets: NMNIST, DVS Gesture and N-TIDIGITS18. In addition, we demonstrate state of the art performances on these datasets, achieving faster inference speed and less energy consumption. </details>
<details>	<summary>注释</summary>	Accepted by IJCNN 2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月23日</details>

# 442、用于图像识别的时态神经形态编码器脉冲间隔的设计与数学建模
- [ ] Design and Mathematical Modelling of Inter Spike Interval of Temporal Neuromorphic Encoder for Image Recognition 
时间：2022年05月19日                         第一作者：Aadhitiya VS                       [链接](https://arxiv.org/abs/2205.09519).                     
## 摘要：神经形态计算系统使用混合模式模拟或数字VLSI电路模拟生物神经系统的电生理行为。这些系统在执行认知任务时显示出卓越的准确性和能效。神经形态计算系统中使用的神经网络结构是类似于生物神经系统的脉冲神经网络（SNN）。SNN作为时间的函数在脉冲列车上运行。神经形态编码器将感觉数据转换为脉冲序列。本文实现了一种用于图像处理的低功耗神经形态编码器。还建立了图像像素与峰间间隔之间的数学模型。其中，获得像素和峰间间隔之间的指数关系。最后，通过电路仿真对数学方程进行了验证。
<details>	<summary>英文摘要</summary>	Neuromorphic computing systems emulate the electrophysiological behavior of the biological nervous system using mixed-mode analog or digital VLSI circuits. These systems show superior accuracy and power efficiency in carrying out cognitive tasks. The neural network architecture used in neuromorphic computing systems is spiking neural networks (SNNs) analogous to the biological nervous system. SNN operates on spike trains as a function of time. A neuromorphic encoder converts sensory data into spike trains. In this paper, a low-power neuromorphic encoder for image processing is implemented. A mathematical model between pixels of an image and the inter-spike intervals is also formulated. Wherein an exponential relationship between pixels and inter-spike intervals is obtained. Finally, the mathematical equation is validated with circuit simulation. </details>
<details>	<summary>注释</summary>	4 pages, 6 figures, one table, IEEE ICEE 2020 conference proceeding </details>
<details>	<summary>邮件日期</summary>	2022年05月20日</details>

# 441、基于脉冲序列的关系表示学习
- [ ] Relational representation learning with spike trains 
时间：2022年05月18日                         第一作者：Dominik Dold                       [链接](https://arxiv.org/abs/2205.09140).                     
## 摘要：关系表示学习最近受到了越来越多的关注，因为它可以灵活地建模各种系统，如相互作用的粒子、材料和工业项目，例如航天器的设计。处理关系数据的一种突出方法是知识图嵌入算法，其中知识图的实体和关系映射到低维向量空间，同时保留其语义结构。最近，提出了一种将图元素映射到脉冲神经网络时域的图嵌入方法。然而，它依赖于通过神经元群对图形元素进行编码，这些神经元群只出现一次脉冲。在这里，我们提出了一个模型，允许我们学习基于脉冲序列的知识图嵌入，通过充分利用脉冲模式的时域，每个图元素只需要一个神经元。这种编码方案可以在任意脉冲神经元模型上实现，只要可以计算脉冲时间的梯度，这一点我们在integrate和fire神经元模型上进行了证明。总的来说，本文的结果显示了关系知识如何集成到基于spike的系统中，为将基于事件的计算和关系数据合并以构建功能强大且节能的人工智能应用程序和推理系统开辟了可能性。
<details>	<summary>英文摘要</summary>	Relational representation learning has lately received an increase in interest due to its flexibility in modeling a variety of systems like interacting particles, materials and industrial projects for, e.g., the design of spacecraft. A prominent method for dealing with relational data are knowledge graph embedding algorithms, where entities and relations of a knowledge graph are mapped to a low-dimensional vector space while preserving its semantic structure. Recently, a graph embedding method has been proposed that maps graph elements to the temporal domain of spiking neural networks. However, it relies on encoding graph elements through populations of neurons that only spike once. Here, we present a model that allows us to learn spike train-based embeddings of knowledge graphs, requiring only one neuron per graph element by fully utilizing the temporal domain of spike patterns. This coding scheme can be implemented with arbitrary spiking neuron models as long as gradients with respect to spike times can be calculated, which we demonstrate for the integrate-and-fire neuron model. In general, the presented results show how relational knowledge can be integrated into spike-based systems, opening up the possibility of merging event-based computing and relational data to build powerful and energy efficient artificial intelligence applications and reasoning systems. </details>
<details>	<summary>注释</summary>	Accepted for publication at the WCCI 2022 (IJCNN) </details>
<details>	<summary>邮件日期</summary>	2022年05月20日</details>

# 440、使用脉冲深度网的函数回归
- [ ] Function Regression using Spiking DeepONet 
时间：2022年05月17日                         第一作者：Adar Kahana                       [链接](https://arxiv.org/abs/2205.10130).                     
## 摘要：深度学习的主要广泛应用之一是函数回归。然而，尽管现代神经网络体系结构具有较高的准确性和鲁棒性，但它需要大量的计算资源进行训练。缓解甚至解决这种效率低下的一种方法是从大脑中获得进一步的灵感，并以一种更具生物合理性的方式重新构建学习过程，开发出近年来越来越受欢迎的脉冲神经网络（SNN）。在本文中，我们提出了一种基于SNN的方法来执行回归，这是一个挑战，因为在将函数的输入域和连续输出值表示为峰值时存在固有的困难。我们使用DeepONet（设计用于学习操作符的神经网络）来学习脉冲的行为。然后，我们使用这种方法进行函数回归。我们提出了几种在spiking框架中使用DeepONet的方法，并给出了不同基准的精度和训练时间。
<details>	<summary>英文摘要</summary>	One of the main broad applications of deep learning is function regression. However, despite their demonstrated accuracy and robustness, modern neural network architectures require heavy computational resources to train. One method to mitigate or even resolve this inefficiency has been to draw further inspiration from the brain and reformulate the learning process in a more biologically-plausible way, developing what are known as Spiking Neural Networks (SNNs), which have been gaining traction in recent years. In this paper we present an SNN-based method to perform regression, which has been a challenge due to the inherent difficulty in representing a function's input domain and continuous output values as spikes. We use a DeepONet - neural network designed to learn operators - to learn the behavior of spikes. Then, we use this approach to do function regression. We propose several methods to use a DeepONet in the spiking framework, and present accuracy and training time for different benchmarks. </details>
<details>	<summary>注释</summary>	15 pages, 5 figures and 4 tables </details>
<details>	<summary>邮件日期</summary>	2022年05月23日</details>

# 439、基于双相位优化的超低延迟无损ANN-SNN转换
- [ ] Towards Lossless ANN-SNN Conversion under Ultra-Low Latency with Dual-Phase Optimization 
时间：2022年05月16日                         第一作者：Ziming Wang                       [链接](https://arxiv.org/abs/2205.07473).                     
## 摘要：具有异步离散事件的脉冲神经网络（SNN）具有较高的能量效率。实现深层SNN的一种流行方法是将ANN中的有效训练和SNN中的有效推理相结合的ANN-SNN转换。然而，以前的工作大多需要数千个时间步来实现无损转换。在本文中，我们首先确定了根本原因，即对SNN中的负剩余膜电位或溢出剩余膜电位的错误表述。此外，我们系统地分析了SNN和ANN之间的转换误差，并将其分解为三个方面：量化误差、剪裁误差和剩余膜电位表示误差。基于这样的见解，我们提出了一种双相位转换算法来最小化这些误差。因此，我们的模型在精度和精度延迟方面与深层架构（ResNet和VGG net）实现了SOTA权衡。具体而言，与最新结果相比，我们报告的SOTA精度在16$\倍的加速比内。同时，无损转换的推理性能至少快2$\倍。
<details>	<summary>英文摘要</summary>	Spiking neural network (SNN) operating with asynchronous discrete events shows higher energy efficiency. A popular approach to implement deep SNNs is ANN-SNN conversion combining both efficient training in ANNs and efficient inference in SNNs. However, the previous works mostly required thousands of time steps to achieve lossless conversion. In this paper, we first identify the underlying cause, i.e., misrepresentation of the negative or overflow residual membrane potential in SNNs. Furthermore, we systematically analyze the conversion error between SNNs and ANNs, and then decompose it into three folds: quantization error, clipping error, and residual membrane potential representation error. With such insights, we propose a dual-phase conversion algorithm to minimize those errors. As a result, our model achieves SOTA in both accuracy and accuracy-delay tradeoff with deep architectures (ResNet and VGG net). Specifically, we report SOTA accuracy within 16$\times$ speedup compared with the latest results. Meanwhile, lossless conversion is performed with at least 2$\times$ faster reasoning performance. </details>
<details>	<summary>邮件日期</summary>	2022年05月17日</details>

# 438、皮质微电路的计算框架近似符号协调随机反向传播
- [ ] A Computational Framework of Cortical Microcircuits Approximates Sign-concordant Random Backpropagation 
时间：2022年05月15日                         第一作者：Yukun Yang                       [链接](https://arxiv.org/abs/2205.07292).                     
## 摘要：最近的几项研究试图解决众所周知的反向传播（BP）方法在生物学上的不真实性。虽然反馈对齐、直接反馈对齐及其变体（如符号协调反馈对齐）等有前途的方法解决了BP的重量传输问题，但由于一系列其他未解决的问题，其有效性仍然存在争议。在这项工作中，我们回答了一个问题，即是否有可能仅根据神经科学中观察到的机制实现随机反向传播。我们提出了一个由新的微电路体系结构及其支持的Hebbian学习规则组成的假设框架。该微电路结构由三种类型的细胞和两种类型的突触连接组成，通过局部反馈连接计算和传播错误信号，并支持训练具有全局定义的脉冲错误函数的多层脉冲神经网络。我们利用Hebbian规则在局部隔室中操作来更新突触权重，并以生物学上合理的方式实现监督学习。最后，我们从优化的角度解释了所提出的框架，并展示了它与符号一致反馈对齐的等价性。所提议的框架在包括MNIST和CIFAR10在内的多个数据集上进行了基准测试，证明了有希望的BP可比精度。
<details>	<summary>英文摘要</summary>	Several recent studies attempt to address the biological implausibility of the well-known backpropagation (BP) method. While promising methods such as feedback alignment, direct feedback alignment, and their variants like sign-concordant feedback alignment tackle BP's weight transport problem, their validity remains controversial owing to a set of other unsolved issues. In this work, we answer the question of whether it is possible to realize random backpropagation solely based on mechanisms observed in neuroscience. We propose a hypothetical framework consisting of a new microcircuit architecture and its supporting Hebbian learning rules. Comprising three types of cells and two types of synaptic connectivity, the proposed microcircuit architecture computes and propagates error signals through local feedback connections and supports the training of multi-layered spiking neural networks with a globally defined spiking error function. We employ the Hebbian rule operating in local compartments to update synaptic weights and achieve supervised learning in a biologically plausible manner. Finally, we interpret the proposed framework from an optimization point of view and show its equivalence to sign-concordant feedback alignment. The proposed framework is benchmarked on several datasets including MNIST and CIFAR10, demonstrating promising BP-comparable accuracy. </details>
<details>	<summary>邮件日期</summary>	2022年05月17日</details>

# 437、基于梯度重加权的脉冲神经网络时间有效训练
- [ ] Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting 
时间：2022年05月15日                         第一作者：Shikuang Deng                       [链接](https://arxiv.org/abs/2202.11946).                     
<details>	<summary>注释</summary>	Published as a conference paper at ICLR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月17日</details>

# 436、深SNN中MaxPooling操作的脉冲近似
- [ ] Spiking Approximations of the MaxPooling Operation in Deep SNNs 
时间：2022年05月14日                         第一作者：Ramashish Gaurav                       [链接](https://arxiv.org/abs/2205.07076).                     
## 摘要：脉冲神经网络（SNN）是一个新兴的受生物启发的神经网络领域，已显示出低功耗AI的前景。有许多方法可以构建深层SNN，其中人工神经网络（ANN）到SNN的转换非常成功。卷积神经网络（CNN）中的MaxPooling层是对中间特征映射进行降采样并引入平移不变性的一个组成部分，但由于缺乏硬件友好的脉冲等价物，限制了此类CNN向深层SNN的转换。在本文中，我们提出了两种硬件友好的方法来实现深度SNN中的最大池，从而便于将具有最大池层的CNN轻松转换为SNN。首先，我们还在Intel的Loihi神经形态硬件（使用MNIST、FMNIST和CIFAR10数据集）上执行具有脉冲MaxPooling层的SNN；从而说明了我们方法的可行性。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) are an emerging domain of biologically inspired neural networks that have shown promise for low-power AI. A number of methods exist for building deep SNNs, with Artificial Neural Network (ANN)-to-SNN conversion being highly successful. MaxPooling layers in Convolutional Neural Networks (CNNs) are an integral component to downsample the intermediate feature maps and introduce translational invariance, but the absence of their hardware-friendly spiking equivalents limits such CNNs' conversion to deep SNNs. In this paper, we present two hardware-friendly methods to implement Max-Pooling in deep SNNs, thus facilitating easy conversion of CNNs with MaxPooling layers to SNNs. In a first, we also execute SNNs with spiking-MaxPooling layers on Intel's Loihi neuromorphic hardware (with MNIST, FMNIST, & CIFAR10 dataset); thus, showing the feasibility of our approach. </details>
<details>	<summary>注释</summary>	Accepted in IJCNN-2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月17日</details>

# 435、SpiNNaker海马CA3区生物激发记忆的棘波计算模型
- [ ] Spike-based computational models of bio-inspired memories in the hippocampal CA3 region on SpiNNaker 
时间：2022年05月10日                         第一作者：Daniel Casanueva-Morato                       [链接](https://arxiv.org/abs/2205.04782).                     
## 摘要：人脑是当今存在的最强大、最高效的机器，在许多方面超过了现代计算机的能力。目前，神经形态工程学的研究领域正在尝试开发模拟大脑功能的硬件，以获得这些卓越的功能。其中一个仍在发展中的领域是仿生记忆的设计，海马体在其中发挥着重要作用。大脑的这一区域起着短期记忆的作用，能够存储大脑中不同感觉流的信息关联，并在以后回忆它们。这是可能的，这要归功于构成海马主要亚区CA3的循环侧支网络结构。在这项工作中，我们开发了两个基于棘波的全功能海马仿生记忆计算模型，用于在SpiNNaker硬件平台上使用棘波神经网络实现复杂模式的存储和回忆。这些模型呈现了不同层次的生物抽象，第一个模型具有更接近生物模型的恒定振荡活性，第二个模型具有节能调节活性，尽管它仍然是受生物启发的，但选择了更具功能性的方法。为了测试他们的学习/回忆能力，对每个模型进行了不同的实验。对所提出模型的功能性和生物学合理性进行了全面比较，显示了它们的优缺点。这两种模型可供研究人员公开使用，可为未来基于峰值的实现和应用铺平道路。
<details>	<summary>英文摘要</summary>	The human brain is the most powerful and efficient machine in existence today, surpassing in many ways the capabilities of modern computers. Currently, lines of research in neuromorphic engineering are trying to develop hardware that mimics the functioning of the brain to acquire these superior capabilities. One of the areas still under development is the design of bio-inspired memories, where the hippocampus plays an important role. This region of the brain acts as a short-term memory with the ability to store associations of information from different sensory streams in the brain and recall them later. This is possible thanks to the recurrent collateral network architecture that constitutes CA3, the main sub-region of the hippocampus. In this work, we developed two spike-based computational models of fully functional hippocampal bio-inspired memories for the storage and recall of complex patterns implemented with spiking neural networks on the SpiNNaker hardware platform. These models present different levels of biological abstraction, with the first model having a constant oscillatory activity closer to the biological model, and the second one having an energy-efficient regulated activity, which, although it is still bio-inspired, opts for a more functional approach. Different experiments were performed for each of the models, in order to test their learning/recalling capabilities. A comprehensive comparison between the functionality and the biological plausibility of the presented models was carried out, showing their strengths and weaknesses. The two models, which are publicly available for researchers, could pave the way for future spike-based implementations and applications. </details>
<details>	<summary>注释</summary>	9 pages, 6 figures, 1 table, conference, IJCNN 2022, accepted for publication </details>
<details>	<summary>邮件日期</summary>	2022年05月11日</details>

# 434、基于梯度重加权的脉冲神经网络时间有效训练
- [ ] Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting 
时间：2022年05月10日                         第一作者：Shikuang Deng                       [链接](https://arxiv.org/abs/2202.11946).                     
<details>	<summary>注释</summary>	Published as a conference paper at ICLR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月11日</details>

# 433、基于脉冲神经网络的汽车事件数据目标检测
- [ ] Object Detection with Spiking Neural Networks on Automotive Event Data 
时间：2022年05月09日                         第一作者：Lo\"ic Cordone                       [链接](https://arxiv.org/abs/2205.04339).                     
## 摘要：汽车嵌入式算法在延迟、准确性和功耗方面有很高的限制。在这项工作中，我们建议直接对来自事件摄影机的数据训练脉冲神经网络（SNN），以设计快速高效的汽车嵌入式应用程序。事实上，SNN是一种更具生物真实感的神经网络，其中神经元使用离散和异步脉冲进行通信，这是一种自然节能且对硬件友好的操作模式。因此，在空间和时间上都是二进制且稀疏的事件数据是脉冲神经网络的理想输入。但到目前为止，它们的性能还不足以解决汽车实际问题，例如在不受控制的环境中检测复杂物体。为了解决这个问题，我们利用棘波反向传播方面的最新进展（替代梯度学习、参数LIF、棘波果冻框架）和我们新的事件编码，基于流行的深度学习网络训练4种不同的SNN：挤压网、VGG、MobileNet和DenseNet。因此，我们设法增加了文献中通常考虑的SNN的大小和复杂性。在本文中，我们对两个汽车事件数据集进行了实验，为脉冲神经网络建立了最新的分类结果。基于这些结果，我们将SNN与SSD相结合，提出了第一个能够在复杂的GEN1汽车检测事件数据集上执行目标检测的脉冲神经网络。
<details>	<summary>英文摘要</summary>	Automotive embedded algorithms have very high constraints in terms of latency, accuracy and power consumption. In this work, we propose to train spiking neural networks (SNNs) directly on data coming from event cameras to design fast and efficient automotive embedded applications. Indeed, SNNs are more biologically realistic neural networks where neurons communicate using discrete and asynchronous spikes, a naturally energy-efficient and hardware friendly operating mode. Event data, which are binary and sparse in space and time, are therefore the ideal input for spiking neural networks. But to date, their performance was insufficient for automotive real-world problems, such as detecting complex objects in an uncontrolled environment. To address this issue, we took advantage of the latest advancements in matter of spike backpropagation - surrogate gradient learning, parametric LIF, SpikingJelly framework - and of our new \textit{voxel cube} event encoding to train 4 different SNNs based on popular deep learning networks: SqueezeNet, VGG, MobileNet, and DenseNet. As a result, we managed to increase the size and the complexity of SNNs usually considered in the literature. In this paper, we conducted experiments on two automotive event datasets, establishing new state-of-the-art classification results for spiking neural networks. Based on these results, we combined our SNNs with SSD to propose the first spiking neural networks capable of performing object detection on the complex GEN1 Automotive Detection event dataset. </details>
<details>	<summary>注释</summary>	Accepted to the International Joint Conference on Neural Networks (IJCNN) 2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月10日</details>

# 432、SpiNNaker上使用脉冲神经网络执行逻辑运算的基于脉冲的构建块
- [ ] Spike-based building blocks for performing logic operations using Spiking Neural Networks on SpiNNaker 
时间：2022年05月09日                         第一作者：Alvaro Ayuso-Martinez                       [链接](https://arxiv.org/abs/2205.04430).                     
## 摘要：其中一个最有趣且仍在不断发展的科学领域是神经形态工程学，它专注于研究和设计硬件和软件，目的是模仿生物神经系统的基本原理。目前，有许多研究小组基于神经科学知识开发实际应用。这项工作为研究人员提供了一种基于脉冲神经网络的新型构建块工具包，该网络模拟不同逻辑门的行为。由于逻辑门是数字电路的基础，因此这些在许多基于脉冲的应用中可能非常有用。提出的设计和模型在SpiNNaker硬件平台上进行了介绍和实现。为了验证预期行为，进行了不同的实验，并对所得结果进行了讨论。研究了传统逻辑门和所提出的模块的功能，并讨论了所提出方法的可行性。
<details>	<summary>英文摘要</summary>	One of the most interesting and still growing scientific fields is neuromorphic engineering, which is focused on studying and designing hardware and software with the purpose of mimicking the basic principles of biological nervous systems. Currently, there are many research groups developing practical applications based on neuroscientific knowledge. This work provides researchers with a novel toolkit of building blocks based on Spiking Neural Networks that emulate the behavior of different logic gates. These could be very useful in many spike-based applications, since logic gates are the basis of digital circuits. The designs and models proposed are presented and implemented on a SpiNNaker hardware platform. Different experiments were performed in order to validate the expected behavior, and the obtained results are discussed. The functionality of traditional logic gates and the proposed blocks is studied, and the feasibility of the presented approach is discussed. </details>
<details>	<summary>注释</summary>	9 pages, 9 figures, 1 table, conference, IJCNN 2022, accepted for publication </details>
<details>	<summary>邮件日期</summary>	2022年05月10日</details>

# 431、IM／DD光通信中的脉冲神经网络均衡
- [ ] Spiking Neural Network Equalization for IM/DD Optical Communication 
时间：2022年05月09日                         第一作者：Elias Arnold                       [链接](https://arxiv.org/abs/2205.04263).                     
## 摘要：针对IM／DD链路，设计了一种适用于电子神经形态硬件的脉冲神经网络（SNN）均衡器模型。SNN实现了与人工神经网络相同的误码率，优于线性均衡。
<details>	<summary>英文摘要</summary>	A spiking neural network (SNN) equalizer model suitable for electronic neuromorphic hardware is designed for an IM/DD link. The SNN achieves the same bit-error-rate as an artificial neural network, outperforming linear equalization. </details>
<details>	<summary>邮件日期</summary>	2022年05月10日</details>

# 430、加速神经形态基底上脉冲神经网络的通用仿真
- [ ] Versatile emulation of spiking neural networks on an accelerated neuromorphic substrate 
时间：2022年05月09日                         第一作者：Sebastian Billaudelle                       [链接](https://arxiv.org/abs/1912.12980).                     
<details>	<summary>邮件日期</summary>	2022年05月10日</details>

# 429、突发脉冲神经网络的高效精确转换
- [ ] Efficient and Accurate Conversion of Spiking Neural Network with Burst Spikes 
时间：2022年05月08日                         第一作者：Yang Li                       [链接](https://arxiv.org/abs/2204.13271).                     
<details>	<summary>注释</summary>	This paper was accepted by IJCAI2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月10日</details>

# 428、通过参数标定将人工神经网络转换为脉冲神经网络
- [ ] Converting Artificial Neural Networks to Spiking Neural Networks via Parameter Calibration 
时间：2022年05月06日                         第一作者：Yuhang Li                       [链接](https://arxiv.org/abs/2205.10121).                     
## 摘要：脉冲神经网络（SNN）起源于生物学中的神经行为，是公认的下一代神经网络之一。传统上，SNN可以通过将预先训练的人工神经网络（ANN）转换为脉冲神经元来代替非线性激活，而无需改变参数。在这项工作中，我们认为简单地将ANN的权重复制和粘贴到SNN中不可避免地会导致激活不匹配，尤其是对于使用批量归一化（BN）层训练的ANN。为了解决激活不匹配问题，我们首先通过将局部转换误差分解为剪裁误差和地板误差进行理论分析，然后使用二阶分析定量测量该误差如何在整个层中传播。受理论结果的启发，我们提出了一套分层参数校准算法，通过调整参数来最小化激活失配。在现代体系结构和大规模任务（包括ImageNet分类和MS COCO检测）上对所提出的算法进行了广泛的实验。我们证明，我们的方法可以处理带有批量归一化层的SNN转换，并且即使在32个时间步内也能有效地保持较高的精度。例如，当使用BN层转换VGG-16时，我们的校准算法可以提高高达65%的精度。
<details>	<summary>英文摘要</summary>	Spiking Neural Network (SNN), originating from the neural behavior in biology, has been recognized as one of the next-generation neural networks. Conventionally, SNNs can be obtained by converting from pre-trained Artificial Neural Networks (ANNs) by replacing the non-linear activation with spiking neurons without changing the parameters. In this work, we argue that simply copying and pasting the weights of ANN to SNN inevitably results in activation mismatch, especially for ANNs that are trained with batch normalization (BN) layers. To tackle the activation mismatch issue, we first provide a theoretical analysis by decomposing local conversion error to clipping error and flooring error, and then quantitatively measure how this error propagates throughout the layers using the second-order analysis. Motivated by the theoretical results, we propose a set of layer-wise parameter calibration algorithms, which adjusts the parameters to minimize the activation mismatch. Extensive experiments for the proposed algorithms are performed on modern architectures and large-scale tasks including ImageNet classification and MS COCO detection. We demonstrate that our method can handle the SNN conversion with batch normalization layers and effectively preserve the high accuracy even in 32 time steps. For example, our calibration algorithms can increase up to 65% accuracy when converting VGG-16 with BN layers. </details>
<details>	<summary>注释</summary>	arXiv admin note: text overlap with arXiv:2106.06984 </details>
<details>	<summary>邮件日期</summary>	2022年05月23日</details>

# 427、脉冲图卷积网络
- [ ] Spiking Graph Convolutional Networks 
时间：2022年05月05日                         第一作者：Zulun Zhu                       [链接](https://arxiv.org/abs/2205.02767).                     
## 摘要：图卷积网络（GCN）由于在学习图信息时具有显著的表示能力而获得了令人印象深刻的性能。然而，当在深度网络上实施GCN时，需要昂贵的计算能力，使其难以部署在电池供电的设备上。相比之下，执行生物保真度推理过程的脉冲神经网络（SNN）提供了一种节能的神经架构。在这项工作中，我们提出了SPIKINGCN，这是一个端到端的框架，旨在将GCN的嵌入与SNN的生物相似性特征相结合。原始图形数据基于图形卷积的合并被编码成脉冲序列。我们进一步利用与神经元节点相结合的完全连接层来模拟生物信息处理。在广泛的场景中（例如引文网络、图像图分类和推荐系统），我们的实验结果表明，所提出的方法可以获得与最先进方法相比的竞争性能。此外，我们还表明，在神经形态芯片上添加GCN可以在图形数据分析中带来明显的能效优势，这表明它在构建环境友好的机器学习模型方面具有巨大潜力。
<details>	<summary>英文摘要</summary>	Graph Convolutional Networks (GCNs) achieve an impressive performance due to the remarkable representation ability in learning the graph information. However, GCNs, when implemented on a deep network, require expensive computation power, making them difficult to be deployed on battery-powered devices. In contrast, Spiking Neural Networks (SNNs), which perform a bio-fidelity inference process, offer an energy-efficient neural architecture. In this work, we propose SpikingGCN, an end-to-end framework that aims to integrate the embedding of GCNs with the biofidelity characteristics of SNNs. The original graph data are encoded into spike trains based on the incorporation of graph convolution. We further model biological information processing by utilizing a fully connected layer combined with neuron nodes. In a wide range of scenarios (e.g. citation networks, image graph classification, and recommender systems), our experimental results show that the proposed method could gain competitive performance against state-of-the-art approaches. Furthermore, we show that SpikingGCN on a neuromorphic chip can bring a clear advantage of energy efficiency into graph data analysis, which demonstrates its great potential to construct environment-friendly machine learning models. </details>
<details>	<summary>注释</summary>	Accepted by IJCAI 2022; Code available at https://github.com/ZulunZhu/SpikingGCN </details>
<details>	<summary>邮件日期</summary>	2022年05月06日</details>

# 426、基于模拟脉冲神经网络的声场景分析
- [ ] Acoustic Scene Analysis using Analog Spiking Neural Network 
时间：2022年05月03日                         第一作者：An                       [链接](https://arxiv.org/abs/1912.10905).                     
<details>	<summary>注释</summary>	21 pages, Journal </details>
<details>	<summary>邮件日期</summary>	2022年05月04日</details>

# 425、利用灵长类视觉皮层脉冲神经网络特征的显著性图
- [ ] Saliency map using features derived from spiking neural networks of primate visual cortex 
时间：2022年05月02日                         第一作者：Reza Hojjaty Saeedy                       [链接](https://arxiv.org/abs/2205.01159).                     
## 摘要：我们提出了一个受生物视觉系统启发的框架来生成数字图像的显著性图。使用了视觉皮层中专门用于颜色和方向感知的区域的感受野的著名计算模型。为了模拟这些区域之间的连接，我们使用了CARLsim库，这是一个脉冲神经网络（SNN）模拟器。CARLsim生成的脉冲，然后作为提取的特征并输入到我们的显著性检测算法中。描述了这种新的显著性检测方法，并将其应用于基准图像。
<details>	<summary>英文摘要</summary>	We propose a framework inspired by biological vision systems to produce saliency maps of digital images. Well-known computational models for receptive fields of areas in the visual cortex that are specialized for color and orientation perception are used. To model the connectivity between these areas we use the CARLsim library which is a spiking neural network(SNN) simulator. The spikes generated by CARLsim, then serve as extracted features and input to our saliency detection algorithm. This new method of saliency detection is described and applied to benchmark images. </details>
<details>	<summary>注释</summary>	19 pages, 8 figures, 1 table </details>
<details>	<summary>邮件日期</summary>	2022年05月04日</details>

# 424、一种优化的无梯度深脉冲神经网络结构
- [ ] An optimised deep spiking neural network architecture without gradients 
时间：2022年05月02日                         第一作者：Yeshwanth Bethi                       [链接](https://arxiv.org/abs/2109.12813).                     
<details>	<summary>注释</summary>	18 pages, 6 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2022年05月04日</details>

# 423、基于片上可塑性的Loihi序列学习与整合
- [ ] Sequence Learning and Consolidation on Loihi using On-chip Plasticity 
时间：2022年05月02日                         第一作者：Jack Lindsey                       [链接](https://arxiv.org/abs/2205.00643).                     
## 摘要：在这项工作中，我们开发了一个神经形态硬件上的预测学习模型。我们的模型使用Loihi芯片的芯片可塑性能力来记忆观察到的事件序列，并使用该存储器实时生成对未来事件的预测。考虑到芯片塑性规则的局部性约束，在不干扰正在进行的学习过程的情况下生成预测是非常重要的。我们以海马回放为灵感，采用记忆巩固方法来应对这一挑战。序列存储器使用脉冲时间依赖性可塑性存储在初始存储器模块中。之后，在离线期间，记忆被整合到一个独特的预测模块中。然后，第二个模块能够表示预测的未来事件，而不干扰第一个模块中的活动和可塑性，从而实现预测和地面真相观测之间的在线比较。我们的模型证明了在线预测学习模型可以部署在具有片上可塑性的神经形态硬件上。
<details>	<summary>英文摘要</summary>	In this work we develop a model of predictive learning on neuromorphic hardware. Our model uses the on-chip plasticity capabilities of the Loihi chip to remember observed sequences of events and use this memory to generate predictions of future events in real time. Given the locality constraints of on-chip plasticity rules, generating predictions without interfering with the ongoing learning process is nontrivial. We address this challenge with a memory consolidation approach inspired by hippocampal replay. Sequence memory is stored in an initial memory module using spike-timing dependent plasticity. Later, during an offline period, memories are consolidated into a distinct prediction module. This second module is then able to represent predicted future events without interfering with the activity, and plasticity, in the first module, enabling online comparison between predictions and ground-truth observations. Our model serves as a proof-of-concept that online predictive learning models can be deployed on neuromorphic hardware with on-chip plasticity. </details>
<details>	<summary>注释</summary>	NICE 2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月03日</details>

# 422、用于目标检测的稀疏压缩脉冲神经网络加速器
- [ ] Sparse Compressed Spiking Neural Network Accelerator for Object Detection 
时间：2022年05月02日                         第一作者：Hong-Han Lien                        [链接](https://arxiv.org/abs/2205.00778).                     
## 摘要：受人脑启发的脉冲神经网络（Spiking neural networks，SNN）由于其相对简单、低功耗的硬件传输二进制脉冲和高度稀疏的激活图，最近得到了广泛的应用。然而，由于SNN包含额外的时间维度信息，SNN加速器将需要更多的缓冲区，并需要更长的时间来推断，尤其是对于更困难的高分辨率目标检测任务。因此，本文提出了一种稀疏压缩脉冲神经网络加速器，该加速器利用了激活映射和权重的高度稀疏性，通过利用所提出的选通一对所有乘积来实现低功耗和高度并行的模型执行。神经网络的实验结果显示，在IVS 3cls数据集上，71.5$\%$映射具有混合（1,3）时间步长。采用TSMC 28nm CMOS工艺的加速器可达到1024$\倍$576@29以500MHz频率运行时每秒处理帧数，能效为35.88TOPS/W，每帧能耗为1.05mJ。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs), which are inspired by the human brain, have recently gained popularity due to their relatively simple and low-power hardware for transmitting binary spikes and highly sparse activation maps. However, because SNNs contain extra time dimension information, the SNN accelerator will require more buffers and take longer to infer, especially for the more difficult high-resolution object detection task. As a result, this paper proposes a sparse compressed spiking neural network accelerator that takes advantage of the high sparsity of activation maps and weights by utilizing the proposed gated one-to-all product for low power and highly parallel model execution. The experimental result of the neural network shows 71.5$\%$ mAP with mixed (1,3) time steps on the IVS 3cls dataset. The accelerator with the TSMC 28nm CMOS process can achieve 1024$\times$576@29 frames per second processing when running at 500MHz with 35.88TOPS/W energy efficiency and 1.05mJ energy consumption per frame. </details>
<details>	<summary>注释</summary>	11 pages, 18 figures, to be published in IEEE Transactions on Circuits and Systems--I: Regular Papers ACM-class: B.5.m DOI: 10.1109/TCSI.2022.3149006 </details>
<details>	<summary>邮件日期</summary>	2022年05月03日</details>

# 421、脉冲神经网络的最新进展和新前沿
- [ ] Recent Advances and New Frontiers in Spiking Neural Networks 
时间：2022年05月02日                         第一作者：Duzhen Zhang                       [链接](https://arxiv.org/abs/2204.07050).                     
<details>	<summary>注释</summary>	Accepted at IJCAI2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月03日</details>

# 420、利用脉冲表征微分训练高性能低潜伏期脉冲神经网络
- [ ] Training High-Performance Low-Latency Spiking Neural Networks by Differentiation on Spike Representation 
时间：2022年05月01日                         第一作者：Qingyan Meng                       [链接](https://arxiv.org/abs/2205.00459).                     
## 摘要：当在神经形态硬件上实现时，脉冲神经网络（SNN）是一种很有前途的节能人工智能模型。然而，由于SNN的不可微性，有效地训练SNN是一个挑战。大多数现有方法要么存在高延迟（即长模拟时间步长），要么无法实现人工神经网络（ANN）那样的高性能。在本文中，我们提出了基于脉冲表示的差分（DSR）方法，该方法可以在低延迟的情况下获得与人工神经网络竞争的高性能。首先，我们使用（加权）发射率编码将脉冲序列编码为脉冲表示。在脉冲表示的基础上，我们系统地推导了具有常见神经模型的脉冲动态可以表示为一些次可微映射。基于这种观点，我们提出的DSR方法通过映射的梯度来训练SNN，避免了SNN训练中常见的不可微性问题。然后，我们分析了用SNN的正向计算表示特定映射时的误差。为了减少这种误差，我们建议在每一层中训练脉冲阈值，并为神经模型引入一个新的超参数。有了这些组件，DSR方法可以在静态和神经形态数据集（包括CIFAR-10、CIFAR-100、ImageNet和DVS-CIFAR10）上以低延迟实现最先进的SNN性能。
<details>	<summary>英文摘要</summary>	Spiking Neural Network (SNN) is a promising energy-efficient AI model when implemented on neuromorphic hardware. However, it is a challenge to efficiently train SNNs due to their non-differentiability. Most existing methods either suffer from high latency (i.e., long simulation time steps), or cannot achieve as high performance as Artificial Neural Networks (ANNs). In this paper, we propose the Differentiation on Spike Representation (DSR) method, which could achieve high performance that is competitive to ANNs yet with low latency. First, we encode the spike trains into spike representation using (weighted) firing rate coding. Based on the spike representation, we systematically derive that the spiking dynamics with common neural models can be represented as some sub-differentiable mapping. With this viewpoint, our proposed DSR method trains SNNs through gradients of the mapping and avoids the common non-differentiability problem in SNN training. Then we analyze the error when representing the specific mapping with the forward computation of the SNN. To reduce such error, we propose to train the spike threshold in each layer, and to introduce a new hyperparameter for the neural models. With these components, the DSR method can achieve state-of-the-art SNN performance with low latency on both static and neuromorphic datasets, including CIFAR-10, CIFAR-100, ImageNet, and DVS-CIFAR10. </details>
<details>	<summary>注释</summary>	Accepted by CVPR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月03日</details>

# 419、具有突发脉冲的脉冲神经网络的高效精确转换
- [ ] Efficient and Accurate Conversion of Spiking Neural Network with Burst Spikes 
时间：2022年04月28日                         第一作者：Yang Li                       [链接](https://arxiv.org/abs/2204.13271).                     
## 摘要：脉冲神经网络（SNN）作为一种受大脑启发的节能神经网络，引起了研究人员的兴趣。而脉冲神经网络的训练仍然是一个悬而未决的问题。一种有效的方法是将训练后的神经网络的权重映射到SNN，以获得较高的推理能力。然而，转换后的脉冲神经网络往往会出现性能下降和相当大的时延。为了加快推理过程并获得更高的精度，我们从三个角度对转换过程中的错误进行了理论分析：IF和ReLU之间的差异、时间维度和池运算。我们提出了一种释放突发脉冲的神经元模型，这是一种廉价但高效的解决剩余信息的方法。此外，还提出了侧向抑制池（LIPooling）来解决最大池在转换过程中造成的不准确问题。在CIFAR和ImageNet上的实验结果表明，我们的算法是有效和准确的。例如，我们的方法可以确保SNN的几乎无损转换，在0.693$\倍于典型方法能耗的情况下，仅使用大约1/10（小于100）的模拟时间。我们的代码可在https://github.com/Brain-Inspired-Cognitive-Engine/Conversion_Burst.
<details>	<summary>英文摘要</summary>	Spiking neural network (SNN), as a brain-inspired energy-efficient neural network, has attracted the interest of researchers. While the training of spiking neural networks is still an open problem. One effective way is to map the weight of trained ANN to SNN to achieve high reasoning ability. However, the converted spiking neural network often suffers from performance degradation and a considerable time delay. To speed up the inference process and obtain higher accuracy, we theoretically analyze the errors in the conversion process from three perspectives: the differences between IF and ReLU, time dimension, and pooling operation. We propose a neuron model for releasing burst spikes, a cheap but highly efficient method to solve residual information. In addition, Lateral Inhibition Pooling (LIPooling) is proposed to solve the inaccuracy problem caused by MaxPooling in the conversion process. Experimental results on CIFAR and ImageNet demonstrate that our algorithm is efficient and accurate. For example, our method can ensure nearly lossless conversion of SNN and only use about 1/10 (less than 100) simulation time under 0.693$\times$ energy consumption of the typical method. Our code is available at https://github.com/Brain-Inspired-Cognitive-Engine/Conversion_Burst. </details>
<details>	<summary>注释</summary>	This paper was accepted by IJCAI2022 </details>
<details>	<summary>邮件日期</summary>	2022年04月29日</details>

# 418、脉冲神经网络的最新进展和新前沿
- [ ] Recent Advances and New Frontiers in Spiking Neural Networks 
时间：2022年04月25日                         第一作者：Duzhen Zhang                       [链接](https://arxiv.org/abs/2204.07050).                     
<details>	<summary>注释</summary>	Accepted at IJCAI2022 </details>
<details>	<summary>邮件日期</summary>	2022年04月26日</details>

# 417、MAP-SNN：将具有多样性、适应性和可塑性的脉冲活动映射到生物似脉冲神经网络
- [ ] MAP-SNN: Mapping Spike Activities with Multiplicity, Adaptability, and Plasticity into Bio-Plausible Spiking Neural Networks 
时间：2022年04月21日                         第一作者：Chengting Yu                       [链接](https://arxiv.org/abs/2204.09893).                     
## 摘要：脉冲神经网络（SNN）模仿人脑的基本机制，因此被认为在生物学上更真实、更节能。最近，利用深度学习框架的基于反向传播（BP）的SNN学习算法取得了良好的性能。然而，在这些基于BP的算法中，生物可解释性部分被忽略。对于生物合理的基于BP的SNN，我们在模拟脉冲活动时考虑了三个属性：多样性、适应性和可塑性（MAP）。在多重性方面，我们提出了一种具有多脉冲传输的多脉冲模式（MSP），以增强离散时间迭代中的模型鲁棒性。为了实现适应性，我们在MSP下采用了脉冲频率自适应（SFA）来减少脉冲活动以提高效率。在可塑性方面，我们提出了一种可训练的卷积突触，该突触模拟脉冲反应电流，以增强脉冲神经元的多样性，用于时间特征提取。提出的SNN模型在神经形态数据集：N-MNIST和SHD上实现了竞争性性能。此外，实验结果表明，提出的三个方面对脉冲活动的迭代鲁棒性、脉冲效率和时间特征提取能力具有重要意义。综上所述，本研究提出了一种利用MAP进行生物激发棘波活动的可行方案，为将生物学特性嵌入棘波神经网络提供了一种新的神经形态学视角。
<details>	<summary>英文摘要</summary>	Spiking Neural Network (SNN) is considered more biologically realistic and power-efficient as it imitates the fundamental mechanism of the human brain. Recently, backpropagation (BP) based SNN learning algorithms that utilize deep learning frameworks have achieved good performance. However, bio-interpretability is partially neglected in those BP-based algorithms. Toward bio-plausible BP-based SNNs, we consider three properties in modeling spike activities: Multiplicity, Adaptability, and Plasticity (MAP). In terms of multiplicity, we propose a Multiple-Spike Pattern (MSP) with multiple spike transmission to strengthen model robustness in discrete time-iteration. To realize adaptability, we adopt Spike Frequency Adaption (SFA) under MSP to decrease spike activities for improved efficiency. For plasticity, we propose a trainable convolutional synapse that models spike response current to enhance the diversity of spiking neurons for temporal feature extraction. The proposed SNN model achieves competitive performances on neuromorphic datasets: N-MNIST and SHD. Furthermore, experimental results demonstrate that the proposed three aspects are significant to iterative robustness, spike efficiency, and temporal feature extraction capability of spike activities. In summary, this work proposes a feasible scheme for bio-inspired spike activities with MAP, offering a new neuromorphic perspective to embed biological characteristics into spiking neural networks. </details>
<details>	<summary>邮件日期</summary>	2022年04月22日</details>

# 416、轴突延迟作为前馈深脉冲神经网络的短期记忆
- [ ] Axonal Delay As a Short-Term Memory for Feed Forward Deep Spiking Neural Networks 
时间：2022年04月20日                         第一作者：Pengfei Sun                       [链接](https://arxiv.org/abs/2205.02115).                     
## 摘要：脉冲神经网络（SNN）的信息通过脉冲在相邻的生物神经元之间传播，这为模拟人脑提供了一种计算范式。最近的研究发现，神经元的时间延迟在学习过程中起着重要作用。因此，配置脉冲的精确定时对于理解和改进SNN中时间信息的传输过程是一个有希望的方向。然而，现有的棘突神经元学习方法大多侧重于突触重量的调节，而对轴突延迟的研究很少。在本文中，我们验证了将时间延迟整合到监督学习中的有效性，并提出了一个通过短期记忆调节轴突延迟的模块。为此，将校正轴突延迟（RAD）模块与脉冲模型集成，以调整脉冲时间，从而提高时间特征的表征学习能力。在三个神经形态基准数据集NMNIST、DVS手势和N-TIDIGITS18上的实验表明，该方法在使用最少参数的情况下达到了最先进的性能。
<details>	<summary>英文摘要</summary>	The information of spiking neural networks (SNNs) are propagated between the adjacent biological neuron by spikes, which provides a computing paradigm with the promise of simulating the human brain. Recent studies have found that the time delay of neurons plays an important role in the learning process. Therefore, configuring the precise timing of the spike is a promising direction for understanding and improving the transmission process of temporal information in SNNs. However, most of the existing learning methods for spiking neurons are focusing on the adjustment of synaptic weight, while very few research has been working on axonal delay. In this paper, we verify the effectiveness of integrating time delay into supervised learning and propose a module that modulates the axonal delay through short-term memory. To this end, a rectified axonal delay (RAD) module is integrated with the spiking model to align the spike timing and thus improve the characterization learning ability of temporal features. Experiments on three neuromorphic benchmark datasets : NMNIST, DVS Gesture and N-TIDIGITS18 show that the proposed method achieves the state-of-the-art performance while using the fewest parameters. </details>
<details>	<summary>注释</summary>	Accepted at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, 2022 </details>
<details>	<summary>邮件日期</summary>	2022年05月05日</details>

# 415、超导光电单光子突触的演示
- [ ] Demonstration of Superconducting Optoelectronic Single-Photon Synapses 
时间：2022年04月20日                         第一作者：Saeed Khan                       [链接](https://arxiv.org/abs/2204.09665).                     
## 摘要：超导光电硬件正在探索一条通向人工脉冲神经网络的道路，它具有前所未有的复杂性和计算能力。这种硬件将用于少光子、光速通信的集成光子组件与用于快速、节能计算的超导电路相结合。超导和光子器件的单片集成对于该技术的扩展是必要的。在目前的工作中，超导纳米线单光子探测器首次与约瑟夫森结单片集成，实现了超导光电突触。我们提出了对单光子突触前信号进行模拟加权和时间泄漏积分的电路。突触加权是在电子领域实现的，这样就可以维持二进制单光子通信。最近突触活动的记录以电流的形式存储在超导回路中。树突和神经元的非线性通过约瑟夫森电路的第二阶段实现。该硬件具有极大的设计灵活性，具有跨越四个数量级（数百纳秒到毫秒）的突触时间常数。突触对超过10 MHz的突触前峰值频率有反应，在考虑冷却之前，每个突触事件消耗大约33 aJ的动态功率。除了神经形态硬件外，这些电路还为实现用于各种成像、传感和量子通信应用的大规模单光子探测器阵列提供了新的途径。
<details>	<summary>英文摘要</summary>	Superconducting optoelectronic hardware is being explored as a path towards artificial spiking neural networks with unprecedented scales of complexity and computational ability. Such hardware combines integrated-photonic components for few-photon, light-speed communication with superconducting circuits for fast, energy-efficient computation. Monolithic integration of superconducting and photonic devices is necessary for the scaling of this technology. In the present work, superconducting-nanowire single-photon detectors are monolithically integrated with Josephson junctions for the first time, enabling the realization of superconducting optoelectronic synapses. We present circuits that perform analog weighting and temporal leaky integration of single-photon presynaptic signals. Synaptic weighting is implemented in the electronic domain so that binary, single-photon communication can be maintained. Records of recent synaptic activity are locally stored as current in superconducting loops. Dendritic and neuronal nonlinearities are implemented with a second stage of Josephson circuitry. The hardware presents great design flexibility, with demonstrated synaptic time constants spanning four orders of magnitude (hundreds of nanoseconds to milliseconds). The synapses are responsive to presynaptic spike rates exceeding 10 MHz and consume approximately 33 aJ of dynamic power per synapse event before accounting for cooling. In addition to neuromorphic hardware, these circuits introduce new avenues towards realizing large-scale single-photon-detector arrays for diverse imaging, sensing, and quantum communication applications. </details>
<details>	<summary>注释</summary>	23 pages, 20 figures </details>
<details>	<summary>邮件日期</summary>	2022年04月21日</details>

# 414、脉冲神经网络的最新进展和新前沿
- [ ] Recent Advances and New Frontiers in Spiking Neural Networks 
时间：2022年04月17日                         第一作者：Duzhen Zhang                       [链接](https://arxiv.org/abs/2204.07050).                     
<details>	<summary>注释</summary>	Accepted at IJCAI2022 </details>
<details>	<summary>邮件日期</summary>	2022年04月19日</details>

# 413、Sapinet：一种用于野外学习的基于稀疏事件的时空振荡器
- [ ] Sapinet: A sparse event-based spatiotemporal oscillator for learning in the wild 
时间：2022年04月13日                         第一作者：Ayon Borthakur                       [链接](https://arxiv.org/abs/2204.06216).                     
## 摘要：我们介绍了Sapinet——一种用于野外学习的基于脉冲时间（事件）的多层神经网络——也就是说：无灾难性遗忘的多输入一次性在线学习，无需数据特定的超参数重新调整。Sapinet的主要功能包括数据正则化、模型缩放、数据分类和去噪。该模型还支持刺激相似性映射。我们提出了一种系统的方法来调整网络的性能。我们研究了不同气味相似性、高斯噪声和脉冲噪声水平下的模型性能。Sapinet在标准机器嗅觉数据集上实现了高分类精度，无需对特定数据集进行微调。
<details>	<summary>英文摘要</summary>	We introduce Sapinet -- a spike timing (event)-based multilayer neural network for \textit{learning in the wild} -- that is: one-shot online learning of multiple inputs without catastrophic forgetting, and without the need for data-specific hyperparameter retuning. Key features of Sapinet include data regularization, model scaling, data classification, and denoising. The model also supports stimulus similarity mapping. We propose a systematic method to tune the network for performance. We studied the model performance on different levels of odor similarity, gaussian and impulse noise. Sapinet achieved high classification accuracies on standard machine olfaction datasets without the requirement of fine tuning for a specific dataset. </details>
<details>	<summary>注释</summary>	PhD thesis Journal-ref: "Mechanisms and Architectural Priors for Learning in the Wild" (Cornell University, Ithaca, NY, USA). ProQuest Publication Number: 28652661. Submission Date: 2021-07-28 </details>
<details>	<summary>邮件日期</summary>	2022年04月14日</details>

# 412、对抗干扰的鲁棒脉冲神经网络
- [ ] Toward Robust Spiking Neural Network Against Adversarial Perturbation 
时间：2022年04月12日                         第一作者：Ling Liang                       [链接](https://arxiv.org/abs/2205.01625).                     
## 摘要：随着脉冲神经网络（SNN）越来越多地应用于现实世界中对效率至关重要的应用，SNN中的安全问题越来越受到关注。目前，研究人员已经证明SNN可以通过对抗性的例子进行攻击。如何构建一个健壮的SNN成为一个迫切的问题。近年来，许多研究将认证训练应用于人工神经网络（ANN），这可以很好地提高神经网络模型的鲁棒性。然而，由于SNN具有不同的神经元行为和输入格式，现有的认证无法直接转移到SNN。在这项工作中，我们首先设计了S-IBP和S-CROWN来处理SNN神经元建模中的非线性函数。然后，我们将数字和脉冲输入的边界形式化。最后，我们在不同的数据集和模型体系结构中证明了我们提出的鲁棒训练方法的有效性。根据我们的实验，我们可以实现最大37.7\%$的攻击错误减少，原始准确度损失为3.7\%$。据我们所知，这是关于SNN稳健训练的首次分析。
<details>	<summary>英文摘要</summary>	As spiking neural networks (SNNs) are deployed increasingly in real-world efficiency critical applications, the security concerns in SNNs attract more attention. Currently, researchers have already demonstrated an SNN can be attacked with adversarial examples. How to build a robust SNN becomes an urgent issue. Recently, many studies apply certified training in artificial neural networks (ANNs), which can improve the robustness of an NN model promisely. However, existing certifications cannot transfer to SNNs directly because of the distinct neuron behavior and input formats for SNNs. In this work, we first design S-IBP and S-CROWN that tackle the non-linear functions in SNNs' neuron modeling. Then, we formalize the boundaries for both digital and spike inputs. Finally, we demonstrate the efficiency of our proposed robust training method in different datasets and model architectures. Based on our experiment, we can achieve a maximum $37.7\%$ attack error reduction with $3.7\%$ original accuracy loss. To the best of our knowledge, this is the first analysis on robust training of SNNs. </details>
<details>	<summary>邮件日期</summary>	2022年05月04日</details>

# 411、利用剩余脉冲神经网络进行精确特征提取的关键
- [ ] Keys to Accurate Feature Extraction Using Residual Spiking Neural Networks 
时间：2022年04月12日                         第一作者：Alex Vicente-Sola                       [链接](https://arxiv.org/abs/2111.05955).                     
<details>	<summary>注释</summary>	16 pages, 6 figures, 17 tables ACM-class: I.2.6; I.2.10; I.4.8; I.5.2; D.2.13 </details>
<details>	<summary>邮件日期</summary>	2022年04月14日</details>

# 410、基于脉冲神经网络的面向功率的故障注入攻击分析
- [ ] Analysis of Power-Oriented Fault Injection Attacks on Spiking Neural Networks 
时间：2022年04月10日                         第一作者：Karthikeyan Nagarajan                       [链接](https://arxiv.org/abs/2204.04768).                     
## 摘要：作为深度神经网络（DNN）的一种可行替代方案，脉冲神经网络（SNN）正在迅速获得关注。与DNN相比，SNN的计算能力更强，并提供更高的能效。SNN一出现就令人兴奋，但它包含安全敏感资产（如神经元阈值电压）和对手可以利用的漏洞（如分类精度对神经元阈值电压变化的敏感性）。我们通过使用外部电源和激光诱导的局部电源故障来破坏关键训练参数，如使用普通模拟神经元开发的SNN上的脉冲幅度和神经元膜阈值电位，来研究全局故障注入攻击。我们还评估了0%（即无攻击）到100%（即整个层受到攻击）的基于功率的攻击对单个SNN层的影响。我们研究了攻击对数字分类任务的影响，发现在最坏的情况下，分类准确率降低了85.65%。我们还提出了防御措施，例如，一种对面向电源的攻击免疫的强大电流驱动器设计，改进神经元组件的电路尺寸，以减少/恢复对抗性精度下降，而代价是可忽略的面积和25%的电源开销。我们还提出了一种基于虚拟神经元的电压故障注入检测系统，具有1%的功率和面积开销。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNN) are quickly gaining traction as a viable alternative to Deep Neural Networks (DNN). In comparison to DNNs, SNNs are more computationally powerful and provide superior energy efficiency. SNNs, while exciting at first appearance, contain security-sensitive assets (e.g., neuron threshold voltage) and vulnerabilities (e.g., sensitivity of classification accuracy to neuron threshold voltage change) that adversaries can exploit. We investigate global fault injection attacks by employing external power supplies and laser-induced local power glitches to corrupt crucial training parameters such as spike amplitude and neuron's membrane threshold potential on SNNs developed using common analog neurons. We also evaluate the impact of power-based attacks on individual SNN layers for 0% (i.e., no attack) to 100% (i.e., whole layer under attack). We investigate the impact of the attacks on digit classification tasks and find that in the worst-case scenario, classification accuracy is reduced by 85.65%. We also propose defenses e.g., a robust current driver design that is immune to power-oriented attacks, improved circuit sizing of neuron components to reduce/recover the adversarial accuracy degradation at the cost of negligible area and 25% power overhead. We also present a dummy neuron-based voltage fault injection detection system with 1% power and area overhead. </details>
<details>	<summary>注释</summary>	Design, Automation and Test in Europe Conference (DATE) 2022 </details>
<details>	<summary>邮件日期</summary>	2022年04月12日</details>

# 409、基于时域神经元的高能效高精度脉冲神经网络推理
- [ ] Energy-Efficient High-Accuracy Spiking Neural Network Inference Using Time-Domain Neurons 
时间：2022年04月10日                         第一作者：Joonghyun Song                       [链接](https://arxiv.org/abs/2202.02015).                     
<details>	<summary>注释</summary>	Accepted in AICAS 2022 </details>
<details>	<summary>邮件日期</summary>	2022年04月12日</details>

# 408、脉冲神经网络与人工神经网络：从生物智能到人工智能
- [ ] An Introductory Review of Spiking Neural Network and Artificial Neural Network: From Biological Intelligence to Artificial Intelligence 
时间：2022年04月09日                         第一作者：Shengjie Zheng                       [链接](https://arxiv.org/abs/2204.07519).                     
## 摘要：最近，随着人工智能的快速发展，神经科学也取得了巨大的进步。人工智能在模式识别、机器人技术和生物信息学方面取得了巨大的成功。一种具有生物可解释性的脉冲神经网络正逐渐受到广泛关注，这种神经网络也被认为是通用人工智能的发展方向之一。本综述介绍了以下几个部分，脉冲神经元的生物学背景和理论基础，不同的神经元模型，神经回路的连通性，主流的神经网络学习机制和网络结构，这篇综述希望能吸引不同的研究人员，推动脑启发智能和人工智能的发展。
<details>	<summary>英文摘要</summary>	Recently, stemming from the rapid development of artificial intelligence, which has gained expansive success in pattern recognition, robotics, and bioinformatics, neuroscience is also gaining tremendous progress. A kind of spiking neural network with biological interpretability is gradually receiving wide attention, and this kind of neural network is also regarded as one of the directions toward general artificial intelligence. This review introduces the following sections, the biological background of spiking neurons and the theoretical basis, different neuronal models, the connectivity of neural circuits, the mainstream neural network learning mechanisms and network architectures, etc. This review hopes to attract different researchers and advance the development of brain-inspired intelligence and artificial intelligence. </details>
<details>	<summary>注释</summary>	12 pages, 24 figures </details>
<details>	<summary>邮件日期</summary>	2022年04月18日</details>

# 407、一种实现强化学习的脉冲神经网络结构
- [ ] A Spiking Neural Network Structure Implementing Reinforcement Learning 
时间：2022年04月09日                         第一作者：Mikhail Kiselev                       [链接](https://arxiv.org/abs/2204.04431).                     
## 摘要：目前，尽管提出了大量的脉冲神经网络学习算法，但在脉冲神经网络（SNN）中实现学习机制并不能被视为一个已解决的科学问题。对于强化学习（RL）的SNN实现也是如此，而RL对于SNN尤其重要，因为它与从SNN应用的角度来看最有前途的领域（如机器人）密切相关。在本文中，我描述了一种SNN结构，它似乎可以用于广泛的RL任务。我的方法的显著特点是只使用所有相关信号的脉冲形式——感官输入流、发送到执行器的输出信号和奖惩信号。除此之外，选择神经元/可塑性模型时，我的指导原则是，它们应易于在现代神经芯片上实现。本文考虑的SNN结构包括由LIFAT（具有自适应阈值的漏积分和激发神经元）模型的推广和一个简单的棘波时间依赖性突触可塑性模型（多巴胺调制可塑性的推广）描述的棘波神经元。我的概念是基于关于RL任务特征的非常普遍的假设，对其适用性没有明显的限制。为了测试它，我选择了一个简单但不平凡的任务，训练网络在模拟DVS摄像机的视野中保持一个无序移动的光点。通过所描述的SNN成功地解决了这个RL问题可以被认为是有利于我的方法效率的证据。
<details>	<summary>英文摘要</summary>	At present, implementation of learning mechanisms in spiking neural networks (SNN) cannot be considered as a solved scientific problem despite plenty of SNN learning algorithms proposed. It is also true for SNN implementation of reinforcement learning (RL), while RL is especially important for SNNs because of its close relationship to the domains most promising from the viewpoint of SNN application such as robotics. In the present paper, I describe an SNN structure which, seemingly, can be used in wide range of RL tasks. The distinctive feature of my approach is usage of only the spike forms of all signals involved - sensory input streams, output signals sent to actuators and reward/punishment signals. Besides that, selecting the neuron/plasticity models, I was guided by the requirement that they should be easily implemented on modern neurochips. The SNN structure considered in the paper includes spiking neurons described by a generalization of the LIFAT (leaky integrate-and-fire neuron with adaptive threshold) model and a simple spike timing dependent synaptic plasticity model (a generalization of dopamine-modulated plasticity). My concept is based on very general assumptions about RL task characteristics and has no visible limitations on its applicability. To test it, I selected a simple but non-trivial task of training the network to keep a chaotically moving light spot in the view field of an emulated DVS camera. Successful solution of this RL problem by the SNN described can be considered as evidence in favor of efficiency of my approach. </details>
<details>	<summary>邮件日期</summary>	2022年04月12日</details>

# 406、基于星形胶质细胞神经网络的容错计算设计方法
- [ ] A Design Methodology for Fault-Tolerant Computing using Astrocyte Neural Networks 
时间：2022年04月06日                         第一作者：Murat I\c{s}{\i}k                       [链接](https://arxiv.org/abs/2204.02942).                     
## 摘要：我们提出了一种设计方法来促进深度学习模型的容错性。首先，我们实现了一种多核容错神经形态硬件设计，其中每个神经形态核心中的神经元和突触回路由星形胶质细胞回路包围，星形胶质细胞是大脑中的星形胶质细胞，通过使用闭环逆行反馈信号恢复故障神经元的脉冲放电频率，从而促进自我修复。接下来，我们在深入学习模型中引入星形胶质细胞，以达到所需的硬件故障容忍度。最后，我们使用系统软件将启用星形胶质细胞的模型划分为多个簇，并在所提出的容错神经形态设计上实现它们。我们使用七种深度学习推理模型对这种设计方法进行了评估，结果表明它既节省面积又节省能源。
<details>	<summary>英文摘要</summary>	We propose a design methodology to facilitate fault tolerance of deep learning models. First, we implement a many-core fault-tolerant neuromorphic hardware design, where neuron and synapse circuitries in each neuromorphic core are enclosed with astrocyte circuitries, the star-shaped glial cells of the brain that facilitate self-repair by restoring the spike firing frequency of a failed neuron using a closed-loop retrograde feedback signal. Next, we introduce astrocytes in a deep learning model to achieve the required degree of tolerance to hardware faults. Finally, we use a system software to partition the astrocyte-enabled model into clusters and implement them on the proposed fault-tolerant neuromorphic design. We evaluate this design methodology using seven deep learning inference models and show that it is both area and power efficient. </details>
<details>	<summary>注释</summary>	Accepted at ACM Computing Frontiers, 2022 </details>
<details>	<summary>邮件日期</summary>	2022年04月07日</details>

# 405、基于嵌入式脉冲神经细胞自动机的模块化软机器人集体控制
- [ ] Collective control of modular soft robots via embodied Spiking Neural Cellular Automata 
时间：2022年04月05日                         第一作者：Giorgia Nadizar                       [链接](https://arxiv.org/abs/2204.02099).                     
## 摘要：基于体素的软机器人（VSR）是模块化软机器人的一种形式，由多个可变形立方体（即体素）组成。因此，每个VSR都是简单代理的集合，即体素，它们必须相互配合才能产生整体VSR行为。在这种范式中，集体智能在促成协调的出现方面发挥着关键作用，因为每个体素都是独立控制的，只利用局部感官信息以及从其直接邻居（分布式或集体控制）传递的一些知识。在这项工作中，我们提出了一种受神经细胞自动机（NCA）影响并基于仿生脉冲神经网络的新型集体控制形式：体现脉冲NCA（SNCA）。我们对SNCA的不同变体进行了实验，发现它们在运动任务方面与最先进的分布式控制器具有竞争力。此外，我们的研究结果表明，相对于基线，在对不可预见的环境变化的适应性方面有显著改善，这可能是VSR物理实用性的一个决定因素。
<details>	<summary>英文摘要</summary>	Voxel-based Soft Robots (VSRs) are a form of modular soft robots, composed of several deformable cubes, i.e., voxels. Each VSR is thus an ensemble of simple agents, namely the voxels, which must cooperate to give rise to the overall VSR behavior. Within this paradigm, collective intelligence plays a key role in enabling the emerge of coordination, as each voxel is independently controlled, exploiting only the local sensory information together with some knowledge passed from its direct neighbors (distributed or collective control). In this work, we propose a novel form of collective control, influenced by Neural Cellular Automata (NCA) and based on the bio-inspired Spiking Neural Networks: the embodied Spiking NCA (SNCA). We experiment with different variants of SNCA, and find them to be competitive with the state-of-the-art distributed controllers for the task of locomotion. In addition, our findings show significant improvement with respect to the baseline in terms of adaptability to unforeseen environmental changes, which could be a determining factor for physical practicability of VSRs. </details>
<details>	<summary>注释</summary>	Workshop on "From Cells to Societies: Collective Learning across Scales" at the International Conference on Learning Representations (Cells2Societies@ICLR) </details>
<details>	<summary>邮件日期</summary>	2022年04月06日</details>

# 404、前向信号传播学习
- [ ] Forward Signal Propagation Learning 
时间：2022年04月04日                         第一作者：Adam Kohan                       [链接](https://arxiv.org/abs/2204.01723).                     
## 摘要：我们提出了一种新的学习算法，用于通过前向传递传播学习信号和更新神经网络参数，作为反向传播的替代方法。在前向信号传播学习（sigprop）中，只有用于学习和推理的前向路径，因此不存在对学习的额外结构或计算约束，例如反馈连接性、权重传输或反向传递。Sigprop仅通过正向路径实现全局监督学习。这是分层或模块并行训练的理想选择。在生物学中，这解释了没有反馈连接的神经元如何仍能接收到全局学习信号。在硬件方面，这提供了一种无需反向连接的全局监督学习方法。与反向传播和其他放松学习约束的方法相比，Sigprop在设计上与大脑和硬件中的学习模型具有更好的兼容性。我们还证明了sigprop在时间和内存方面比它们更有效。为了进一步解释sigprop的行为，我们提供了证据，证明sigprop在反向传播的上下文中提供了有用的学习信号。为了进一步支持与生物和硬件学习的相关性，我们使用sigprop来训练具有Hebbian更新的连续时间神经网络，以及训练没有替代函数的脉冲神经网络。
<details>	<summary>英文摘要</summary>	We propose a new learning algorithm for propagating a learning signal and updating neural network parameters via a forward pass, as an alternative to backpropagation. In forward signal propagation learning (sigprop), there is only the forward path for learning and inference, so there are no additional structural or computational constraints on learning, such as feedback connectivity, weight transport, or a backward pass, which exist under backpropagation. Sigprop enables global supervised learning with only a forward path. This is ideal for parallel training of layers or modules. In biology, this explains how neurons without feedback connections can still receive a global learning signal. In hardware, this provides an approach for global supervised learning without backward connectivity. Sigprop by design has better compatibility with models of learning in the brain and in hardware than backpropagation and alternative approaches to relaxing learning constraints. We also demonstrate that sigprop is more efficient in time and memory than they are. To further explain the behavior of sigprop, we provide evidence that sigprop provides useful learning signals in context to backpropagation. To further support relevance to biological and hardware learning, we use sigprop to train continuous time neural networks with Hebbian updates and train spiking neural networks without surrogate functions. </details>
<details>	<summary>邮件日期</summary>	2022年04月06日</details>

# 403、用活动正则化优化脉冲神经网络的消耗
- [ ] Optimizing the Consumption of Spiking Neural Networks with Activity Regularization 
时间：2022年04月04日                         第一作者：Simon Narduzzi                       [链接](https://arxiv.org/abs/2204.01460).                     
## 摘要：对于在边缘设备上运行的神经网络模型来说，降低能耗是一个关键点。在这方面，减少在边缘硬件加速器上运行的深度神经网络（DNN）的乘法累加（MAC）操作次数将减少推理过程中的能耗。脉冲神经网络（SNN）是一种仿生技术，可以通过使用二进制激活进一步节约能源，避免在不脉冲时消耗能源。通过DNN到SNN的转换框架，可以对网络进行配置，使其在任务上具有同等的准确性，但它们的转换基于速率编码，因此突触操作可能很高。在这项工作中，我们研究了在神经网络激活图上实施稀疏性的不同技术，并比较了不同训练正则化器对优化的DNN和SNN效率的影响。
<details>	<summary>英文摘要</summary>	Reducing energy consumption is a critical point for neural network models running on edge devices. In this regard, reducing the number of multiply-accumulate (MAC) operations of Deep Neural Networks (DNNs) running on edge hardware accelerators will reduce the energy consumption during inference. Spiking Neural Networks (SNNs) are an example of bio-inspired techniques that can further save energy by using binary activations, and avoid consuming energy when not spiking. The networks can be configured for equivalent accuracy on a task through DNN-to-SNN conversion frameworks but their conversion is based on rate coding therefore the synaptic operations can be high. In this work, we look into different techniques to enforce sparsity on the neural network activation maps and compare the effect of different training regularizers on the efficiency of the optimized DNNs and SNNs. </details>
<details>	<summary>注释</summary>	5 pages, 3 figures; accepted at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, 2022 </details>
<details>	<summary>邮件日期</summary>	2022年04月05日</details>

# 402、皮层振荡在脉冲神经网络中实现了基于采样的计算
- [ ] Cortical oscillations implement a backbone for sampling-based computation in spiking neural networks 
时间：2022年04月04日                         第一作者：Agnes Korcsak-Gorzo                       [链接](https://arxiv.org/abs/2006.11099).                     
<details>	<summary>注释</summary>	34 pages, 9 figures Journal-ref: PLoS Comput Biol 18(3): e1009753 (2022) DOI: 10.1371/journal.pcbi.1009753 </details>
<details>	<summary>邮件日期</summary>	2022年04月05日</details>

# 401、脉冲相机的光流估计
- [ ] Optical Flow Estimation for Spiking Camera 
时间：2022年04月03日                         第一作者：Liwen Hu                       [链接](https://arxiv.org/abs/2110.03916).                     
<details>	<summary>注释</summary>	The first two authors contributed equally. Accepted to CVPR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年04月05日</details>

# 400、脉冲相量神经网络的深度学习
- [ ] Deep Learning in Spiking Phasor Neural Networks 
时间：2022年04月01日                         第一作者：Connor Bybee                        [链接](https://arxiv.org/abs/2204.00507).                     
## 摘要：由于用于低延迟、低功耗的神经形态硬件，以及用于理解神经科学的模型，脉冲神经网络（SNN）已经吸引了深度学习社区的注意。本文介绍了脉冲相量神经网络（SPNN）。SPNN基于复数深层神经网络（DNN），通过脉冲时间表示相位。我们的模型采用脉冲计时码进行稳健计算，并且可以使用复数域形成梯度。我们在CIFAR-10上训练了SPNN，并证明其性能超过了其他定时编码的SNN，接近可比实值DNN的结果。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have attracted the attention of the deep learning community for use in low-latency, low-power neuromorphic hardware, as well as models for understanding neuroscience. In this paper, we introduce Spiking Phasor Neural Networks (SPNNs). SPNNs are based on complex-valued Deep Neural Networks (DNNs), representing phases by spike times. Our model computes robustly employing a spike timing code and gradients can be formed using the complex domain. We train SPNNs on CIFAR-10, and demonstrate that the performance exceeds that of other timing coded SNNs, approaching results with comparable real-valued DNNs. </details>
<details>	<summary>注释</summary>	10 pages, 5 figures, work presented at Intel Neuromorphic Community Fall 2019 workshop in Graz, Austria and the UC Berkeley Center for Computational Biology Retreat 2019 </details>
<details>	<summary>邮件日期</summary>	2022年04月04日</details>

# 399、SIT：一种用于脉冲神经网络的仿生非线性神经元
- [ ] SIT: A Bionic and Non-Linear Neuron for Spiking Neural Network 
时间：2022年04月01日                         第一作者：Cheng Jin                       [链接](https://arxiv.org/abs/2203.16117).                     
<details>	<summary>邮件日期</summary>	2022年04月04日</details>

# 398、神经形态硬件中的时间编码脉冲傅里叶变换
- [ ] Time-coded Spiking Fourier Transform in Neuromorphic Hardware 
时间：2022年03月31日                         第一作者：Javier L\'opez-R                       [链接](https://arxiv.org/abs/2202.12650).                     
<details>	<summary>注释</summary>	Accepted version on IEEE Transactions on Computers (early access). Added copyright notice DOI: 10.1109/TC.2022.3162708 </details>
<details>	<summary>邮件日期</summary>	2022年04月01日</details>

# 397、SIT：一种用于脉冲神经网络的仿生非线性神经元
- [ ] SIT: A Bionic and Non-Linear Neuron for Spiking Neural Network 
时间：2022年03月30日                         第一作者：Cheng Jin                       [链接](https://arxiv.org/abs/2203.16117).                     
## 摘要：脉冲神经网络（SNN）因其处理时间信息的能力和低功耗而引起了研究人员的兴趣。然而，目前最先进的方法限制了它们的生物学合理性和性能，因为它们的神经元通常是建立在简单的泄漏积分和火灾（LIF）模型上的。由于高度的动态复杂性，现代神经元模型很少在SNN实践中实现。在这项研究中，我们采用了相平面分析（PPA）技术，这是一种在神经动力学领域经常使用的技术，来整合一个最近的神经元模型，即Izhikevich神经元。根据神经科学进展中的发现，伊兹克维奇神经元模型在生物学上是合理的，同时保持了与LIF神经元相当的计算成本。通过利用所采用的PPA，我们已经完成了将用改进的Izhikevich模型构建的神经元应用于SNN实践，称为标准化的Izhikevich紧张（SIT）神经元。在性能方面，我们评估了自建LIF和SIT组成SNN中图像分类任务的建议技术，即静态MNIST、时尚MNIST、CIFAR-10数据集和神经形态N-MNIST、CIFAR10-DVS和DVS128手势数据集上的混合神经网络（HNN）。实验结果表明，所提出的方法在几乎所有测试数据集上表现出更真实的生物学行为的同时，达到了相当的准确性，证明了这种新策略在弥合神经动力学和SNN实践之间的差距方面的有效性。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have piqued researchers' interest because of their capacity to process temporal information and low power consumption. However, current state-of-the-art methods limited their biological plausibility and performance because their neurons are generally built on the simple Leaky-Integrate-and-Fire (LIF) model. Due to the high level of dynamic complexity, modern neuron models have seldom been implemented in SNN practice. In this study, we adopt the Phase Plane Analysis (PPA) technique, a technique often utilized in neurodynamics field, to integrate a recent neuron model, namely, the Izhikevich neuron. Based on the findings in the advancement of neuroscience, the Izhikevich neuron model can be biologically plausible while maintaining comparable computational cost with LIF neurons. By utilizing the adopted PPA, we have accomplished putting neurons built with the modified Izhikevich model into SNN practice, dubbed as the Standardized Izhikevich Tonic (SIT) neuron. For performance, we evaluate the suggested technique for image classification tasks in self-built LIF-and-SIT-consisted SNNs, named Hybrid Neural Network (HNN) on static MNIST, Fashion-MNIST, CIFAR-10 datasets and neuromorphic N-MNIST, CIFAR10-DVS, and DVS128 Gesture datasets. The experimental results indicate that the suggested method achieves comparable accuracy while exhibiting more biologically realistic behaviors on nearly all test datasets, demonstrating the efficiency of this novel strategy in bridging the gap between neurodynamics and SNN practice. </details>
<details>	<summary>邮件日期</summary>	2022年03月31日</details>

# 396、基于事件的电位辅助脉冲神经网络视频重建
- [ ] Event-based Video Reconstruction via Potential-assisted Spiking Neural Network 
时间：2022年03月30日                         第一作者：Lin Zhu                       [链接](https://arxiv.org/abs/2201.10943).                     
<details>	<summary>注释</summary>	Accepted by CVPR2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月31日</details>

# 395、神经生物学中的时空模式：未来人工智能综述
- [ ] Spatiotemporal Patterns in Neurobiology: An Overview for Future Artificial Intelligence 
时间：2022年03月29日                         第一作者：Sean Knight                       [链接](https://arxiv.org/abs/2203.15415).                     
## 摘要：近年来，人们对开发模型和工具以解决脑组织中发现的复杂连接模式越来越感兴趣。具体来说，这是因为需要了解这些网络结构在多个时空尺度上如何产生涌现属性。我们认为，计算模型是阐明多尺度时空域上由复杂网络连接的异质神经元相互作用可能产生的功能的关键工具。在这里，我们回顾了几类模型，包括脉冲神经元、具有短期可塑性（STP）的整合和激发神经元、基于电导的具有短期可塑性（STP）的整合和激发模型，以及使用简单示例的群体密度神经场（PDNF）模型，重点介绍了神经科学的应用，同时也为人工智能提供了一些潜在的未来研究方向。这些计算方法使我们能够从实验和理论上探索改变潜在机制对产生的网络功能的影响。因此，我们希望这些研究将为人工智能算法的未来发展提供信息，并帮助验证我们对基于动物或人类实验的大脑过程的理解。
<details>	<summary>英文摘要</summary>	In recent years, there has been increasing interest in developing models and tools to address the complex patterns of connectivity found in brain tissue. Specifically, this is due to a need to understand how emergent properties emerge from these network structures at multiple spatiotemporal scales. We argue that computational models are key tools for elucidating the possible functionalities that can emerge from interactions of heterogeneous neurons connected by complex networks on multi-scale temporal and spatial domains. Here we review several classes of models including spiking neurons, integrate and fire neurons with short term plasticity (STP), conductance based integrate-and-fire models with STP, and population density neural field (PDNF) models using simple examples with emphasis on neuroscience applications while also providing some potential future research directions for AI. These computational approaches allow us to explore the impact of changing underlying mechanisms on resulting network function both experimentally as well as theoretically. Thus we hope these studies will inform future developments in artificial intelligence algorithms as well as help validate our understanding of brain processes based on experiments in animals or humans. </details>
<details>	<summary>注释</summary>	8 pages </details>
<details>	<summary>邮件日期</summary>	2022年03月30日</details>

# 394、具有脉冲神经元的脑激励多层感知器
- [ ] Brain-inspired Multilayer Perceptron with Spiking Neurons 
时间：2022年03月28日                         第一作者：Wenshuo Li                       [链接](https://arxiv.org/abs/2203.14679).                     
## 摘要：近年来，多层感知器（MLP）成为计算机视觉领域的研究热点。在没有归纳偏差的情况下，MLP在特征提取方面表现良好，并取得了惊人的效果。然而，由于其结构简单，性能在很大程度上取决于局部特征和通信机制。为了进一步提高MLP的性能，我们引入了来自大脑启发神经网络的信息通信机制。脉冲神经网络（SNN）是最著名的脑激励神经网络，在处理稀疏数据方面取得了巨大的成功。SNN中的泄漏整合与激发（LIF）神经元用于在不同的时间步之间进行通信。在本文中，我们将LIF神经元的机制纳入MLP模型中，以在没有额外失败的情况下获得更好的精度。我们提出了一种全精度的LIF操作来实现面片之间的通信，包括不同方向的水平LIF和垂直LIF。我们还建议使用组LIF来提取更好的局部特征。有了LIF模块，我们的SNN-MLP模型在ImageNet数据集上达到了81.9%、83.3%和83.5%的顶级精度，分别只有4.4G、8.5G和15.2G次，这是我们所知的最先进的结果。
<details>	<summary>英文摘要</summary>	Recently, Multilayer Perceptron (MLP) becomes the hotspot in the field of computer vision tasks. Without inductive bias, MLPs perform well on feature extraction and achieve amazing results. However, due to the simplicity of their structures, the performance highly depends on the local features communication machenism. To further improve the performance of MLP, we introduce information communication mechanisms from brain-inspired neural networks. Spiking Neural Network (SNN) is the most famous brain-inspired neural network, and achieve great success on dealing with sparse data. Leaky Integrate and Fire (LIF) neurons in SNNs are used to communicate between different time steps. In this paper, we incorporate the machanism of LIF neurons into the MLP models, to achieve better accuracy without extra FLOPs. We propose a full-precision LIF operation to communicate between patches, including horizontal LIF and vertical LIF in different directions. We also propose to use group LIF to extract better local features. With LIF modules, our SNN-MLP model achieves 81.9%, 83.3% and 83.5% top-1 accuracy on ImageNet dataset with only 4.4G, 8.5G and 15.2G FLOPs, respectively, which are state-of-the-art results as far as we know. </details>
<details>	<summary>注释</summary>	This paper is accepted by CVPR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月29日</details>

# 393、一种基于神经流形的棘波神经网络用于增强皮质内脑-机接口数据
- [ ] A Spiking Neural Network based on Neural Manifold for Augmenting Intracortical Brain-Computer Interface Data 
时间：2022年03月26日                         第一作者：Shengjie Zheng                       [链接](https://arxiv.org/abs/2204.05132).                     
## 摘要：脑-机接口（BCI），将大脑中的神经信号转换为指令，以控制外部设备。然而，获得足够的训练数据既困难又有限。随着先进的机器学习方法的出现，脑-机接口的能力得到了前所未有的增强，然而，这些方法需要大量的训练数据，因此需要对有限的可用数据进行数据扩充。这里，我们使用脉冲神经网络（SNN）作为数据生成器。它被誉为下一代神经网络，并被认为是面向一般人工智能的算法之一，因为它借用了生物逻辑神经元的神经信息处理。我们使用SNN生成可生物解释的神经棘波信息，并符合原始神经数据中的内在模式。实验表明，该模型可以直接合成新的脉冲序列，从而提高BCI解码器的泛化能力。脉冲神经模型的输入和输出都是脉冲信息，这是一种大脑启发的智能方法，将来可以更好地与BCI集成。
<details>	<summary>英文摘要</summary>	Brain-computer interfaces (BCIs), transform neural signals in the brain into in-structions to control external devices. However, obtaining sufficient training data is difficult as well as limited. With the advent of advanced machine learning methods, the capability of brain-computer interfaces has been enhanced like never before, however, these methods require a large amount of data for training and thus require data augmentation of the limited data available. Here, we use spiking neural networks (SNN) as data generators. It is touted as the next-generation neu-ral network and is considered as one of the algorithms oriented to general artifi-cial intelligence because it borrows the neural information processing from bio-logical neurons. We use the SNN to generate neural spike information that is bio-interpretable and conforms to the intrinsic patterns in the original neural data. Ex-periments show that the model can directly synthesize new spike trains, which in turn improves the generalization ability of the BCI decoder. Both the input and output of the spiking neural model are spike information, which is a brain-inspired intelligence approach that can be better integrated with BCI in the future. </details>
<details>	<summary>注释</summary>	12pages , 9 figures </details>
<details>	<summary>邮件日期</summary>	2022年04月12日</details>

# 392、用人工神经网络发现生理神经元霍奇金-赫胥黎模型的动力学特征
- [ ] Discovering dynamical features of Hodgkin-Huxley-type model of physiological neuron using artificial neural network 
时间：2022年03月26日                         第一作者：Pavel V. Kuptsov                       [链接](https://arxiv.org/abs/2203.14138).                     
## 摘要：我们考虑Hodgkin-Huxley型模型，它是一个具有两个快变量和一个慢变量的刚性常微分方程系统。对于所考虑的参数范围，模型的原始版本具有不稳定不动点和振荡吸引子，该吸引子表现出从爆破到脉冲动力学的分叉。此外，还考虑了一种改进的情况，即双稳态发生时，参数空间中出现了一个不动点变得稳定并与爆破吸引子共存的区域。对于这两个系统，我们创建了能够重现其动态的人工神经网络。所创建的网络作为循环映射运行，并根据在一定范围内随机参数值采样的轨迹切割进行训练。虽然网络只在振荡轨迹切割上训练，但它也能发现所考虑系统的不动点。其位置甚至特征值都与初始常微分方程的不动点非常吻合。对于双稳态模型，这意味着只在一个解决方案的早午餐上训练的网络恢复另一个早午餐，而在训练期间看不到它。在我们看来，这些结果能够触发复杂动力学重建和发现新方法的发展。从实用的角度来看，用神经网络再现动力学可以被认为是一种用于当代并行硬件和软件的数值建模的替代方法。
<details>	<summary>英文摘要</summary>	We consider Hodgkin-Huxley-type model that is a stiff ODE system with two fast and one slow variables. For the parameter ranges under consideration the original version of the model has unstable fixed point and the oscillating attractor that demonstrates bifurcation from bursting to spiking dynamics. Also a modified version is considered where the bistability occurs such that an area in the parameter space appears where the fixed point becomes stable and coexists with the bursting attractor. For these two systems we create artificial neural networks that are able to reproduce their dynamics. The created networks operate as recurrent maps and are trained on trajectory cuts sampled at random parameter values within a certain range. Although the networks are trained only on oscillatory trajectory cuts, it also discover the fixed point of the considered systems. The position and even the eigenvalues coincide very well with the fixed point of the initial ODEs. For the bistable model it means that the network being trained only on one brunch of the solutions recovers another brunch without seeing it during the training. These results, as we see it, are able to trigger the development of new approaches to complex dynamics reconstruction and discovering. From the practical point of view reproducing dynamics with the neural network can be considered as a sort of alternative method of numerical modeling intended for use with contemporary parallel hard- and software. </details>
<details>	<summary>注释</summary>	17 pages, 12 figures, 2 tables </details>
<details>	<summary>邮件日期</summary>	2022年03月29日</details>

# 391、脉冲神经流二进制算法
- [ ] Spiking Neural Streaming Binary Arithmetic 
时间：2022年03月23日                         第一作者：James B. Aimone                       [链接](https://arxiv.org/abs/2203.12662).                     
## 摘要：布尔函数和二进制算术运算是标准计算范式的核心。因此，计算领域的许多进展都集中在如何提高这些操作的效率，以及探索它们可以计算什么。为了充分利用新计算范式的优势，重要的是考虑它们提供了哪些独特的计算方法。然而，对于任何特殊用途的协处理器，布尔函数和二进制算术运算非常有用，尤其是通过在设备上预处理和后处理数据来避免不必要的I/O打开和关闭协处理器。这尤其适用于脉冲神经形态结构，其中这些基本操作不是基本的低级操作。相反，这些功能需要具体实现。在这里，我们讨论了一种有利的流式二进制编码方法以及一些设计用于精确计算基本布尔运算和二进制运算的电路的含义。
<details>	<summary>英文摘要</summary>	Boolean functions and binary arithmetic operations are central to standard computing paradigms. Accordingly, many advances in computing have focused upon how to make these operations more efficient as well as exploring what they can compute. To best leverage the advantages of novel computing paradigms it is important to consider what unique computing approaches they offer. However, for any special-purpose co-processor, Boolean functions and binary arithmetic operations are useful for, among other things, avoiding unnecessary I/O on-and-off the co-processor by pre- and post-processing data on-device. This is especially true for spiking neuromorphic architectures where these basic operations are not fundamental low-level operations. Instead, these functions require specific implementation. Here we discuss the implications of an advantageous streaming binary encoding method as well as a handful of circuits designed to exactly compute elementary Boolean and binary operations. </details>
<details>	<summary>注释</summary>	Accepted and presented at the 2021 International Conference on Rebooting Computing (ICRC) Report-no: SAND2021-13472 C </details>
<details>	<summary>邮件日期</summary>	2022年03月25日</details>

# 390、稀疏主动卷积脉冲神经网络的高效硬件加速
- [ ] Efficient Hardware Acceleration of Sparsely Active Convolutional Spiking Neural Networks 
时间：2022年03月23日                         第一作者：Jan Sommer                       [链接](https://arxiv.org/abs/2203.12437).                     
## 摘要：脉冲神经网络（SNN）在基于事件的情况下进行计算，以实现比标准神经网络更高效的计算。在SNN中，神经元输出（即激活）不是用实值激活编码的，而是用二元脉冲序列编码的。与传统神经网络相比，使用SNN的动机植根于SNN的特殊计算方面，尤其是神经输出激活的高度稀疏性。传统卷积神经网络（CNN）结构成熟，其特点是处理单元（PE）的大型空间阵列在激活稀疏性面前仍然高度未充分利用。我们提出了一种新的架构，该架构针对具有高度激活稀疏性的卷积SNN（CSNN）的处理进行了优化。在我们的体系结构中，主要策略是使用较少但利用率较高的PEs。用于执行卷积的PE阵列仅与内核大小一样大，只要有峰值需要处理，所有PE都可以处于活动状态。通过将特征映射（即激活）压缩到队列中，然后逐峰处理，可以确保这种恒定的峰值流。这种压缩是在运行时使用专用电路执行的，从而实现了自定时调度。这允许处理时间与峰值数量直接成比例。采用一种称为记忆交错的新型记忆组织方案，使用多个小型并行片上ram高效地存储和检索单个神经元的膜电位。每个RAM都通过硬接线连接到其PE，减少了开关电路，并允许RAM位于各自PE附近。我们在FPGA上实现了所提出的架构，与其他实现相比，实现了显著的加速，同时需要更少的硬件资源并保持较低的能耗。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) compute in an event-based matter to achieve a more efficient computation than standard Neural Networks. In SNNs, neuronal outputs (i.e. activations) are not encoded with real-valued activations but with sequences of binary spikes. The motivation of using SNNs over conventional neural networks is rooted in the special computational aspects of SNNs, especially the very high degree of sparsity of neural output activations. Well established architectures for conventional Convolutional Neural Networks (CNNs) feature large spatial arrays of Processing Elements (PEs) that remain highly underutilized in the face of activation sparsity. We propose a novel architecture that is optimized for the processing of Convolutional SNNs (CSNNs) that feature a high degree of activation sparsity. In our architecture, the main strategy is to use less but highly utilized PEs. The PE array used to perform the convolution is only as large as the kernel size, allowing all PEs to be active as long as there are spikes to process. This constant flow of spikes is ensured by compressing the feature maps (i.e. the activations) into queues that can then be processed spike by spike. This compression is performed in run-time using dedicated circuitry, leading to a self-timed scheduling. This allows the processing time to scale directly with the number of spikes. A novel memory organization scheme called memory interlacing is used to efficiently store and retrieve the membrane potentials of the individual neurons using multiple small parallel on-chip RAMs. Each RAM is hardwired to its PE, reducing switching circuitry and allowing RAMs to be located in close proximity to the respective PE. We implemented the proposed architecture on an FPGA and achieved a significant speedup compared to other implementations while needing less hardware resources and maintaining a lower energy consumption. </details>
<details>	<summary>注释</summary>	12 pages, 12 figures, 5 tables, submitted to CODES 2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月24日</details>

# 389、有机对数域整合突触
- [ ] Organic log-domain integrator synapse 
时间：2022年03月23日                         第一作者：Mohammad Javad Mirshojaeian Hosseini                       [链接](https://arxiv.org/abs/2203.12552).                     
## 摘要：突触在记忆、学习和认知中起着关键作用。它们的主要功能包括将突触前的电压脉冲转化为突触后的电流，以及缩放输入信号。有人提出了几种以大脑为灵感的结构来模拟生物突触的行为。虽然这些研究有助于探索神经系统的特性，但制造具有生物相容性和柔性电路的挑战仍然存在，这些电路具有生物上合理的时间常数和可调增益。这里展示了一种物理上灵活的有机对数域积分突触电路来应对这一挑战。特别是，该电路使用电活性有机材料制造，提供灵活性和生物相容性，以及生物学上合理的时间常数（学习神经代码和编码时空模式的关键）。使用10NF突触电容器，弯曲前和弯曲期间的时间常数分别达到126 ms和221 ms。在弯曲之前和弯曲期间对柔性突触回路进行了表征，然后研究了加权电压、突触电容和突触前信号的差异对时间常数的影响。
<details>	<summary>英文摘要</summary>	Synapses play a critical role in memory, learning, and cognition. Their main functions include converting pre-synaptic voltage spikes to post-synaptic currents, as well as scaling the input signal. Several brain-inspired architectures have been proposed to emulate the behavior of biological synapses. While these are useful to explore the properties of nervous systems, the challenge of making biocompatible and flexible circuits with biologically plausible time constants and tunable gain remains. Here, a physically flexible organic log-domain integrator synaptic circuit is shown to address this challenge. In particular, the circuit is fabricated using organic-based materials that are electrically active, offer flexibility and biocompatibility, as well as time constants (critical in learning neural codes and encoding spatiotemporal patterns) that are biologically plausible. Using a 10 nF synaptic capacitor, the time constant reached 126 ms and 221 ms before and during bending, respectively. The flexible synaptic circuit is characterized before and during bending, followed by studies on the effects of weighting voltage, synaptic capacitance, and disparity in pre-synaptic signals on the time constant. </details>
<details>	<summary>注释</summary>	Accepted by Advanced Electronic Materials (18 pages, 17 figures) DOI: 10.1002/aelm.202100724 </details>
<details>	<summary>邮件日期</summary>	2022年03月24日</details>

# 388、电压依赖性突触可塑性（VDSP）：基于神经元膜电位的无监督概率Hebbian可塑性规则
- [ ] Voltage-Dependent Synaptic Plasticity (VDSP): Unsupervised probabilistic Hebbian plasticity rule based on neurons membrane potential 
时间：2022年03月21日                         第一作者：Nikhil Garg                       [链接](https://arxiv.org/abs/2203.11022).                     
## 摘要：本研究提出了电压依赖性突触可塑性（VDSP），这是一种新的大脑启发的无监督局部学习规则，用于在神经形态硬件上在线实现Hebb的可塑性机制。建议的VDSP学习规则仅更新突触后神经元棘突上的突触电导，这将标准的棘突时间依赖性可塑性（STDP）更新次数减少两倍。这种更新依赖于突触前神经元的膜电位，该电位作为神经元实现的一部分随时可用，因此不需要额外的存储内存。此外，这种更新还可以调节突触重量，防止重复刺激时重量爆炸或消失。通过严格的数学分析得出VDSP和STDP之间的等价关系。为了验证VDSP的系统级性能，我们训练了一个用于手写数字识别的单层脉冲神经网络（SNN）。我们报告了MNIST数据集上100个输出神经元网络的准确度为85.01$\pm$0.76%（平均$\pm$S.D.）。当扩展网络大小时（400个输出神经元为89.93$\pm$0.41%，500个神经元为90.56$\pm$0.27），性能得到改善，这验证了所提出的学习规则适用于大规模计算机视觉任务。有趣的是，学习规则比STDP更好地适应输入信号的频率，并且不需要手动调整超参数。
<details>	<summary>英文摘要</summary>	This study proposes voltage-dependent-synaptic plasticity (VDSP), a novel brain-inspired unsupervised local learning rule for the online implementation of Hebb's plasticity mechanism on neuromorphic hardware. The proposed VDSP learning rule updates the synaptic conductance on the spike of the postsynaptic neuron only, which reduces by a factor of two the number of updates with respect to standard spike-timing-dependent plasticity (STDP). This update is dependent on the membrane potential of the presynaptic neuron, which is readily available as part of neuron implementation and hence does not require additional memory for storage. Moreover, the update is also regularized on synaptic weight and prevents explosion or vanishing of weights on repeated stimulation. Rigorous mathematical analysis is performed to draw an equivalence between VDSP and STDP. To validate the system-level performance of VDSP, we train a single-layer spiking neural network (SNN) for the recognition of handwritten digits. We report 85.01 $ \pm $ 0.76% (Mean $ \pm $ S.D.) accuracy for a network of 100 output neurons on the MNIST dataset. The performance improves when scaling the network size (89.93 $ \pm $ 0.41% for 400 output neurons, 90.56 $ \pm $ 0.27 for 500 neurons), which validates the applicability of the proposed learning rule for large-scale computer vision tasks. Interestingly, the learning rule better adapts than STDP to the frequency of input signal and does not require hand-tuning of hyperparameters. </details>
<details>	<summary>邮件日期</summary>	2022年03月22日</details>

# 387、一种基于加速神经形态硬件的可扩展建模方法
- [ ] A Scalable Approach to Modeling on Accelerated Neuromorphic Hardware 
时间：2022年03月21日                         第一作者：Eric M\"uller                       [链接](https://arxiv.org/abs/2203.11102).                     
## 摘要：神经形态系统为扩大计算研究的探索空间提供了机会。然而，将效率和可用性结合起来往往是一个挑战。这项工作介绍了BrainScaleS-2系统的软件方面，这是一种基于物理建模的混合加速神经形态硬件架构。我们将介绍BrainScaleS-2操作系统的关键方面：实验工作流、API分层、软件设计和平台操作。我们展示用例来讨论和导出软件的需求，并展示实现。重点在于新的系统和软件功能，如多隔间神经元、硬件在环训练的快速重新配置、嵌入式处理器的应用、非脉冲操作模式、交互式平台访问和可持续的硬件/软件协同开发。最后，我们讨论了硬件扩展、系统可用性和效率方面的进一步发展。
<details>	<summary>英文摘要</summary>	Neuromorphic systems open up opportunities to enlarge the explorative space for computational research. However, it is often challenging to unite efficiency and usability. This work presents the software aspects of this endeavor for the BrainScaleS-2 system, a hybrid accelerated neuromorphic hardware architecture based on physical modeling. We introduce key aspects of the BrainScaleS-2 Operating System: experiment workflow, API layering, software design, and platform operation. We present use cases to discuss and derive requirements for the software and showcase the implementation. The focus lies on novel system and software features such as multi-compartmental neurons, fast re-configuration for hardware-in-the-loop training, applications for the embedded processors, the non-spiking operation mode, interactive platform access, and sustainable hardware/software co-development. Finally, we discuss further developments in terms of hardware scale-up, system usability and efficiency. </details>
<details>	<summary>邮件日期</summary>	2022年03月22日</details>

# 386、脉冲神经网络的最新进展和新前沿
- [ ] Recent Advances and New Frontiers in Spiking Neural Networks 
时间：2022年03月12日                         第一作者：Duzhen Zhang                       [链接](https://arxiv.org/abs/2204.07050).                     
## 摘要：近年来，脉冲神经网络（spiking neural networks，SNN）由于其丰富的时空动力学、各种编码方案以及与神经形态硬件自然匹配的事件驱动特性，在脑启发智能领域受到了广泛关注。随着SNN的发展，脑启发智能（brain-inspired intelligence，brain-inspired intelligence，简称brain-inspired intelligence，简称SNN）这一以人工通用智能为目标的新兴研究领域正变得越来越热门。在本文中，我们回顾了SNN的最新进展，并从四个主要研究主题讨论了SNN的新前沿，包括基本元素（即脉冲神经元模型、编码方法和拓扑结构）、数据集、优化算法以及软硬件框架。我们希望我们的调查能够帮助研究人员更好地理解SNN，并激发新的工作来推进这一领域。
<details>	<summary>英文摘要</summary>	In recent years, spiking neural networks (SNNs) have received extensive attention in the field of brain-inspired intelligence due to their rich spatially-temporal dynamics, various coding schemes, and event-driven characteristics that naturally fit the neuromorphic hardware. With the development of SNNs, brain-inspired intelligence, an emerging research field inspired by brain science achievements and aiming at artificial general intelligence, is becoming hot. In this paper, we review the recent advances and discuss the new frontiers in SNNs from four major research topics, including essential elements (i.e., spiking neuron models, encoding methods, and topology structures), datasets, optimization algorithms, and software and hardware frameworks. We hope our survey can help researchers understand SNNs better and inspire new works to advance this field. </details>
<details>	<summary>注释</summary>	Under review </details>
<details>	<summary>邮件日期</summary>	2022年04月15日</details>

# 385、基于地球移动距离的暹罗脉冲神经网络的监督训练
- [ ] Supervised Training of Siamese Spiking Neural Networks with Earth's Mover Distance 
时间：2022年02月20日                         第一作者：Mateusz Pabian                       [链接](https://arxiv.org/abs/2203.13207).                     
## 摘要：本研究将高度通用的暹罗神经网络模型应用于事件数据域。我们引入了一个有监督的训练框架，用于利用脉冲神经网络（SNN）优化脉冲序列之间的地震动距离（EMD）。我们在MNIST数据集转换为脉冲域的图像上用新的转换方案训练该模型。通过测量不同数据集编码类型的分类器性能，评估输入图像的暹罗嵌入质量。该模型取得了与现有基于SNN的方法类似的性能（F1得分高达0.9386），同时仅使用约15%的隐层神经元对每个示例进行分类。此外，没有使用稀疏神经代码的模型比稀疏的模型慢约45%。这些特性使该模型适用于低能耗和低预测延迟应用。
<details>	<summary>英文摘要</summary>	This study adapts the highly-versatile siamese neural network model to the event data domain. We introduce a supervised training framework for optimizing Earth's Mover Distance (EMD) between spike trains with spiking neural networks (SNN). We train this model on images of the MNIST dataset converted into spiking domain with novel conversion schemes. The quality of the siamese embeddings of input images was evaluated by measuring the classifier performance for different dataset coding types. The models achieved performance similar to existing SNN-based approaches (F1-score of up to 0.9386) while using only about 15% of hidden layer neurons to classify each example. Furthermore, models which did not employ a sparse neural code were about 45% slower than their sparse counterparts. These properties make the model suitable for low energy consumption and low prediction latency applications. </details>
<details>	<summary>注释</summary>	Manuscript accepted for presentation at 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) </details>
<details>	<summary>邮件日期</summary>	2022年03月25日</details>

# 384、具有时空压缩和突触卷积阻滞的超低潜伏期脉冲神经网络
- [ ] Ultra-low Latency Spiking Neural Networks with Spatio-Temporal Compression and Synaptic Convolutional Block 
时间：2022年03月18日                         第一作者：Changqing Xu                       [链接](https://arxiv.org/abs/2203.10006).                     
## 摘要：脉冲神经网络（Spiking neural networks，SNN）是一种受大脑启发的模型，具有时空信息处理能力强、功耗低、生物合理性高等特点。有效的时空特征使其适用于事件流分类。然而，神经形态数据集，如N-MNIST、CIFAR10-DVS、DVS128手势，需要将单个事件聚合为具有更高时间分辨率的帧，用于事件流分类，这会导致较高的训练和推理延迟。在这项工作中，我们提出了一种时空压缩方法，将单个事件聚合为突触电流的几个时间步，以减少训练和推理延迟。为了在高压缩比下保持SNN的准确性，我们还提出了一种突触卷积块来平衡相邻时间步之间的剧烈变化。并引入了具有可学习膜时间常数的多阈值漏积分与点火（LIF），以提高其信息处理能力。我们在神经形态N-MNIST、CIFAR10-DVS、DVS128手势数据集上评估了所提出的事件流分类任务方法。实验结果表明，我们提出的方法在几乎所有数据集上都优于最先进的精度，使用的时间步长更少。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs), as one of the brain-inspired models, has spatio-temporal information processing capability, low power feature, and high biological plausibility. The effective spatio-temporal feature makes it suitable for event streams classification. However, neuromorphic datasets, such as N-MNIST, CIFAR10-DVS, DVS128-gesture, need to aggregate individual events into frames with a new higher temporal resolution for event stream classification, which causes high training and inference latency. In this work, we proposed a spatio-temporal compression method to aggregate individual events into a few time steps of synaptic current to reduce the training and inference latency. To keep the accuracy of SNNs under high compression ratios, we also proposed a synaptic convolutional block to balance the dramatic change between adjacent time steps. And multi-threshold Leaky Integrate-and-Fire (LIF) with learnable membrane time constant is introduced to increase its information processing capability. We evaluate the proposed method for event streams classification tasks on neuromorphic N-MNIST, CIFAR10-DVS, DVS128 gesture datasets. The experiment results show that our proposed method outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time steps. </details>
<details>	<summary>邮件日期</summary>	2022年03月21日</details>

# 383、使用推文和视频创建多媒体摘要
- [ ] Creating Multimedia Summaries Using Tweets and Videos 
时间：2022年03月16日                         第一作者：Anietie Andy                        [链接](https://arxiv.org/abs/2203.08931).                     
## 摘要：当总统辩论或电视节目等热门电视节目播出时，人们会实时提供评论。在本文中，我们提出了一种简单而有效的方法，将社交媒体评论和视频结合起来，创建电视事件的多媒体摘要。我们的方法基于事件中涉及人员的大量提及来识别这些事件中的场景，并自动从视频中选择推文和帧，这些视频发生在讨论和显示被讨论人员的时间段。
<details>	<summary>英文摘要</summary>	While popular televised events such as presidential debates or TV shows are airing, people provide commentary on them in real-time. In this paper, we propose a simple yet effective approach to combine social media commentary and videos to create a multimedia summary of televised events. Our approach identifies scenes from these events based on spikes of mentions of people involved in the event and automatically selects tweets and frames from the videos that occur during the time period of the spike that talk about and show the people being discussed. </details>
<details>	<summary>注释</summary>	8 pages, 3 figures, 7 tables </details>
<details>	<summary>邮件日期</summary>	2022年03月18日</details>

# 382、快速精确递归神经网络的脉冲激励秩编码
- [ ] Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural Networks 
时间：2022年03月16日                         第一作者：Alan Jeffares                       [链接](https://arxiv.org/abs/2110.02865).                     
<details>	<summary>注释</summary>	Spotlight paper at ICLR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月17日</details>

# 381、Skydiver：利用时空工作负载平衡的脉冲神经网络加速器
- [ ] Skydiver: A Spiking Neural Network Accelerator Exploiting Spatio-Temporal Workload Balance 
时间：2022年03月14日                         第一作者：Qinyu Chen                       [链接](https://arxiv.org/abs/2203.07516).                     
## 摘要：脉冲神经网络（SNN）是人工神经网络（ANN）的一种很有前途的替代方法，因为它具有更真实的大脑启发计算模型。SNN具有随时间变化的稀疏神经元放电，即时空稀疏性；因此，它们有助于实现节能硬件推断。然而，利用硬件中SNN的时空稀疏性会导致不可预测和不平衡的工作负载，降低能效。在这项工作中，我们提出了一种基于FPGA的卷积SNN加速器Skydiver，它利用了时空负载平衡。我们提出了一种近似比例关系构造（APRC）方法和一种通道平衡工作负载调度（CBWS）方法，以将硬件工作负载平衡率提高到90%以上。Skydiver在Xilinx XC7Z045 FPGA上实现，并在图像分割和MNIST分类任务上进行了验证。结果表明，这两项任务的吞吐量分别提高了1.4倍和1.2倍。Skydiver在分类任务中获得了22.6 KFPS的吞吐量和42.4 uJ/图像预测能量，准确率为98.5%。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) are developed as a promising alternative to Artificial Neural networks (ANNs) due to their more realistic brain-inspired computing models. SNNs have sparse neuron firing over time, i.e., spatio-temporal sparsity; thus, they are useful to enable energy-efficient hardware inference. However, exploiting spatio-temporal sparsity of SNNs in hardware leads to unpredictable and unbalanced workloads, degrading the energy efficiency. In this work, we propose an FPGA-based convolutional SNN accelerator called Skydiver that exploits spatio-temporal workload balance. We propose the Approximate Proportional Relation Construction (APRC) method that can predict the relative workload channel-wisely and a Channel-Balanced Workload Schedule (CBWS) method to increase the hardware workload balance ratio to over 90%. Skydiver was implemented on a Xilinx XC7Z045 FPGA and verified on image segmentation and MNIST classification tasks. Results show improved throughput by 1.4X and 1.2X for the two tasks. Skydiver achieved 22.6 KFPS throughput, and 42.4 uJ/Image prediction energy on the classification task with 98.5% accuracy. </details>
<details>	<summary>注释</summary>	Accepted to be published in the IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 2022 DOI: 10.1109/TCAD.2022.3158834 </details>
<details>	<summary>邮件日期</summary>	2022年03月16日</details>

# 380、脉冲神经网络集成电路：趋势和未来方向综述
- [ ] Spiking Neural Network Integrated Circuits: A Review of Trends and Future Directions 
时间：2022年03月14日                         第一作者：Arindam Basu                       [链接](https://arxiv.org/abs/2203.07006).                     
## 摘要：本文回顾了脉冲神经网络（SNN）集成电路的设计，分析了混合信号核、全数字核和大规模多核设计的发展趋势。最近报道的SNN集成电路分为三大类：（a）具有专用于脉冲路由的NOC的大规模多核设计，（b）数字单核设计和（c）混合信号单核设计。最后，我们完成了论文，并对未来的研究方向进行了展望。
<details>	<summary>英文摘要</summary>	In this paper, we reviewed Spiking neural network (SNN) integrated circuit designs and analyzed the trends among mixed-signal cores, fully digital cores and large-scale, multi-core designs. Recently reported SNN integrated circuits are compared under three broad categories: (a) Large-scale multi-core designs that have dedicated NOC for spike routing, (b) digital single-core designs and (c) mixed-signal single-core designs. Finally, we finish the paper with some directions for future progress. </details>
<details>	<summary>邮件日期</summary>	2022年03月15日</details>

# 379、神经体系结构寻找脉冲神经网络
- [ ] Neural Architecture Search for Spiking Neural Networks 
时间：2022年03月12日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2201.10355).                     
<details>	<summary>邮件日期</summary>	2022年03月15日</details>

# 378、SoftSNN：软错误下脉冲神经网络加速器的低成本容错
- [ ] SoftSNN: Low-Cost Fault Tolerance for Spiking Neural Network Accelerators under Soft Errors 
时间：2022年03月12日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2203.05523).                     
<details>	<summary>注释</summary>	To appear at the 59th IEEE/ACM Design Automation Conference (DAC), July 2022, San Francisco, CA, USA </details>
<details>	<summary>邮件日期</summary>	2022年03月15日</details>

# 377、SNN中的整体可塑性和网络适应性
- [ ] Ensemble plasticity and network adaptability in SNNs 
时间：2022年03月11日                         第一作者：Mahima Milinda Alwis Weerasinghe                       [链接](https://arxiv.org/abs/2203.07039).                     
## 摘要：由于基于离散事件（即脉冲）的计算，人工脉冲神经网络（ASNN）有望提高信息处理效率。一些机器学习（ML）应用程序使用生物启发的可塑性机制作为无监督学习技术，以提高ASNN的鲁棒性，同时保持效率。脉冲时间依赖性可塑性（STDP）和内在可塑性（IP）（即动态脉冲阈值适应）是两种这样的机制，它们被结合起来形成了一种集成学习方法。然而，目前尚不清楚这种整体学习应该如何基于扣球活动进行调节。此外，之前的研究已经尝试在STDP后进行基于阈值的突触修剪，以提高ASNN的推理效率，同时牺牲ASNN的性能。然而，这种类型的结构适应，采用个体的重量机制，不考虑脉冲活动修剪，这是一个更好的表示输入刺激。我们设想，基于可塑性的穗调控和基于穗的修剪将导致ASSN在低资源情况下表现更好。本文介绍了一种基于熵和网络激活的集成学习方法，该方法与专门使用脉冲活动的脉冲率神经元修剪技术相结合。使用两个脑电图（EEG）数据集作为分类实验的输入，使用一次通过学习训练的三层前馈ASNN。在学习过程中，我们观察到神经元根据脉冲率聚集成一系列簇。研究发现，修剪低棘波率神经元簇会导致泛化程度的增加或可预测的性能下降。
<details>	<summary>英文摘要</summary>	Artificial Spiking Neural Networks (ASNNs) promise greater information processing efficiency because of discrete event-based (i.e., spike) computation. Several Machine Learning (ML) applications use biologically inspired plasticity mechanisms as unsupervised learning techniques to increase the robustness of ASNNs while preserving efficiency. Spike Time Dependent Plasticity (STDP) and Intrinsic Plasticity (IP) (i.e., dynamic spiking threshold adaptation) are two such mechanisms that have been combined to form an ensemble learning method. However, it is not clear how this ensemble learning should be regulated based on spiking activity. Moreover, previous studies have attempted threshold based synaptic pruning following STDP, to increase inference efficiency at the cost of performance in ASNNs. However, this type of structural adaptation, that employs individual weight mechanisms, does not consider spiking activity for pruning which is a better representation of input stimuli. We envisaged that plasticity-based spike-regulation and spike-based pruning will result in ASSNs that perform better in low resource situations. In this paper, a novel ensemble learning method based on entropy and network activation is introduced, which is amalgamated with a spike-rate neuron pruning technique, operated exclusively using spiking activity. Two electroencephalography (EEG) datasets are used as the input for classification experiments with a three-layer feed forward ASNN trained using one-pass learning. During the learning process, we observed neurons assembling into a hierarchy of clusters based on spiking rate. It was discovered that pruning lower spike-rate neuron clusters resulted in increased generalization or a predictable decline in performance. </details>
<details>	<summary>注释</summary>	19 pages, 12 figures ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2022年03月15日</details>

# 376、细胞自动机可以通过诱导轨迹相位共存来对数据进行分类
- [ ] Cellular automata can classify data by inducing trajectory phase coexistence 
时间：2022年03月10日                         第一作者：Stephen Whitelam                       [链接](https://arxiv.org/abs/2203.05551).                     
## 摘要：我们证明了细胞自动机可以通过诱导一种形式的动态相位共存来对数据进行分类。我们使用蒙特卡罗方法来搜索一般的二维确定性自动机，该自动机根据活动、从图像开始的轨迹中发生的状态变化数量对图像进行分类。当自动机的深度是一个可训练的参数时，搜索方案会根据初始条件识别自动机，自动机生成一组动态轨迹，显示高或低活动。这种性质的自动机表现为非线性激活函数，其输出实际上是二进制的，类似于脉冲神经元的出现版本。我们的工作将机器学习和水库计算与概念上类似于磁铁和眼镜等物理系统的现象联系起来。
<details>	<summary>英文摘要</summary>	We show that cellular automata can classify data by inducing a form of dynamical phase coexistence. We use Monte Carlo methods to search for general two-dimensional deterministic automata that classify images on the basis of activity, the number of state changes that occur in a trajectory initiated from the image. When the depth of the automaton is a trainable parameter, the search scheme identifies automata that generate a population of dynamical trajectories displaying high or low activity, depending on initial conditions. Automata of this nature behave as nonlinear activation functions with an output that is effectively binary, resembling an emergent version of a spiking neuron. Our work connects machine learning and reservoir computing to phenomena conceptually similar to those seen in physical systems such as magnets and glasses. </details>
<details>	<summary>邮件日期</summary>	2022年03月11日</details>

# 375、SoftSNN：软错误下脉冲神经网络加速器的低成本容错
- [ ] SoftSNN: Low-Cost Fault Tolerance for Spiking Neural Network Accelerators under Soft Errors 
时间：2022年03月10日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2203.05523).                     
## 摘要：专门的硬件加速器被设计和使用，以最大限度地提高脉冲神经网络（SNN）的性能效率。然而，这种加速器容易受到瞬态故障（即软错误）的影响，这些故障是由高能粒子撞击引起的，并在硬件层表现为位翻转。这些错误可能会改变SNN加速器计算引擎中的权重值和神经元操作，从而导致不正确的输出和精度下降。然而，对于SNN，计算引擎中软错误的影响以及相应的缓解技术尚未得到彻底研究。一个潜在的解决方案是使用冗余执行（重新执行）来确保正确的输出，但它会导致巨大的延迟和能源开销。为此，我们提出了SoftSNN，这是一种新的方法，可以在不重新执行的情况下减轻SNN加速器的权重寄存器（突触）和神经元中的软错误，从而在低延迟和能量开销的情况下保持准确性。我们的软SNN方法采用了以下关键步骤：（1）分析软错误下的SNN特征，以识别错误权重和神经元操作，这是识别错误SNN行为所必需的；（2） 一种限制和保护技术，利用这种分析，通过限制权重值和保护神经元免受错误操作，提高SNN的容错能力；（3）为神经硬件加速器设计轻量级硬件增强，以有效支持所提出的技术。实验结果表明，对于高故障率的900个神经元网络，我们的SoftSNN保持了3%以下的精度下降，同时与重新执行技术相比，延迟和能量分别减少了3倍和2.3倍。
<details>	<summary>英文摘要</summary>	Specialized hardware accelerators have been designed and employed to maximize the performance efficiency of Spiking Neural Networks (SNNs). However, such accelerators are vulnerable to transient faults (i.e., soft errors), which occur due to high-energy particle strikes, and manifest as bit flips at the hardware layer. These errors can change the weight values and neuron operations in the compute engine of SNN accelerators, thereby leading to incorrect outputs and accuracy degradation. However, the impact of soft errors in the compute engine and the respective mitigation techniques have not been thoroughly studied yet for SNNs. A potential solution is employing redundant executions (re-execution) for ensuring correct outputs, but it leads to huge latency and energy overheads. Toward this, we propose SoftSNN, a novel methodology to mitigate soft errors in the weight registers (synapses) and neurons of SNN accelerators without re-execution, thereby maintaining the accuracy with low latency and energy overheads. Our SoftSNN methodology employs the following key steps: (1) analyzing the SNN characteristics under soft errors to identify faulty weights and neuron operations, which are required for recognizing faulty SNN behavior; (2) a Bound-and-Protect technique that leverages this analysis to improve the SNN fault tolerance by bounding the weight values and protecting the neurons from faulty operations; and (3) devising lightweight hardware enhancements for the neural hardware accelerator to efficiently support the proposed technique. The experimental results show that, for a 900-neuron network with even a high fault rate, our SoftSNN maintains the accuracy degradation below 3%, while reducing latency and energy by up to 3x and 2.3x respectively, as compared to the re-execution technique. </details>
<details>	<summary>注释</summary>	To appear at the 59th IEEE/ACM Design Automation Conference (DAC), July 2022, San Francisco, CA, USA </details>
<details>	<summary>邮件日期</summary>	2022年03月11日</details>

# 374、一种具有无监督学习的全记忆脉冲神经网络
- [ ] A Fully Memristive Spiking Neural Network with Unsupervised Learning 
时间：2022年03月10日                         第一作者：Peng Zhou                       [链接](https://arxiv.org/abs/2203.01416).                     
<details>	<summary>邮件日期</summary>	2022年03月11日</details>

# 373、SPICEprop：通过记忆脉冲神经网络反向传播错误
- [ ] SPICEprop: Backpropagating Errors Through Memristive Spiking Neural Networks 
时间：2022年03月10日                         第一作者：Peng Zhou                       [链接](https://arxiv.org/abs/2203.01426).                     
<details>	<summary>邮件日期</summary>	2022年03月11日</details>

# 372、SPICEprop：通过记忆脉冲神经网络反向传播错误
- [ ] SPICEprop: Backpropagating Errors Through Memristive Spiking Neural Networks 
时间：2022年03月08日                         第一作者：Peng Zhou                       [链接](https://arxiv.org/abs/2203.01426).                     
<details>	<summary>邮件日期</summary>	2022年03月09日</details>

# 371、基于STDP的脉冲神经网络监督学习算法
- [ ] An STDP-Based Supervised Learning Algorithm for Spiking Neural Networks 
时间：2022年03月07日                         第一作者：Zhanhao Hu                       [链接](https://arxiv.org/abs/2203.03379).                     
## 摘要：与基于速率的人工神经网络相比，脉冲神经网络（SNN）为大脑提供了一个更具生物合理性的模型。但他们如何进行监督学习仍然是个谜。受Bengio等人最近工作的启发，我们提出了一种基于脉冲时间依赖可塑性（STDP）的监督学习算法，用于由漏积分和激发（LIF）神经元组成的分层SNN。为突触前神经元设计了一个时间窗口，只有该窗口中的脉冲参与STDP更新过程。模型在MNIST数据集上进行训练。分类精度接近由标准反向传播算法训练的具有类似结构的多层感知器（MLP）。
<details>	<summary>英文摘要</summary>	Compared with rate-based artificial neural networks, Spiking Neural Networks (SNN) provide a more biological plausible model for the brain. But how they perform supervised learning remains elusive. Inspired by recent works of Bengio et al., we propose a supervised learning algorithm based on Spike-Timing Dependent Plasticity (STDP) for a hierarchical SNN consisting of Leaky Integrate-and-fire (LIF) neurons. A time window is designed for the presynaptic neuron and only the spikes in this window take part in the STDP updating process. The model is trained on the MNIST dataset. The classification accuracy approach that of a Multilayer Perceptron (MLP) with similar architecture trained by the standard back-propagation algorithm. </details>
<details>	<summary>邮件日期</summary>	2022年03月08日</details>

# 370、基于事件的电位辅助脉冲神经网络视频重建
- [ ] Event-based Video Reconstruction via Potential-assisted Spiking Neural Network 
时间：2022年03月03日                         第一作者：Lin Zhu                       [链接](https://arxiv.org/abs/2201.10943).                     
<details>	<summary>注释</summary>	Accepted at CVPR2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月07日</details>

# 369、重新思考标准化和剩余块在脉冲神经网络中的作用
- [ ] Rethinking the role of normalization and residual blocks for spiking neural networks 
时间：2022年03月03日                         第一作者：Shin-ichi Ikegawa                       [链接](https://arxiv.org/abs/2203.01544).                     
## 摘要：受生物启发的脉冲神经网络（SNN）被广泛用于实现超低功耗。然而，由于隐藏层中的脉冲神经元过度放电，深层SNN不容易训练。为了解决这个问题，我们提出了一种新颖但简单的标准化技术，称为突触后电位标准化。这种归一化从标准归一化中删除减法项，并使用第二个原始矩而不是方差作为除法项。通过对突触后电位进行简单的标准化，可以控制棘波放电，使训练能够进行分配。实验结果表明，使用我们的归一化处理的SNN优于使用其他归一化处理的其他模型。此外，通过预激活剩余块，该模型可以训练超过100层，而无需其他专用于SNN的特殊技术。
<details>	<summary>英文摘要</summary>	Biologically inspired spiking neural networks (SNNs) are widely used to realize ultralow-power energy consumption. However, deep SNNs are not easy to train due to the excessive firing of spiking neurons in the hidden layers. To tackle this problem, we propose a novel but simple normalization technique called postsynaptic potential normalization. This normalization removes the subtraction term from the standard normalization and uses the second raw moment instead of the variance as the division term. The spike firing can be controlled, enabling the training to proceed appropriating, by conducting this simple normalization to the postsynaptic potential. The experimental results show that SNNs with our normalization outperformed other models using other normalizations. Furthermore, through the pre-activation residual blocks, the proposed model can train with more than 100 layers without other special techniques dedicated to SNNs. </details>
<details>	<summary>注释</summary>	14 pages, 9 figures, 3 tables </details>
<details>	<summary>邮件日期</summary>	2022年03月04日</details>

# 368、用于噪声图像识别的随机量子神经网络
- [ ] Random Quantum Neural Networks (RQNN) for Noisy Image Recognition 
时间：2022年03月03日                         第一作者：Debanjan Konar                       [链接](https://arxiv.org/abs/2203.01764).                     
## 摘要：经典的随机神经网络（RNN）在决策、信号处理和图像识别任务中得到了有效的应用。然而，它们的实现仅限于确定性数字系统，这些系统输出概率分布来代替随机脉冲信号的随机行为。我们引入了一类新的有监督随机量子神经网络（RQNN），该网络具有鲁棒性训练策略，可以更好地利用脉冲RNN的随机性。受量子信息理论和大脑神经元信息编码的时空随机脉冲特性的启发，提出的RQNN采用了具有叠加态和振幅编码特征的混合经典量子算法。我们已经广泛验证了我们提出的RQNN模型，通过PennyLane量子模拟器使用有限数量的\emph{qubits}依赖混合经典量子算法。在MNIST、FashionMNIST和KMNIST数据集上的实验表明，所提出的RQNN模型的平均分类准确率为94.9\%。此外，实验结果表明，与经典神经网络（RNN）、经典脉冲神经网络（SNN）和经典卷积神经网络（AlexNet）相比，所提出的RQNN在噪声环境下的有效性和弹性，以及增强的图像分类精度。此外，RQNN可以处理噪声，这对各种应用非常有用，包括NISQ设备中的计算机视觉。PyTorch密码(https://github.com/darthsimpus/RQN)可在GitHub上获取，以复制本手稿中报告的结果。
<details>	<summary>英文摘要</summary>	Classical Random Neural Networks (RNNs) have demonstrated effective applications in decision making, signal processing, and image recognition tasks. However, their implementation has been limited to deterministic digital systems that output probability distributions in lieu of stochastic behaviors of random spiking signals. We introduce the novel class of supervised Random Quantum Neural Networks (RQNNs) with a robust training strategy to better exploit the random nature of the spiking RNN. The proposed RQNN employs hybrid classical-quantum algorithms with superposition state and amplitude encoding features, inspired by quantum information theory and the brain's spatial-temporal stochastic spiking property of neuron information encoding. We have extensively validated our proposed RQNN model, relying on hybrid classical-quantum algorithms via the PennyLane Quantum simulator with a limited number of \emph{qubits}. Experiments on the MNIST, FashionMNIST, and KMNIST datasets demonstrate that the proposed RQNN model achieves an average classification accuracy of $94.9\%$. Additionally, the experimental findings illustrate the proposed RQNN's effectiveness and resilience in noisy settings, with enhanced image classification accuracy when compared to the classical counterparts (RNNs), classical Spiking Neural Networks (SNNs), and the classical convolutional neural network (AlexNet). Furthermore, the RQNN can deal with noise, which is useful for various applications, including computer vision in NISQ devices. The PyTorch code (https://github.com/darthsimpus/RQN) is made available on GitHub to reproduce the results reported in this manuscript. </details>
<details>	<summary>注释</summary>	This article is submitted to Nature Machine Intelligence journal for review and possible publications </details>
<details>	<summary>邮件日期</summary>	2022年03月04日</details>

# 367、一种具有无监督学习的全记忆脉冲神经网络
- [ ] A Fully Memristive Spiking Neural Network with Unsupervised Learning 
时间：2022年03月02日                         第一作者：Peng Zhou                       [链接](https://arxiv.org/abs/2203.01416).                     
## 摘要：我们提出了一个由物理可实现的记忆神经元和记忆突触组成的全记忆脉冲神经网络（MSNN），以实现无监督的脉冲时间依赖性可塑性（STDP）学习规则。这个系统是完全记忆的，因为神经元和突触的动力学都可以通过记忆器来实现。该神经元采用SPICE级记忆积分和激发（MIF）模型实现，该模型由实现不同去极化、超极化和复极电压波形所需的最少电路元件组成。所提出的MSNN独特地实现了STDP学习，它利用记忆性突触中的累积权重变化来实现记忆性突触的学习。记忆性突触中的电压波形变化源于训练过程中突触前和突触后的脉冲电压信号。研究了两种MSNN结构：1）生物学上合理的记忆检索系统，2）多类分类系统。我们的电路仿真结果通过复制生物记忆检索机制验证了MSNN的无监督学习效率，并在大规模判别式MSNN的4模式识别问题中实现了97.5%的准确率。
<details>	<summary>英文摘要</summary>	We present a fully memristive spiking neural network (MSNN) consisting of physically-realizable memristive neurons and memristive synapses to implement an unsupervised Spiking Time Dependent Plasticity (STDP) learning rule. The system is fully memristive in that both neuronal and synaptic dynamics can be realized by using memristors. The neuron is implemented using the SPICE-level memristive integrate-and-fire (MIF) model, which consists of a minimal number of circuit elements necessary to achieve distinct depolarization, hyperpolarization, and repolarization voltage waveforms. The proposed MSNN uniquely implements STDP learning by using cumulative weight changes in memristive synapses from the voltage waveform changes across the synapses, which arise from the presynaptic and postsynaptic spiking voltage signals during the training process. Two types of MSNN architectures are investigated: 1) a biologically plausible memory retrieval system, and 2) a multi-class classification system. Our circuit simulation results verify the MSNN's unsupervised learning efficacy by replicating biological memory retrieval mechanisms, and achieving 97.5% accuracy in a 4-pattern recognition problem in a large scale discriminative MSNN. </details>
<details>	<summary>邮件日期</summary>	2022年03月04日</details>

# 366、SPICEprop：通过记忆脉冲神经网络反向传播错误
- [ ] SPICEprop: Backpropagating Errors Through Memristive Spiking Neural Networks 
时间：2022年03月02日                         第一作者：Peng Zhou                       [链接](https://arxiv.org/abs/2203.01426).                     
## 摘要：我们提出了一种完全忆阻性脉冲神经网络（MSNN），由使用时间反向传播（BPTT）学习规则训练的新型忆阻神经元组成。梯度下降直接应用于记忆集成与激发（MIF）神经元，该神经元使用模拟SPICE电路模型设计，产生明显的去极化、超极化和复极电压波形。突触重量由BPTT利用MIF神经元模型的膜电位进行训练，并可在记忆交叉杆上进行处理。MIF神经元模型的自然脉冲动态和完全可微性，消除了脉冲神经网络文献中普遍存在的梯度近似的需要。尽管直接在SPICE电路模型上进行训练会增加复杂性，但我们在MNIST测试数据集和时装MNIST测试数据集上的准确率分别达到97.58%和75.26%，是所有完全MSNN中准确率最高的。
<details>	<summary>英文摘要</summary>	We present a fully memristive spiking neural network (MSNN) consisting of novel memristive neurons trained using the backpropagation through time (BPTT) learning rule. Gradient descent is applied directly to the memristive integrated-and-fire (MIF) neuron designed using analog SPICE circuit models, which generates distinct depolarization, hyperpolarization, and repolarization voltage waveforms. Synaptic weights are trained by BPTT using the membrane potential of the MIF neuron model and can be processed on memristive crossbars. The natural spiking dynamics of the MIF neuron model and fully differentiable, eliminating the need for gradient approximations that are prevalent in the spiking neural network literature. Despite the added complexity of training directly on SPICE circuit models, we achieve 97.58% accuracy on the MNIST testing dataset and 75.26% on the Fashion-MNIST testing dataset, the highest accuracies among all fully MSNNs. </details>
<details>	<summary>邮件日期</summary>	2022年03月04日</details>

# 365、重新思考预培训作为从ANN到SNN的桥梁
- [ ] Rethinking Pretraining as a Bridge from ANNs to SNNs 
时间：2022年03月02日                         第一作者：Yihan Lin                       [链接](https://arxiv.org/abs/2203.01158).                     
## 摘要：脉冲神经网络（Spiking neural networks，SNN）是一种典型的脑激励模型，具有丰富的神经元动力学特性、多样的编码方案和低功耗特性。如何获得高精度的模型一直是SNN领域的主要挑战。目前，有两种主流方法，即通过将经过良好训练的人工神经网络（ANN）转换为其对应的SNN来获得转换后的SNN，或直接训练SNN。然而，转换后的SNN的推理时间太长，而SNN训练通常非常昂贵且效率低下。在这项工作中，通过结合两种不同训练方法的概念，借助预训练技术和基于BP的深度SNN训练机制，提出了一种新的SNN训练范式。我们认为，提出的范例是训练SNN的更有效途径。管道包括用于静态数据传输任务的管道和用于动态数据传输任务的管道。SOTA结果是在大规模事件驱动数据集ES ImageNet中获得的。对于训练加速，我们使用ImageNet-1K上的1/10训练时间和ES ImageNet上的2/5训练时间，实现了与类似LIF SNN相同（或更高）的最佳精度，并为新数据集ES-UCF101提供了时间精度基准。这些实验结果揭示了ANN和SNN之间参数函数的相似性，也展示了该SNN训练管道的各种潜在应用。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are known as a typical kind of brain-inspired models with their unique features of rich neuronal dynamics, diverse coding schemes and low power consumption properties. How to obtain a high-accuracy model has always been the main challenge in the field of SNN. Currently, there are two mainstream methods, i.e., obtaining a converted SNN through converting a well-trained Artificial Neural Network (ANN) to its SNN counterpart or training an SNN directly. However, the inference time of a converted SNN is too long, while SNN training is generally very costly and inefficient. In this work, a new SNN training paradigm is proposed by combining the concepts of the two different training methods with the help of the pretrain technique and BP-based deep SNN training mechanism. We believe that the proposed paradigm is a more efficient pipeline for training SNNs. The pipeline includes pipeS for static data transfer tasks and pipeD for dynamic data transfer tasks. SOTA results are obtained in a large-scale event-driven dataset ES-ImageNet. For training acceleration, we achieve the same (or higher) best accuracy as similar LIF-SNNs using 1/10 training time on ImageNet-1K and 2/5 training time on ES-ImageNet and also provide a time-accuracy benchmark for a new dataset ES-UCF101. These experimental results reveal the similarity of the functions of parameters between ANNs and SNNs and also demonstrate the various potential applications of this SNN training pipeline. </details>
<details>	<summary>注释</summary>	8 pages, 4 figures </details>
<details>	<summary>邮件日期</summary>	2022年03月03日</details>

# 364、利用基于时间的神经元提高脉冲神经网络的精度
- [ ] Improving Spiking Neural Network Accuracy Using Time-based Neurons 
时间：2022年03月02日                         第一作者：Hanseok Kim                       [链接](https://arxiv.org/abs/2201.01394).                     
<details>	<summary>注释</summary>	Accepted in ISCAS 2022 </details>
<details>	<summary>邮件日期</summary>	2022年03月03日</details>

# 363、神经形态硬件中的时间编码脉冲傅里叶变换
- [ ] Time-coded Spiking Fourier Transform in Neuromorphic Hardware 
时间：2022年02月25日                         第一作者：Javier L\'opez-R                       [链接](https://arxiv.org/abs/2202.12650).                     
## 摘要：经过几十年的不断优化计算系统，摩尔定律正在走向终结。然而，人们对快速高效的处理系统的需求越来越大，这些系统可以处理大量数据流，同时减少系统占用。神经形态计算通过创建随时间推移与二进制事件通信的分散架构来满足这一需求。尽管在过去几年中快速增长，但需要新的算法来利用这种新兴计算范式的潜力，并刺激高级神经形态芯片的设计。在这项工作中，我们提出了一种基于时间的脉冲神经网络，它在数学上等价于傅里叶变换。我们在神经形态芯片Loihi中实现了该网络，并用汽车调频连续波雷达在五种不同的实际场景中进行了实验。实验结果验证了该算法的有效性，我们希望它们能促进adhoc神经形态芯片的设计，从而提高最先进的数字信号处理器的效率，并鼓励神经形态计算用于信号处理的研究。
<details>	<summary>英文摘要</summary>	After several decades of continuously optimizing computing systems, the Moore's law is reaching itsend. However, there is an increasing demand for fast and efficient processing systems that can handlelarge streams of data while decreasing system footprints. Neuromorphic computing answers thisneed by creating decentralized architectures that communicate with binary events over time. Despiteits rapid growth in the last few years, novel algorithms are needed that can leverage the potential ofthis emerging computing paradigm and can stimulate the design of advanced neuromorphic chips.In this work, we propose a time-based spiking neural network that is mathematically equivalent tothe Fourier transform. We implemented the network in the neuromorphic chip Loihi and conductedexperiments on five different real scenarios with an automotive frequency modulated continuouswave radar. Experimental results validate the algorithm, and we hope they prompt the design of adhoc neuromorphic chips that can improve the efficiency of state-of-the-art digital signal processorsand encourage research on neuromorphic computing for signal processing. </details>
<details>	<summary>注释</summary>	Submitted to IEEE Transactions on Computers. Revised version </details>
<details>	<summary>邮件日期</summary>	2022年02月28日</details>

# 362、生物纠错码产生容错神经网络
- [ ] Biological error correction codes generate fault-tolerant neural networks 
时间：2022年02月25日                         第一作者：Alex                       [链接](https://arxiv.org/abs/2202.12887).                     
## 摘要：在深度学习中，容错计算是否可行一直是一个悬而未决的问题：仅使用不可靠的神经元能否实现任意可靠的计算？在哺乳动物的大脑皮层中，人们观察到被称为网格码的模拟纠错码可以保护状态免受神经脉冲噪声的影响，但它们在信息处理中的作用尚不清楚。在这里，我们使用这些生物代码来表明，如果每个神经元的不完美性低于一个尖锐的阈值，则可以实现一个通用的容错神经网络，我们发现该阈值在数量级上与生物神经元中观察到的噪声相一致。从故障到容错神经计算的急剧相变的发现为理解人工智能和神经科学中的噪声模拟系统开辟了一条道路。
<details>	<summary>英文摘要</summary>	It has been an open question in deep learning if fault-tolerant computation is possible: can arbitrarily reliable computation be achieved using only unreliable neurons? In the mammalian cortex, analog error correction codes known as grid codes have been observed to protect states against neural spiking noise, but their role in information processing is unclear. Here, we use these biological codes to show that a universal fault-tolerant neural network can be achieved if the faultiness of each neuron lies below a sharp threshold, which we find coincides in order of magnitude with noise observed in biological neurons. The discovery of a sharp phase transition from faulty to fault-tolerant neural computation opens a path towards understanding noisy analog systems in artificial intelligence and neuroscience. </details>
<details>	<summary>邮件日期</summary>	2022年02月28日</details>

# 361、用脉冲神经网络进化学习强化学习任务
- [ ] Evolving-to-Learn Reinforcement Learning Tasks with Spiking Neural Networks 
时间：2022年02月24日                         第一作者：J. Lu                       [链接](https://arxiv.org/abs/2202.12322).                     
## 摘要：受自然神经系统启发，突触可塑性规则被用于训练具有局部信息的脉冲神经网络，使其适合在神经形态硬件上进行在线学习。然而，当实现这些规则来学习不同的新任务时，它们通常需要在依赖于任务的微调方面进行大量工作。本文旨在通过采用进化算法，为手头的任务进化出合适的突触可塑性规则，使这一过程变得更容易。更具体地说，我们提供了一组不同的局部信号、一组数学算子和一个全局奖励信号，然后笛卡尔遗传规划过程从这些组件中找到一个最优学习规则。使用这种方法，我们找到了成功解决XOR和cart-pole任务的学习规则，并发现了优于文献中基线规则的新学习规则。
<details>	<summary>英文摘要</summary>	Inspired by the natural nervous system, synaptic plasticity rules are applied to train spiking neural networks with local information, making them suitable for online learning on neuromorphic hardware. However, when such rules are implemented to learn different new tasks, they usually require a significant amount of work on task-dependent fine-tuning. This paper aims to make this process easier by employing an evolutionary algorithm that evolves suitable synaptic plasticity rules for the task at hand. More specifically, we provide a set of various local signals, a set of mathematical operators, and a global reward signal, after which a Cartesian genetic programming process finds an optimal learning rule from these components. Using this approach, we find learning rules that successfully solve an XOR and cart-pole task, and discover new learning rules that outperform the baseline rules from literature. </details>
<details>	<summary>邮件日期</summary>	2022年02月28日</details>

# 360、基于梯度重加权的脉冲神经网络时间有效训练
- [ ] Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting 
时间：2022年02月24日                         第一作者：Shikuang Deng                       [链接](https://arxiv.org/abs/2202.11946).                     
## 摘要：近年来，脑激励的脉冲神经元网络（SNN）因其事件驱动和高效节能的特点引起了广泛的研究兴趣。然而，由于深层SNN激活函数的不可微性，很难有效地训练深层SNN，这使得传统人工神经网络（ANN）中通常使用的梯度下降方法失效。虽然采用替代梯度（SG）形式上允许损失的反向传播，但离散脉冲机制实际上将SNN的损失情况与ANN的损失情况区分开来，使替代梯度方法无法达到与ANN相当的精度。在本文中，我们首先分析了为什么当前使用替代梯度的直接训练方法会导致SNN泛化性差。然后，我们引入时间有效训练（TET）方法来补偿梯度下降过程中的动量损失，从而使训练过程收敛到更平坦的极小值，具有更好的泛化性。同时，我们证明了TET提高了SNN的时间可伸缩性，并诱导了一种时间可继承的加速训练。我们的方法在所有报告的主流数据集（包括CIFAR-10/100和ImageNet）上始终优于SOTA。值得注意的是，在DVS-CIFAR10上，我们获得了83$\%%$top-1精度，与现有技术水平相比，提高了10$\%%$。代码可从\url获取{https://github.com/Gus-Lab/temporal_efficient_training}.
<details>	<summary>英文摘要</summary>	Recently, brain-inspired spiking neuron networks (SNNs) have attracted widespread research interest because of their event-driven and energy-efficient characteristics. Still, it is difficult to efficiently train deep SNNs due to the non-differentiability of its activation function, which disables the typically used gradient descent approaches for traditional artificial neural networks (ANNs). Although the adoption of surrogate gradient (SG) formally allows for the back-propagation of losses, the discrete spiking mechanism actually differentiates the loss landscape of SNNs from that of ANNs, failing the surrogate gradient methods to achieve comparable accuracy as for ANNs. In this paper, we first analyze why the current direct training approach with surrogate gradient results in SNNs with poor generalizability. Then we introduce the temporal efficient training (TET) approach to compensate for the loss of momentum in the gradient descent with SG so that the training process can converge into flatter minima with better generalizability. Meanwhile, we demonstrate that TET improves the temporal scalability of SNN and induces a temporal inheritable training for acceleration. Our method consistently outperforms the SOTA on all reported mainstream datasets, including CIFAR-10/100 and ImageNet. Remarkably on DVS-CIFAR10, we obtained 83$\%$ top-1 accuracy, over 10$\%$ improvement compared to existing state of the art. Codes are available at \url{https://github.com/Gus-Lab/temporal_efficient_training}. </details>
<details>	<summary>注释</summary>	Published as a conference paper at ICLR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月25日</details>

# 359、BioLCNet：奖励调制的局部连接脉冲神经网络
- [ ] BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks 
时间：2022年02月24日                         第一作者：Hafez Ghaemi                       [链接](https://arxiv.org/abs/2109.05539).                     
<details>	<summary>注释</summary>	9 pages, 6 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2022年02月25日</details>

# 358、脉冲神经元的自然梯度学习
- [ ] Natural-gradient learning for spiking neurons 
时间：2022年02月23日                         第一作者：Elena Kreutzer                       [链接](https://arxiv.org/abs/2011.11710).                     
<details>	<summary>注释</summary>	Joint senior authorship: Walter M. Senn and Mihai A. Petrovici </details>
<details>	<summary>邮件日期</summary>	2022年02月25日</details>

# 357、用于时空特征学习的脉冲时间相关可塑性网络的新视角
- [ ] A New Look at Spike-Timing-Dependent Plasticity Networks for Spatio-Temporal Feature Learning 
时间：2022年02月22日                         第一作者：Ali Safa                       [链接](https://arxiv.org/abs/2111.00791).                     
<details>	<summary>邮件日期</summary>	2022年02月23日</details>

# 356、早产儿高效节能呼吸异常检测
- [ ] Energy-Efficient Respiratory Anomaly Detection in Premature Newborn Infants 
时间：2022年02月21日                         第一作者：Ankita Paul                       [链接](https://arxiv.org/abs/2202.10570).                     
## 摘要：准确监测早产儿的呼吸频率对于根据需要启动医疗干预至关重要。有线技术可能对患者具有侵入性和侵扰性。我们提出了一种针对早产儿的支持深度学习的可穿戴式监测系统，该系统使用从佩戴在婴儿身上的无创可穿戴式Bellypatch无线采集的信号来预测呼吸停止。我们提出了一个五阶段的设计流程，包括数据收集和标记、特征缩放、带有超参数调整的模型选择、模型训练和验证、模型测试和部署。所使用的模型是一个一维卷积神经网络（1DCNN）结构，具有1个卷积层、1个池层和3个完全连接层，实现了97.15%的精度。为了解决可穿戴处理的能量限制，探索了几种量化技术，并对其性能和能耗进行了分析。我们提出了一种新的基于脉冲神经网络（SNN）的呼吸分类解决方案，可以在事件驱动的神经形态硬件上实现。我们提出了一种将基线1DCNN的模拟操作转换为其峰值等效值的方法。我们使用转换后的SNN的参数进行设计空间探索，以生成具有不同精度和能量足迹的推理解。我们选择了一个解决方案，与基线1DCNN模型相比，该解决方案以18倍的能量实现了93.33%的精度。此外，提出的SNN解决方案实现了类似的精度，但能耗减少了4倍。
<details>	<summary>英文摘要</summary>	Precise monitoring of respiratory rate in premature infants is essential to initiate medical interventions as required. Wired technologies can be invasive and obtrusive to the patients. We propose a Deep Learning enabled wearable monitoring system for premature newborn infants, where respiratory cessation is predicted using signals that are collected wirelessly from a non-invasive wearable Bellypatch put on infant's body. We propose a five-stage design pipeline involving data collection and labeling, feature scaling, model selection with hyperparameter tuning, model training and validation, model testing and deployment. The model used is a 1-D Convolutional Neural Network (1DCNN) architecture with 1 convolutional layer, 1 pooling layer and 3 fully-connected layers, achieving 97.15% accuracy. To address energy limitations of wearable processing, several quantization techniques are explored and their performance and energy consumption are analyzed. We propose a novel Spiking-Neural-Network(SNN) based respiratory classification solution, which can be implemented on event-driven neuromorphic hardware. We propose an approach to convert the analog operations of our baseline 1DCNN to their spiking equivalent. We perform a design-space exploration using the parameters of the converted SNN to generate inference solutions having different accuracy and energy footprints. We select a solution that achieves 93.33% accuracy with 18 times lower energy compared with baseline 1DCNN model. Additionally the proposed SNN solution achieves similar accuracy but with 4 times less energy. </details>
<details>	<summary>邮件日期</summary>	2022年02月23日</details>

# 355、在神经形态结构上实现脉冲神经网络：综述
- [ ] Implementing Spiking Neural Networks on Neuromorphic Architectures: A Review 
时间：2022年02月17日                         第一作者：Phu Khanh Huynh                       [链接](https://arxiv.org/abs/2202.08897).                     
## 摘要：最近，工业界和学术界都提出了几种不同的神经形态系统来执行使用脉冲神经网络（SNN）设计的机器学习应用程序。随着设计和技术前沿的日益复杂，为此类系统编程以接纳和执行机器学习应用程序变得越来越具有挑战性。此外，神经形态系统需要保证实时性能，消耗较低的能量，并提供对逻辑和内存故障的容忍度。因此，显然需要系统软件框架，能够在当前和新兴的神经形态系统上实现机器学习应用，同时解决性能、能量和可靠性问题。在这里，我们将对基于平台的设计和软硬件协同设计提出的此类框架进行全面概述。我们强调了神经形态计算系统软件技术领域未来面临的挑战和机遇。
<details>	<summary>英文摘要</summary>	Recently, both industry and academia have proposed several different neuromorphic systems to execute machine learning applications that are designed using Spiking Neural Networks (SNNs). With the growing complexity on design and technology fronts, programming such systems to admit and execute a machine learning application is becoming increasingly challenging. Additionally, neuromorphic systems are required to guarantee real-time performance, consume lower energy, and provide tolerance to logic and memory failures. Consequently, there is a clear need for system software frameworks that can implement machine learning applications on current and emerging neuromorphic systems, and simultaneously address performance, energy, and reliability. Here, we provide a comprehensive overview of such frameworks proposed for both, platform-based design and hardware-software co-design. We highlight challenges and opportunities that the future holds in the area of system software technology for neuromorphic computing. </details>
<details>	<summary>邮件日期</summary>	2022年02月21日</details>

# 354、通过解决脉冲神经网络中的退化问题来推进深度剩余学习
- [ ] Advancing Deep Residual Learning by Solving the Crux of Degradation in Spiking Neural Networks 
时间：2022年02月17日                         第一作者：Yifan Hu                       [链接](https://arxiv.org/abs/2201.07209).                     
<details>	<summary>注释</summary>	It is an older version of arXiv:2112.08954 and was submitted by mistake </details>
<details>	<summary>邮件日期</summary>	2022年02月18日</details>

# 353、学习探测飞行中的人：一个基于仿生事件的无人机视觉系统
- [ ] Learning to Detect People on the Fly: A Bio-inspired Event-based Visual System for Drones 
时间：2022年02月16日                         第一作者：Ali Safa                       [链接](https://arxiv.org/abs/2202.08023).                     
## 摘要：我们首次证明，配备有脉冲时间依赖性可塑性（STDP）学习功能的生物可启动脉冲神经网络（SNN）可以持续学习使用视网膜启发的、基于事件的相机数据来检测飞行中行走的人。我们的管道工作如下。首先，向卷积SNNSTDP系统显示从飞行无人机捕捉行走的人的短序列事件数据（<2分钟），该系统还从卷积读数（形成半监督系统）接收教师脉冲信号。然后，停止STDP自适应，并根据测试序列评估学习系统。我们进行了一些实验来研究我们系统中关键机制的影响，并将我们的精确回忆性能与使用RGB或基于事件的相机帧的常规训练CNN进行比较。
<details>	<summary>英文摘要</summary>	We demonstrate for the first time that a biologicallyplausible spiking neural network (SNN) equipped with Spike- Timing-Dependent Plasticity (STDP) learning can continuously learn to detect walking people on the fly using retina-inspired, event-based camera data. Our pipeline works as follows. First, a short sequence of event data (< 2 minutes), capturing a walking human from a flying drone, is shown to a convolutional SNNSTDP system which also receives teacher spiking signals from a convolutional readout (forming a semi-supervised system). Then, STDP adaptation is stopped and the learned system is assessed on testing sequences. We conduct several experiments to study the effect of key mechanisms in our system and we compare our precision-recall performance to conventionally-trained CNNs working with either RGB or event-based camera frames. </details>
<details>	<summary>邮件日期</summary>	2022年02月17日</details>

# 352、AutoSNN：走向节能的脉冲神经网络
- [ ] AutoSNN: Towards Energy-Efficient Spiking Neural Networks 
时间：2022年02月16日                         第一作者：Byunggook Na                       [链接](https://arxiv.org/abs/2201.12738).                     
<details>	<summary>邮件日期</summary>	2022年02月17日</details>

# 351、失重脉冲神经网络的时间延迟记忆
- [ ] Memory via Temporal Delays in weightless Spiking Neural Network 
时间：2022年02月15日                         第一作者：Hananel Hazan                       [链接](https://arxiv.org/abs/2202.07132).                     
## 摘要：神经科学界的一个普遍观点是，记忆编码在神经元之间的连接强度中。这种认知导致人工神经网络模型将连接权重作为调节学习的关键变量。在本文中，我们提出了一个失重脉冲神经网络的原型，可以执行一个简单的分类任务。该网络中的记忆存储在神经元之间的时间，而不是连接的强度，并使用Hebbian Spike timing Dependent Plastics（STDP）进行训练，它调节连接的延迟。
<details>	<summary>英文摘要</summary>	A common view in the neuroscience community is that memory is encoded in the connection strength between neurons. This perception led artificial neural network models to focus on connection weights as the key variables to modulate learning. In this paper, we present a prototype for weightless spiking neural networks that can perform a simple classification task. The memory in this network is stored in the timing between neurons, rather than the strength of the connection, and is trained using a Hebbian Spike Timing Dependent Plasticity (STDP), which modulates the delays of the connection. </details>
<details>	<summary>邮件日期</summary>	2022年02月16日</details>

# 350、量化脉冲神经网络中的局部极小值导航
- [ ] Navigating Local Minima in Quantized Spiking Neural Networks 
时间：2022年02月15日                         第一作者：Jason K. Eshraghian                       [链接](https://arxiv.org/abs/2202.07221).                     
## 摘要：脉冲和量化神经网络（NNs）对于高效实现深度学习（DL）算法变得极其重要。然而，由于应用硬阈值时没有梯度信号，这些网络在使用错误反向传播进行训练时面临挑战。克服这一问题的广为接受的技巧是通过使用有偏梯度估计器：替代梯度近似脉冲神经网络（SNN）中的阈值，而直通估计器（STE）完全绕过量化神经网络（QNN）中的阈值。虽然噪声梯度反馈在简单的有监督学习任务中实现了合理的性能，但人们认为，这种噪声增加了在损失环境中寻找最优解的难度，尤其是在优化的后期阶段。通过在训练期间定期提高学习率（LR），我们期望网络能够导航未探索的解决方案空间，否则由于局部极小值、障碍或平坦表面，这些空间将难以到达。本文对应用于量化SNN（QSNN）的余弦退火LR调度与权重无关的自适应矩估计进行了系统评估。我们对这项技术在三个数据集的高精度和4位量化SNN上进行了严格的实证评估，在更复杂的数据集上展示了（接近）最先进的性能。我们的源代码可通过以下链接获得：https://github.com/jeshraghian/QSNNs.
<details>	<summary>英文摘要</summary>	Spiking and Quantized Neural Networks (NNs) are becoming exceedingly important for hyper-efficient implementations of Deep Learning (DL) algorithms. However, these networks face challenges when trained using error backpropagation, due to the absence of gradient signals when applying hard thresholds. The broadly accepted trick to overcoming this is through the use of biased gradient estimators: surrogate gradients which approximate thresholding in Spiking Neural Networks (SNNs), and Straight-Through Estimators (STEs), which completely bypass thresholding in Quantized Neural Networks (QNNs). While noisy gradient feedback has enabled reasonable performance on simple supervised learning tasks, it is thought that such noise increases the difficulty of finding optima in loss landscapes, especially during the later stages of optimization. By periodically boosting the Learning Rate (LR) during training, we expect the network can navigate unexplored solution spaces that would otherwise be difficult to reach due to local minima, barriers, or flat surfaces. This paper presents a systematic evaluation of a cosine-annealed LR schedule coupled with weight-independent adaptive moment estimation as applied to Quantized SNNs (QSNNs). We provide a rigorous empirical evaluation of this technique on high precision and 4-bit quantized SNNs across three datasets, demonstrating (close to) state-of-the-art performance on the more complex datasets. Our source code is available at this link: https://github.com/jeshraghian/QSNNs. </details>
<details>	<summary>邮件日期</summary>	2022年02月16日</details>

# 349、动力系统的递归神经网络：常微分方程、集体运动和水文建模的应用
- [ ] Recurrent Neural Networks for Dynamical Systems: Applications to Ordinary Differential Equations, Collective Motion, and Hydrological Modeling 
时间：2022年02月14日                         第一作者：Yonggi Park                       [链接](https://arxiv.org/abs/2202.07022).                     
## 摘要：求解时空动力系统的经典方法包括统计方法，如自回归积分移动平均法，该方法假定系统先前输出之间存在线性和平稳关系。线性方法的开发和实现相对简单，但它们通常不会捕获数据中的非线性关系。因此，人工神经网络（ANN）在分析和预测动力系统方面受到了研究人员的关注。递归神经网络（RNN）源于前馈神经网络，它利用内部存储器处理可变长度的输入序列。这使得RNN可以应用于寻找时空动力系统中各种问题的解决方案。因此，在本文中，我们利用RNN来处理一些与动力系统相关的特定问题。具体来说，我们分析了应用于三项任务的RNN的性能：重建具有公式误差的系统的正确Lorenz解，重建损坏的集体运动轨迹，以及预测具有脉冲的流量时间序列，代表三个领域，即常微分方程，集体运动和水文建模。我们在每项任务中对RNN进行独特的训练和测试，以证明RNN在重建和预测动力系统动力学方面的广泛适用性。
<details>	<summary>英文摘要</summary>	Classical methods of solving spatiotemporal dynamical systems include statistical approaches such as autoregressive integrated moving average, which assume linear and stationary relationships between systems' previous outputs. Development and implementation of linear methods are relatively simple, but they often do not capture non-linear relationships in the data. Thus, artificial neural networks (ANNs) are receiving attention from researchers in analyzing and forecasting dynamical systems. Recurrent neural networks (RNN), derived from feed-forward ANNs, use internal memory to process variable-length sequences of inputs. This allows RNNs to applicable for finding solutions for a vast variety of problems in spatiotemporal dynamical systems. Thus, in this paper, we utilize RNNs to treat some specific issues associated with dynamical systems. Specifically, we analyze the performance of RNNs applied to three tasks: reconstruction of correct Lorenz solutions for a system with a formulation error, reconstruction of corrupted collective motion trajectories, and forecasting of streamflow time series possessing spikes, representing three fields, namely, ordinary differential equations, collective motion, and hydrological modeling, respectively. We train and test RNNs uniquely in each task to demonstrate the broad applicability of RNNs in reconstruction and forecasting the dynamics of dynamical systems. </details>
<details>	<summary>注释</summary>	15 pages, 9 figures, submitted into "Chaos: An Interdisciplinary Journal of Nonlinear Science" MSC-class: 37M99 ACM-class: I.2.1 </details>
<details>	<summary>邮件日期</summary>	2022年02月16日</details>

# 348、带系统级局部自动增益控制的脉冲耳蜗
- [ ] Spiking Cochlea with System-level Local Automatic Gain Control 
时间：2022年02月14日                         第一作者：Ilya Kiselev                       [链接](https://arxiv.org/abs/2202.06707).                     
## 摘要：由于晶体管失配和模型的复杂性，将局部自动增益控制（AGC）电路纳入硅耳蜗设计一直具有挑战性。为了解决这个问题，我们提出了一种替代的系统级算法，通过测量单个通道的输出脉冲活动，在硅脉冲耳蜗中实现特定于通道的AGC。信道的带通滤波器增益动态地适应输入振幅，以便平均输出脉冲速率保持在定义的范围内。由于这种AGC机制只需要计数和加法运算，因此在未来的设计中可以以较低的硬件成本实现。我们评估了本地AGC算法对输入信号在32 dB输入范围内变化的分类任务的影响。在语音与噪声分类任务中测试了两种接收耳蜗脉冲特征的分类器类型。启用AGC时，逻辑回归分类器的准确度平均提高6%，相对提高40.8%。深度神经网络分类器在AGC情况下表现出类似的改进，与逻辑回归分类器的最佳精度91%相比，其平均精度更高，达到96%。
<details>	<summary>英文摘要</summary>	Including local automatic gain control (AGC) circuitry into a silicon cochlea design has been challenging because of transistor mismatch and model complexity. To address this, we present an alternative system-level algorithm that implements channel-specific AGC in a silicon spiking cochlea by measuring the output spike activity of individual channels. The bandpass filter gain of a channel is adapted dynamically to the input amplitude so that the average output spike rate stays within a defined range. Because this AGC mechanism only needs counting and adding operations, it can be implemented at low hardware cost in a future design. We evaluate the impact of the local AGC algorithm on a classification task where the input signal varies over 32 dB input range. Two classifier types receiving cochlea spike features were tested on a speech versus noise classification task. The logistic regression classifier achieves an average of 6% improvement and 40.8% relative improvement in accuracy when the AGC is enabled. The deep neural network classifier shows a similar improvement for the AGC case and achieves a higher mean accuracy of 96% compared to the best accuracy of 91% from the logistic regression classifier. </details>
<details>	<summary>注释</summary>	Accepted for publication at the IEEE Transactions on Circuits and Systems I - Regular Papers, 2022 DOI: 10.1109/TCSI.2022.3150165 </details>
<details>	<summary>邮件日期</summary>	2022年02月15日</details>

# 347、Motif拓扑和奖励学习改进的脉冲神经网络用于高效的多感官整合
- [ ] Motif-topology and Reward-learning improved Spiking Neural Network for Efficient Multi-sensory Integration 
时间：2022年02月11日                         第一作者：Shuncheng Jia                       [链接](https://arxiv.org/abs/2202.06821).                     
## 摘要：在人工神经网络（ANN）和脉冲神经网络（SNN）中，网络结构和学习原理是形成复杂函数的关键。SNN被认为是新一代人工神经网络，它融合了比ANN更多的生物学特性，包括动态脉冲神经元、功能特定的体系结构和高效的学习范式。在本文中，我们提出了一种母题拓扑和奖励学习改进的SNN（MR-SNN），以实现高效的多感官整合。MR-SNN包含13种类型的3节点基序拓扑，这些基序拓扑首先从独立的单感官学习范式中提取，然后集成到多感官分类中。实验结果表明，与其他不使用基序的传统SNN相比，该MR-SNN具有更高的准确性和更强的鲁棒性。此外，提出的奖赏学习范式在生物学上是合理的，能够更好地解释视觉和听觉感觉信号不一致引起的认知麦格效应。
<details>	<summary>英文摘要</summary>	Network architectures and learning principles are key in forming complex functions in artificial neural networks (ANNs) and spiking neural networks (SNNs). SNNs are considered the new-generation artificial networks by incorporating more biological features than ANNs, including dynamic spiking neurons, functionally specified architectures, and efficient learning paradigms. In this paper, we propose a Motif-topology and Reward-learning improved SNN (MR-SNN) for efficient multi-sensory integration. MR-SNN contains 13 types of 3-node Motif topologies which are first extracted from independent single-sensory learning paradigms and then integrated for multi-sensory classification. The experimental results showed higher accuracy and stronger robustness of the proposed MR-SNN than other conventional SNNs without using Motifs. Furthermore, the proposed reward learning paradigm was biologically plausible and can better explain the cognitive McGurk effect caused by incongruent visual and auditory sensory signals. </details>
<details>	<summary>邮件日期</summary>	2022年02月15日</details>

# 346、基于模拟RRAM的脉冲神经网络中补偿异质性的硬件校准学习
- [ ] Hardware calibrated learning to compensate heterogeneity in analog RRAM-based Spiking Neural Networks 
时间：2022年02月10日                         第一作者：Filippo Moro                       [链接](https://arxiv.org/abs/2202.05094).                     
## 摘要：脉冲神经网络（SNN）可以释放基于模拟电阻随机存取存储器（RRAM）的电路的全部功率，用于低功率信号处理。它们固有的计算稀疏性自然会带来能效效益。实现健壮SNN的主要挑战是模拟CMOS电路和RRAM技术的内在可变性（异质性）。在这项工作中，我们评估了使用130纳米技术节点设计和制造的基于RRAM的神经形态电路的性能和可变性。基于这些结果，我们提出了一种神经形态硬件校准（NHC）SNN，其中学习电路根据测量数据进行校准。我们表明，通过考虑片外学习阶段测量的异质性特征，NHC SNN可以自我校正其硬件非理想性，并学习以高精度解决基准任务。这项工作展示了如何应对神经元和突触的异质性，以提高时间任务中的分类准确性。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) can unleash the full power of analog Resistive Random Access Memories (RRAMs) based circuits for low power signal processing. Their inherent computational sparsity naturally results in energy efficiency benefits. The main challenge implementing robust SNNs is the intrinsic variability (heterogeneity) of both analog CMOS circuits and RRAM technology. In this work, we assessed the performance and variability of RRAM-based neuromorphic circuits that were designed and fabricated using a 130\,nm technology node. Based on these results, we propose a Neuromorphic Hardware Calibrated (NHC) SNN, where the learning circuits are calibrated on the measured data. We show that by taking into account the measured heterogeneity characteristics in the off-chip learning phase, the NHC SNN self-corrects its hardware non-idealities and learns to solve benchmark tasks with high accuracy. This work demonstrates how to cope with the heterogeneity of neurons and synapses for increasing classification accuracy in temporal tasks. </details>
<details>	<summary>注释</summary>	Preprint for ISCAS2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月11日</details>

# 345、通过加权神经元分配实现视觉位置识别的脉冲神经网络
- [ ] Spiking Neural Networks for Visual Place Recognition via Weighted Neuronal Assignments 
时间：2022年02月10日                         第一作者：Somayeh Hussaini                       [链接](https://arxiv.org/abs/2109.06452).                     
<details>	<summary>注释</summary>	8 pages, 6 figures, IEEE Robotics and Automation Letters (RA-L), also accepted to IEEE International Conference on Robotics and Automation (ICRA 2022) Journal-ref: IEEE Robotics and Automation Letters 2022 DOI: 10.1109/LRA.2022.3149030 </details>
<details>	<summary>邮件日期</summary>	2022年02月11日</details>

# 344、T-NGA：学习处理脉冲音频传感器事件的时态网络嫁接算法
- [ ] T-NGA: Temporal Network Grafting Algorithm for Learning to Process Spiking Audio Sensor Events 
时间：2022年02月07日                         第一作者：Shu Wang                       [链接](https://arxiv.org/abs/2202.03204).                     
## 摘要：脉冲硅耳蜗传感器将声音编码为来自不同频率通道的异步脉冲流。由于缺少用于刺激耳蜗的标记训练数据集，因此很难根据这些传感器的输出训练深层神经网络。本文提出了一种称为时间网络嫁接算法（T-NGA）的自监督方法，该方法将一个基于谱图特征预训练的递归网络嫁接到耳蜗事件特征上。T-NGA训练只需要暂时对齐的音频频谱图和事件特征。我们的实验表明，嫁接网络的准确性与使用软件脉冲耳蜗模型中的事件从零开始训练语音识别任务的有监督网络的准确性相似。尽管有脉冲硅耳蜗电路的非理想性，但硅耳蜗脉冲记录的嫁接网络准确度仅比使用N-TIDIGITS18数据集的监督网络准确度低5%左右。T-NGA可以训练网络在没有大的标记峰值数据集的情况下处理峰值音频传感器事件。
<details>	<summary>英文摘要</summary>	Spiking silicon cochlea sensors encode sound as an asynchronous stream of spikes from different frequency channels. The lack of labeled training datasets for spiking cochleas makes it difficult to train deep neural networks on the outputs of these sensors. This work proposes a self-supervised method called Temporal Network Grafting Algorithm (T-NGA), which grafts a recurrent network pretrained on spectrogram features so that the network works with the cochlea event features. T-NGA training requires only temporally aligned audio spectrograms and event features. Our experiments show that the accuracy of the grafted network was similar to the accuracy of a supervised network trained from scratch on a speech recognition task using events from a software spiking cochlea model. Despite the circuit non-idealities of the spiking silicon cochlea, the grafted network accuracy on the silicon cochlea spike recordings was only about 5% lower than the supervised network accuracy using the N-TIDIGITS18 dataset. T-NGA can train networks to process spiking audio sensor events in the absence of large labeled spike datasets. </details>
<details>	<summary>注释</summary>	5 pages, 4 figures; accepted at IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Singapore, 2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月08日</details>

# 343、基于时域神经元的高能效高精度脉冲神经网络推理
- [ ] Energy-Efficient High-Accuracy Spiking Neural Network Inference Using Time-Domain Neurons 
时间：2022年02月04日                         第一作者：Joonghyun Song                       [链接](https://arxiv.org/abs/2202.02015).                     
## 摘要：由于在流行的冯·诺依曼体系结构上实现人工神经网络的局限性，最近的研究提出了基于脉冲神经网络（SNN）的神经形态系统，以降低功耗和计算成本。然而，传统的基于电流镜或运算放大器的模拟电压域集成和触发（I&F）神经元电路会带来严重的问题，例如非线性或高功耗，从而降低SNN的推理精度或能量效率。为了同时实现高能量效率和高精度，本文提出了一种低功耗、高线性度的时域I&F神经元电路。在28nm CMOS工艺中设计和模拟，与传统的基于电流镜的神经元相比，该神经元在MNIST推理上的错误率降低了4.3倍以上。此外，所提出的神经元电路的功耗模拟为每个神经元0.230uW，比现有的电压域神经元低几个数量级。
<details>	<summary>英文摘要</summary>	Due to the limitations of realizing artificial neural networks on prevalent von Neumann architectures, recent studies have presented neuromorphic systems based on spiking neural networks (SNNs) to reduce power and computational cost. However, conventional analog voltage-domain integrate-and-fire (I&F) neuron circuits, based on either current mirrors or op-amps, pose serious issues such as nonlinearity or high power consumption, thereby degrading either inference accuracy or energy efficiency of the SNN. To achieve excellent energy efficiency and high accuracy simultaneously, this paper presents a low-power highly linear time-domain I&F neuron circuit. Designed and simulated in a 28nm CMOS process, the proposed neuron leads to more than 4.3x lower error rate on the MNIST inference over the conventional current-mirror-based neurons. In addition, the power consumed by the proposed neuron circuit is simulated to be 0.230uW per neuron, which is orders of magnitude lower than the existing voltage-domain neurons. </details>
<details>	<summary>邮件日期</summary>	2022年02月07日</details>

# 342、低延迟脉冲神经网络的优化电位初始化
- [ ] Optimized Potential Initialization for Low-latency Spiking Neural Networks 
时间：2022年02月03日                         第一作者：Tong Bu                       [链接](https://arxiv.org/abs/2202.01440).                     
## 摘要：脉冲神经网络（SNN）因其低功耗、生物合理性和对抗性鲁棒性等独特特性而受到高度重视。训练深层SNN最有效的方法是通过ANN到SNN的转换，这在深层网络结构和大规模数据集中产生了最好的性能。然而，在准确性和延迟之间需要权衡。为了获得与原始神经网络一样的高精度，需要较长的模拟时间来匹配脉冲神经元的放电频率和模拟神经元的激活值，这阻碍了SNN的实际应用。在本文中，我们的目标是以极低的延迟（少于32个时间步）实现高性能的转换SNN。首先，我们从理论上分析了ANN到SNN的转换，并表明调整阈值确实起到了与权重标准化类似的作用。我们没有引入以模型容量为代价促进ANN到SNN转换的约束，而是采用了一种更直接的方法，通过优化初始膜电位来减少每层的转换损失。此外，我们证明了膜电位的最佳初始化可以实现预期的无误差ANN到SNN的转换。我们在CIFAR-10、CIFAR-100和ImageNet数据集上评估了我们的算法，并使用更少的时间步长实现了最先进的精度。例如，我们在CIFAR-10上以16个时间步长达到了93.38\%的最高精度。此外，我们的方法还可以应用于其他ANN-SNN转换方法，并在时间步长较小时显著提高性能。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have been attached great importance due to the distinctive properties of low power consumption, biological plausibility, and adversarial robustness. The most effective way to train deep SNNs is through ANN-to-SNN conversion, which have yielded the best performance in deep network structure and large-scale datasets. However, there is a trade-off between accuracy and latency. In order to achieve high precision as original ANNs, a long simulation time is needed to match the firing rate of a spiking neuron with the activation value of an analog neuron, which impedes the practical application of SNN. In this paper, we aim to achieve high-performance converted SNNs with extremely low latency (fewer than 32 time-steps). We start by theoretically analyzing ANN-to-SNN conversion and show that scaling the thresholds does play a similar role as weight normalization. Instead of introducing constraints that facilitate ANN-to-SNN conversion at the cost of model capacity, we applied a more direct way by optimizing the initial membrane potential to reduce the conversion loss in each layer. Besides, we demonstrate that optimal initialization of membrane potentials can implement expected error-free ANN-to-SNN conversion. We evaluate our algorithm on the CIFAR-10, CIFAR-100 and ImageNet datasets and achieve state-of-the-art accuracy, using fewer time-steps. For example, we reach top-1 accuracy of 93.38\% on CIFAR-10 with 16 time-steps. Moreover, our method can be applied to other ANN-SNN conversion methodologies and remarkably promote performance when the time-steps is small. </details>
<details>	<summary>注释</summary>	Accepted by AAAI 2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月04日</details>

# 341、速率编码还是直接编码：哪一种更适合于精确、健壮和节能的脉冲神经网络？
- [ ] Rate Coding or Direct Coding: Which One is Better for Accurate, Robust, and Energy-efficient Spiking Neural Networks? 
时间：2022年01月31日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2202.03133).                     
## 摘要：最近的脉冲神经网络（SNN）工作主要集中在图像分类任务上，因此人们提出了各种编码技术来将图像转换为时间二进制脉冲。其中，速率编码和直接编码被认为是构建实用SNN系统的潜在候选，因为它们在大规模数据集上显示了最先进的性能。尽管使用了这两种编码方案，但人们很少注意以公平的方式比较这两种编码方案。在本文中，我们从三个角度对这两种编码进行了综合分析：准确性、对抗性鲁棒性和能源效率。首先，我们比较了两种编码技术在不同体系结构和数据集下的性能。然后，我们测量了编码技术对两种对抗性攻击方法的鲁棒性。最后，我们在数字硬件平台上比较了两种编码方案的能量效率。我们的结果表明，直接编码可以获得更好的精度，尤其是对于少量的时间步长。相比之下，由于不可微脉冲生成过程，速率编码对对抗性攻击表现出更好的鲁棒性。速率编码也比直接编码产生更高的能量效率，直接编码要求第一层具有多位精度。我们的研究探索了两种编码的特征，这是构建SNN的重要设计考虑因素。该代码可在以下网址获得：https://github.com/Intelligent-Computing-Lab-Yale/Rate-vs-Direct.
<details>	<summary>英文摘要</summary>	Recent Spiking Neural Networks (SNNs) works focus on an image classification task, therefore various coding techniques have been proposed to convert an image into temporal binary spikes. Among them, rate coding and direct coding are regarded as prospective candidates for building a practical SNN system as they show state-of-the-art performance on large-scale datasets. Despite their usage, there is little attention to comparing these two coding schemes in a fair manner. In this paper, we conduct a comprehensive analysis of the two codings from three perspectives: accuracy, adversarial robustness, and energy-efficiency. First, we compare the performance of two coding techniques with various architectures and datasets. Then, we measure the robustness of the coding techniques on two adversarial attack methods. Finally, we compare the energy-efficiency of two coding schemes on a digital hardware platform. Our results show that direct coding can achieve better accuracy especially for a small number of timesteps. In contrast, rate coding shows better robustness to adversarial attacks owing to the non-differentiable spike generation process. Rate coding also yields higher energy-efficiency than direct coding which requires multi-bit precision for the first layer. Our study explores the characteristics of two codings, which is an important design consideration for building SNNs. The code is made available at https://github.com/Intelligent-Computing-Lab-Yale/Rate-vs-Direct. </details>
<details>	<summary>注释</summary>	Accepted to ICASSP2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月08日</details>

# 340、快速精确递归神经网络的脉冲激励秩编码
- [ ] Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural Networks 
时间：2022年01月31日                         第一作者：Alan Jeffares                       [链接](https://arxiv.org/abs/2110.02865).                     
<details>	<summary>注释</summary>	Spotlight paper at ICLR 2022 </details>
<details>	<summary>邮件日期</summary>	2022年02月01日</details>

# 339、AutoSNN：走向节能的脉冲神经网络
- [ ] AutoSNN: Towards Energy-Efficient Spiking Neural Networks 
时间：2022年01月30日                         第一作者：Byunggook Na                       [链接](https://arxiv.org/abs/2201.12738).                     
## 摘要：模拟大脑中信息传输的脉冲神经网络（SNN）可以通过离散和稀疏的脉冲有效地处理时空信息，因此受到了广泛关注。为了提高SNN的准确性和能效，以前的大多数研究都只关注训练方法，很少研究体系结构的影响。我们调查了之前研究中使用的设计选择，从准确性和脉冲数量方面考虑，发现它们并不最适合SNN。为了进一步提高准确性并减少SNN产生的脉冲，我们提出了一种脉冲感知神经结构搜索框架AutoSNN。我们定义了一个搜索空间，它由没有不良设计选择的架构组成。为了实现峰值感知体系结构搜索，我们引入了一种适应度，它同时考虑了峰值的准确性和数量。AutoSNN成功搜索在准确性和能效方面优于手工SNN的SNN架构。我们充分展示了AutoSNN在包括神经形态数据集在内的各种数据集上的有效性。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) that mimic information transmission in the brain can energy-efficiently process spatio-temporal information through discrete and sparse spikes, thereby receiving considerable attention. To improve accuracy and energy efficiency of SNNs, most previous studies have focused solely on training methods, and the effect of architecture has rarely been studied. We investigate the design choices used in the previous studies in terms of the accuracy and number of spikes and figure out that they are not best-suited for SNNs. To further improve the accuracy and reduce the spikes generated by SNNs, we propose a spike-aware neural architecture search framework called AutoSNN. We define a search space consisting of architectures without undesirable design choices. To enable the spike-aware architecture search, we introduce a fitness that considers both the accuracy and number of spikes. AutoSNN successfully searches for SNN architectures that outperform hand-crafted SNNs in accuracy and energy efficiency. We thoroughly demonstrate the effectiveness of AutoSNN on various datasets including neuromorphic datasets. </details>
<details>	<summary>邮件日期</summary>	2022年02月01日</details>

# 338、3D FlowNet：基于事件的3D表示光流估计
- [ ] 3D-FlowNet: Event-based optical flow estimation with 3D representation 
时间：2022年01月28日                         第一作者：Haixin Sun                       [链接](https://arxiv.org/abs/2201.12265).                     
## 摘要：基于事件的摄像头可以跨越基于帧的摄像头的限制，用于重要任务，例如在低照度条件下自动驾驶汽车导航期间的高速运动检测。活动摄像机的高时间分辨率和高动态范围，使它们能够在快速运动和极端光线的情况下工作。然而，传统的计算机视觉方法，如深度神经网络，由于它们是异步和离散的，不能很好地适应处理事件数据。此外，传统的事件数据二维编码表示方法牺牲了时间分辨率。在本文中，我们首先通过将二维编码表示扩展到三维来改进二维编码表示，以更好地保留事件的时间分布。然后，我们提出了3D FlowNet，这是一种新的网络结构，可以根据新的编码方法处理3D输入表示和输出光流估计。采用自监督训练策略来弥补基于事件的摄像机缺乏标记数据集的不足。最后，利用多车辆立体事件摄像机（MVSEC）数据集对所提出的网络进行训练和评估。结果表明，我们的3D FlowNet在训练时间更短的情况下（与Spike FlowNet的100次相比，我们的3D FlowNet的训练时间为30次）优于最先进的训练方法。
<details>	<summary>英文摘要</summary>	Event-based cameras can overpass frame-based cameras limitations for important tasks such as high-speed motion detection during self-driving cars navigation in low illumination conditions. The event cameras' high temporal resolution and high dynamic range, allow them to work in fast motion and extreme light scenarios. However, conventional computer vision methods, such as Deep Neural Networks, are not well adapted to work with event data as they are asynchronous and discrete. Moreover, the traditional 2D-encoding representation methods for event data, sacrifice the time resolution. In this paper, we first improve the 2D-encoding representation by expanding it into three dimensions to better preserve the temporal distribution of the events. We then propose 3D-FlowNet, a novel network architecture that can process the 3D input representation and output optical flow estimations according to the new encoding methods. A self-supervised training strategy is adopted to compensate the lack of labeled datasets for the event-based camera. Finally, the proposed network is trained and evaluated with the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. The results show that our 3D-FlowNet outperforms state-of-the-art approaches with less training epoch (30 compared to 100 of Spike-FlowNet). </details>
<details>	<summary>邮件日期</summary>	2022年01月31日</details>

# 337、基于神经形态跌倒检测和动作识别数据集的常规视觉模型基准测试
- [ ] Benchmarking Conventional Vision Models on Neuromorphic Fall Detection and Action Recognition Dataset 
时间：2022年01月28日                         第一作者：Karthik Sivarama Krishnan                        [链接](https://arxiv.org/abs/2201.12285).                     
## 摘要：近几年来，基于神经形态视觉的传感器越来越受欢迎，因为它们能够以低功耗感知捕捉时空事件。这些传感器比传统摄像机记录事件或峰值，有助于保护被记录对象的隐私。根据像素亮度变化捕获这些事件，并使用时间、位置和像素强度变化信息对输出数据流进行编码。本文提出并测试了神经形态人类行为识别和跌倒检测数据集上微调的常规视觉模型的性能。来自动态视觉传感摄像机的时空事件流被编码成标准序列图像帧。这些视频帧用于基准测试传统的基于深度学习的体系结构。在这种提出的方法中，我们为这种动态视觉传感（DVS）应用微调了最先进的视觉模型，并将这些模型命名为DVS-R2+1D、DVS-CSN、DVS-C2D、DVS SlowFast、DVS-X3D和DVS MViT。通过比较这些模型的性能，我们发现当前最先进的基于MViT的体系结构DVS MViT优于所有其他模型，精确度为0.958，F-1分数为0.958。第二好的是DVS-C2D，精确度为0.916，F-1分数为0.916。第三名和第四名是DVS-R2+1D和DVS慢速，准确度分别为0.875和0.833，F-1得分分别为0.875和0.861。DVS-CSN和DVS-X3D是表现最差的模型，准确度分别为0.708和0.625，F1得分分别为0.722和0.625。
<details>	<summary>英文摘要</summary>	Neuromorphic vision-based sensors are gaining popularity in recent years with their ability to capture Spatio-temporal events with low power sensing. These sensors record events or spikes over traditional cameras which helps in preserving the privacy of the subject being recorded. These events are captured as per-pixel brightness changes and the output data stream is encoded with time, location, and pixel intensity change information. This paper proposes and benchmarks the performance of fine-tuned conventional vision models on neuromorphic human action recognition and fall detection datasets. The Spatio-temporal event streams from the Dynamic Vision Sensing cameras are encoded into a standard sequence image frames. These video frames are used for benchmarking conventional deep learning-based architectures. In this proposed approach, we fine-tuned the state-of-the-art vision models for this Dynamic Vision Sensing (DVS) application and named these models as DVS-R2+1D, DVS-CSN, DVS-C2D, DVS-SlowFast, DVS-X3D, and DVS-MViT. Upon comparing the performance of these models, we see the current state-of-the-art MViT based architecture DVS-MViT outperforms all the other models with an accuracy of 0.958 and an F-1 score of 0.958. The second best is the DVS-C2D with an accuracy of 0.916 and an F-1 score of 0.916. Third and Fourth are DVS-R2+1D and DVS-SlowFast with an accuracy of 0.875 and 0.833 and F-1 score of 0.875 and 0.861 respectively. DVS-CSN and DVS-X3D were the least performing models with an accuracy of 0.708 and 0.625 and an F1 score of 0.722 and 0.625 respectively. </details>
<details>	<summary>注释</summary>	6 Pages, 2 Figures </details>
<details>	<summary>邮件日期</summary>	2022年01月31日</details>

# 336、二值化脉冲神经网络中死亡神经元与稀疏性之间的细线
- [ ] The fine line between dead neurons and sparsity in binarized spiking neural networks 
时间：2022年01月28日                         第一作者：Jason K. Eshraghian                       [链接](https://arxiv.org/abs/2201.11915).                     
## 摘要：脉冲神经网络可以通过在时域中编码信息，或通过在更高精度的隐藏状态中处理离散化量来补偿量化误差。理论上，宽动态范围的状态空间可以将多个二值化输入累积在一起，从而提高单个神经元的表征能力。这可以通过增加放电阈值来实现，但如果阈值过高，稀疏的脉冲活动就会变成无脉冲发射。在本文中，我们建议使用“阈值退火”作为触发阈值的预热方法。我们发现，它可以使棘波在多个层面上传播，否则神经元将停止放电，这样做，尽管使用了二值化权重，但在四个不同的数据集上仍能获得极具竞争力的结果。源代码可在https://github.com/jeshraghian/snn-tha/
<details>	<summary>英文摘要</summary>	Spiking neural networks can compensate for quantization error by encoding information either in the temporal domain, or by processing discretized quantities in hidden states of higher precision. In theory, a wide dynamic range state-space enables multiple binarized inputs to be accumulated together, thus improving the representational capacity of individual neurons. This may be achieved by increasing the firing threshold, but make it too high and sparse spike activity turns into no spike emission. In this paper, we propose the use of `threshold annealing' as a warm-up method for firing thresholds. We show it enables the propagation of spikes across multiple layers where neurons would otherwise cease to fire, and in doing so, achieve highly competitive results on four diverse datasets, despite using binarized weights. Source code is available at https://github.com/jeshraghian/snn-tha/ </details>
<details>	<summary>邮件日期</summary>	2022年01月31日</details>

# 335、具有替代梯度下降的元学习脉冲神经网络
- [ ] Meta-learning Spiking Neural Networks with Surrogate Gradient Descent 
时间：2022年01月26日                         第一作者：Kenneth Stewart                       [链接](https://arxiv.org/abs/2201.10777).                     
## 摘要：在边缘和在线任务执行期间进行适应性“终身”学习是人工智能研究的理想目标。在这方面，实现脉冲神经网络（SNN）的神经形态硬件特别有吸引力，因为它们的实时、基于事件的局部计算范式使它们适合边缘实现和快速学习。然而，作为最先进SNN训练特征的长时间迭代学习与神经形态硬件的物理性质和实时操作不兼容。为了克服这些局限性，在深度学习中越来越多地使用元学习等双层学习。在这项工作中，我们使用替代梯度方法在SNN中演示了基于梯度的元学习，该方法近似于梯度估计的脉冲阈值函数。由于替代梯度可以二次可微，因此可以使用模型不可知元学习（MAML）等有效的二阶梯度元学习方法。我们表明，在基于事件的元数据集上，使用MAML的SNN元训练与使用MAML的传统ANN元训练的性能相匹配或超过。此外，我们还展示了元学习带来的特殊优势：快速学习，无需高精度权重或梯度。我们的研究结果强调了元学习技术如何成为在现实世界问题上部署神经形态学习技术的工具。
<details>	<summary>英文摘要</summary>	Adaptive "life-long" learning at the edge and during online task performance is an aspirational goal of AI research. Neuromorphic hardware implementing Spiking Neural Networks (SNNs) are particularly attractive in this regard, as their real-time, event-based, local computing paradigm makes them suitable for edge implementations and fast learning. However, the long and iterative learning that characterizes state-of-the-art SNN training is incompatible with the physical nature and real-time operation of neuromorphic hardware. Bi-level learning, such as meta-learning is increasingly used in deep learning to overcome these limitations. In this work, we demonstrate gradient-based meta-learning in SNNs using the surrogate gradient method that approximates the spiking threshold function for gradient estimations. Because surrogate gradients can be made twice differentiable, well-established, and effective second-order gradient meta-learning methods such as Model Agnostic Meta Learning (MAML) can be used. We show that SNNs meta-trained using MAML match or exceed the performance of conventional ANNs meta-trained with MAML on event-based meta-datasets. Furthermore, we demonstrate the specific advantages that accrue from meta-learning: fast learning without the requirement of high precision weights or gradients. Our results emphasize how meta-learning techniques can become instrumental for deploying neuromorphic learning technologies on real-world problems. </details>
<details>	<summary>注释</summary>	Submitted to IOP Neuromorphic Computing and Engineering for peer review </details>
<details>	<summary>邮件日期</summary>	2022年01月27日</details>

# 334、BrainScaleS-2加速的混合可塑性神经形态系统
- [ ] The BrainScaleS-2 accelerated neuromorphic system with hybrid plasticity 
时间：2022年01月26日                         第一作者：Christian Pehle                       [链接](https://arxiv.org/abs/2201.11063).                     
## 摘要：自从电子元件开始进行信息处理以来，神经系统就一直是计算原语组织的隐喻。当今的脑启发计算包括一类方法，从使用新型纳米设备进行计算到研究大规模神经形态架构，如TrueNorth、SpiNNaker、BrainScaleS、Tianjic和Loihi。虽然实现细节有所不同，但脉冲神经网络（有时被称为第三代神经网络）是用于对此类系统的计算建模的常见抽象。在这里，我们描述了BrainScaleS神经形态架构的第二代，强调了该架构支持的应用。它结合了一个定制的模拟加速器核心，支持仿生脉冲神经网络原语的加速物理仿真，以及一个紧密耦合的数字处理器和一个数字事件路由网络。
<details>	<summary>英文摘要</summary>	Since the beginning of information processing by electronic components, the nervous system has served as a metaphor for the organization of computational primitives. Brain-inspired computing today encompasses a class of approaches ranging from using novel nano-devices for computation to research into large-scale neuromorphic architectures, such as TrueNorth, SpiNNaker, BrainScaleS, Tianjic, and Loihi. While implementation details differ, spiking neural networks -- sometimes referred to as the third generation of neural networks -- are the common abstraction used to model computation with such systems. Here we describe the second generation of the BrainScaleS neuromorphic architecture, emphasizing applications enabled by this architecture. It combines a custom analog accelerator core supporting the accelerated physical emulation of bio-inspired spiking neural network primitives with a tightly coupled digital processor and a digital event-routing network. </details>
<details>	<summary>注释</summary>	22 pages, 10 figures </details>
<details>	<summary>邮件日期</summary>	2022年01月27日</details>

# 333、S$^2$NN：用于训练节能单步神经网络的脉冲替代梯度的时间步长缩减
- [ ] S$^2$NN: Time Step Reduction of Spiking Surrogate Gradients for Training Energy Efficient Single-Step Neural Networks 
时间：2022年01月26日                         第一作者：Kazuma Suetake                       [链接](https://arxiv.org/abs/2201.10879).                     
## 摘要：随着神经网络规模的增加，需要能够使其以较低的计算成本和能源效率运行的技术。从这些需求出发，人们提出了各种有效的神经网络模式，如脉冲神经网络（SNN）或二元神经网络（BNN）。然而，它们有一些棘手的缺点，比如推理精度和延迟降低。为了解决这些问题，我们提出了一种单步神经网络（S$^2$NN），这是一种计算成本低、精度高的节能神经网络。建议的S$^2$NN将隐藏层之间的信息通过脉冲处理为SNN。然而，它没有时间维度，因此在训练和推理阶段没有延迟。因此，与需要时间序列处理的SNN相比，建议的S$^2$NN具有更低的计算成本。然而S$^2$NN不能采用na“{i}由于脉冲的不可微性，我们采用了反向传播算法。我们通过将多时间步长SNN的替代梯度减少到单个时间步长，推导出了一个合适的神经元模型。我们通过实验证明，与现有的SNN和BNN神经元模型相比，获得的神经元模型使S$^2$NN能够更准确、更高效地进行训练。我们还表明，建议的S$^2$NN可以实现与全精度网络相当的精度，同时具有高能效。
<details>	<summary>英文摘要</summary>	As the scales of neural networks increase, techniques that enable them to run with low computational cost and energy efficiency are required. From such demands, various efficient neural network paradigms, such as spiking neural networks (SNNs) or binary neural networks (BNNs), have been proposed. However, they have sticky drawbacks, such as degraded inference accuracy and latency. To solve these problems, we propose a single-step neural network (S$^2$NN), an energy-efficient neural network with low computational cost and high precision. The proposed S$^2$NN processes the information between hidden layers by spikes as SNNs. Nevertheless, it has no temporal dimension so that there is no latency within training and inference phases as BNNs. Thus, the proposed S$^2$NN has a lower computational cost than SNNs that require time-series processing. However, S$^2$NN cannot adopt na\"{i}ve backpropagation algorithms due to the non-differentiability nature of spikes. We deduce a suitable neuron model by reducing the surrogate gradient for multi-time step SNNs to a single-time step. We experimentally demonstrated that the obtained neuron model enables S$^2$NN to train more accurately and energy-efficiently than existing neuron models for SNNs and BNNs. We also showed that the proposed S$^2$NN could achieve comparable accuracy to full-precision networks while being highly energy-efficient. </details>
<details>	<summary>注释</summary>	19 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2022年01月27日</details>

# 332、基于事件的电位辅助脉冲神经网络视频重建
- [ ] Event-based Video Reconstruction via Potential-assisted Spiking Neural Network 
时间：2022年01月25日                         第一作者：Lin Zhu                       [链接](https://arxiv.org/abs/2201.10943).                     
## 摘要：神经形态视觉传感器是一种新的仿生成像模式，它报告称为“事件”的异步、连续每像素亮度变化，具有高时间分辨率和高动态范围。到目前为止，基于事件的图像重建方法都是基于人工神经网络（ANN）或手工制作的时空平滑技术。在本文中，我们首先通过全脉冲神经网络（SNN）结构实现图像重建工作。作为仿生神经网络，SNN在异步二进制峰值随时间分布的情况下运行，可能会在事件驱动硬件上带来更高的计算效率。我们提出了一种基于事件的视频重建框架，该框架基于一个完全脉冲神经网络（EVSNN），该网络利用了漏积分与激发（LIF）神经元和膜电位（MP）神经元。我们发现，脉冲神经元有潜力存储有用的时间信息（记忆），以完成这种时间相关的任务。此外，为了更好地利用时间信息，我们提出了一种混合电位辅助框架（PA-EVSNN），该框架利用了脉冲神经元的膜电位。该神经元称为自适应膜电位（AMP）神经元，它根据输入峰值自适应地更新膜电位。实验结果表明，我们的模型在IJRR、MVSEC和HQF数据集上的性能与基于ANN的模型相当。EVSNN和PA-EVSNN的能耗分别比其ANN结构的计算效率高19.36$\倍和7.75$\倍。
<details>	<summary>英文摘要</summary>	Neuromorphic vision sensor is a new bio-inspired imaging paradigm that reports asynchronous, continuously per-pixel brightness changes called `events' with high temporal resolution and high dynamic range. So far, the event-based image reconstruction methods are based on artificial neural networks (ANN) or hand-crafted spatiotemporal smoothing techniques. In this paper, we first implement the image reconstruction work via fully spiking neural network (SNN) architecture. As the bio-inspired neural networks, SNNs operating with asynchronous binary spikes distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. We propose a novel Event-based Video reconstruction framework based on a fully Spiking Neural Network (EVSNN), which utilizes Leaky-Integrate-and-Fire (LIF) neuron and Membrane Potential (MP) neuron. We find that the spiking neurons have the potential to store useful temporal information (memory) to complete such time-dependent tasks. Furthermore, to better utilize the temporal information, we propose a hybrid potential-assisted framework (PA-EVSNN) using the membrane potential of spiking neuron. The proposed neuron is referred as Adaptive Membrane Potential (AMP) neuron, which adaptively updates the membrane potential according to the input spikes. The experimental results demonstrate that our models achieve comparable performance to ANN-based models on IJRR, MVSEC, and HQF datasets. The energy consumptions of EVSNN and PA-EVSNN are 19.36$\times$ and 7.75$\times$ more computationally efficient than their ANN architectures, respectively. </details>
<details>	<summary>邮件日期</summary>	2022年01月27日</details>

# 331、神经体系结构寻找脉冲神经网络
- [ ] Neural Architecture Search for Spiking Neural Networks 
时间：2022年01月23日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2201.10355).                     
## 摘要：脉冲神经网络（SNN）由于其固有的高稀疏性激活，作为传统人工神经网络（ANN）的一种潜在的节能替代方案，受到了广泛关注。然而，大多数以前的SNN方法使用类似ANN的体系结构（例如VGG网络或ResNet），这可能为SNN中二进制信息的时序处理提供次优性能。为了解决这个问题，在本文中，我们介绍了一种新的神经结构搜索（NAS）方法来寻找更好的SNN结构。受最近从初始化时的激活模式中找到最佳体系结构的NAS方法的启发，我们选择的体系结构可以在不经过训练的情况下跨不同的数据样本表示不同的脉冲激活模式。此外，为了利用峰值之间的时间相关性，我们在层之间搜索前馈连接和反向连接（即时间反馈连接）。有趣的是，通过我们的搜索算法发现的SNASNet在向后连接的情况下实现了更高的性能，这表明了设计SNN体系结构以适当使用时态信息的重要性。我们在三个图像识别基准上进行了大量实验，结果表明，SNASNet以显著更低的时间步长（5个时间步长）实现了最先进的性能。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have gained huge attention as a potential energy-efficient alternative to conventional Artificial Neural Networks (ANNs) due to their inherent high-sparsity activation. However, most prior SNN methods use ANN-like architectures (e.g., VGG-Net or ResNet), which could provide sub-optimal performance for temporal sequence processing of binary information in SNNs. To address this, in this paper, we introduce a novel Neural Architecture Search (NAS) approach for finding better SNN architectures. Inspired by recent NAS approaches that find the optimal architecture from activation patterns at initialization, we select the architecture that can represent diverse spike activation patterns across different data samples without training. Furthermore, to leverage the temporal correlation among the spikes, we search for feed forward connections as well as backward connections (i.e., temporal feedback connections) between layers. Interestingly, SNASNet found by our search algorithm achieves higher performance with backward connections, demonstrating the importance of designing SNN architecture for suitably using temporal information. We conduct extensive experiments on three image recognition benchmarks where we show that SNASNet achieves state-of-the-art performance with significantly lower timesteps (5 timesteps). </details>
<details>	<summary>邮件日期</summary>	2022年01月26日</details>

# 330、使用普通设备的摄像头和机器视觉速度提高1000倍
- [ ] 1000x Faster Camera and Machine Vision with Ordinary Devices 
时间：2022年01月23日                         第一作者：Tiejun Huang                       [链接](https://arxiv.org/abs/2201.09302).                     
## 摘要：在数码相机中，我们发现了一个主要的局限性：从胶片相机继承的图像和视频形式阻碍了它捕捉快速变化的光子世界。在这里，我们介绍了vidar，一种位序列阵列，其中每个位表示光子的累积是否已达到阈值，以记录和重建任何时刻的场景辐射。通过仅使用消费级CMOS传感器和集成电路，我们开发了一款比传统相机快1000倍的vidar相机。通过将vidar视为生物视觉中的脉冲序列，我们进一步开发了一个基于脉冲神经网络的机器视觉系统，该系统将机器的速度与生物视觉的机制结合起来，实现了比人类视觉快1000倍的高速目标检测和跟踪。我们展示了vidar摄像机和super vision系统在辅助裁判和目标指向系统中的应用。我们的研究有望从根本上彻底改变图像和视频概念及相关行业，包括摄影、电影和视觉媒体，并开启一个新的神经网络驱动的无速度机器视觉时代。
<details>	<summary>英文摘要</summary>	In digital cameras, we find a major limitation: the image and video form inherited from a film camera obstructs it from capturing the rapidly changing photonic world. Here, we present vidar, a bit sequence array where each bit represents whether the accumulation of photons has reached a threshold, to record and reconstruct the scene radiance at any moment. By employing only consumer-level CMOS sensors and integrated circuits, we have developed a vidar camera that is 1,000x faster than conventional cameras. By treating vidar as spike trains in biological vision, we have further developed a spiking neural network-based machine vision system that combines the speed of the machine and the mechanism of biological vision, achieving high-speed object detection and tracking 1,000x faster than human vision. We demonstrate the utility of the vidar camera and the super vision system in an assistant referee and target pointing system. Our study is expected to fundamentally revolutionize the image and video concepts and related industries, including photography, movies, and visual media, and to unseal a new spiking neural network-enabled speed-free machine vision era. </details>
<details>	<summary>邮件日期</summary>	2022年01月25日</details>

# 329、深度强化学习与脉冲Q学习
- [ ] Deep Reinforcement Learning with Spiking Q-learning 
时间：2022年01月21日                         第一作者：Ding Chen                       [链接](https://arxiv.org/abs/2201.09754).                     
## 摘要：在特殊的神经形态硬件的帮助下，脉冲神经网络（SNN）有望以更低的能耗实现人工智能。将SNN和深度强化学习（RL）相结合，为实际控制任务提供了一种有前途的节能方法。目前，基于SNN的RL方法很少。大多数算法要么缺乏泛化能力，要么在训练中使用人工神经网络（ANN）来估计值函数。前者需要为每个场景调整大量的超参数，而后者限制了不同类型的RL算法的应用，忽略了训练中的巨大能量消耗。为了开发一种鲁棒的基于脉冲的RL方法，我们从昆虫中发现的非脉冲中间神经元中汲取灵感，提出了深脉冲Q网络（DSQN），使用非脉冲神经元的膜电压作为Q值的表示，它可以直接使用端到端RL从高维感觉输入学习鲁棒策略。在17个Atari游戏上进行的实验表明，DSQN在大多数游戏中都优于基于人工神经网络的深度Q网络（DQN）。此外，实验结果表明，DSQN具有良好的学习稳定性和对抗性攻击的鲁棒性。
<details>	<summary>英文摘要</summary>	With the help of special neuromorphic hardware, spiking neural networks (SNNs) are expected to realize artificial intelligence with less energy consumption. It provides a promising energy-efficient way for realistic control tasks by combing SNNs and deep reinforcement learning (RL). There are only a few existing SNN-based RL methods at present. Most of them either lack generalization ability or employ Artificial Neural Networks (ANNs) to estimate value function in training. The former needs to tune numerous hyper-parameters for each scenario, and the latter limits the application of different types of RL algorithm and ignores the large energy consumption in training. To develop a robust spike-based RL method, we draw inspiration from non-spiking interneurons found in insects and propose the deep spiking Q-network (DSQN), using the membrane voltage of non-spiking neurons as the representation of Q-value, which can directly learn robust policies from high-dimensional sensory inputs using end-to-end RL. Experiments conducted on 17 Atari games demonstrate the effectiveness of DSQN by outperforming the ANN-based deep Q-network (DQN) in most games. Moreover, the experimental results show superior learning stability and robustness to adversarial attacks of DSQN. </details>
<details>	<summary>邮件日期</summary>	2022年01月25日</details>

# 328、神经形态混合脉冲运动检测器
- [ ] NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector 
时间：2022年01月19日                         第一作者：Pedro Machado                        [链接](https://arxiv.org/abs/2112.06102).                     
<details>	<summary>邮件日期</summary>	2022年01月21日</details>

# 327、POPPINS：一种基于群体的数字脉冲神经形态处理器，具有整数二次积分和激发神经元
- [ ] POPPINS : A Population-Based Digital Spiking Neuromorphic Processor with Integer Quadratic Integrate-and-Fire Neurons 
时间：2022年01月19日                         第一作者：Zuo-Wei Yeh                       [链接](https://arxiv.org/abs/2201.07490).                     
## 摘要：人脑作为生物处理系统的内部运作在很大程度上仍然是个谜。受人脑功能的启发，并基于对果蝇等其他物种的简单神经网络系统的分析，神经形态计算系统吸引了相当大的兴趣。在细胞水平的连接组学研究中，我们可以识别生物神经网络（称为群体）的特征，这些特征不仅构成网络中的循环完整连接，还构成每个神经元中的外部刺激和自我连接。基于网络中脉冲传输和输入数据的低数据带宽，脉冲神经网络具有低延迟和低功耗的设计。在这项研究中，我们提出了一种可配置的基于群体的数字脉冲神经形态处理器，采用180nm处理技术，具有两个可配置的层次群体。此外，处理器中的这些神经元可以配置为新模型、整数二次积分和fire神经元模型，其中包含一个无符号的8位膜电位值。该处理器可以实时执行智能决策以避免事故。此外，该方法还可以开发仿生神经形态系统和各种低功耗、低延迟的推理处理应用。
<details>	<summary>英文摘要</summary>	The inner operations of the human brain as a biological processing system remain largely a mystery. Inspired by the function of the human brain and based on the analysis of simple neural network systems in other species, such as Drosophila, neuromorphic computing systems have attracted considerable interest. In cellular-level connectomics research, we can identify the characteristics of biological neural network, called population, which constitute not only recurrent fullyconnection in network, also an external-stimulus and selfconnection in each neuron. Relying on low data bandwidth of spike transmission in network and input data, Spiking Neural Networks exhibit low-latency and low-power design. In this study, we proposed a configurable population-based digital spiking neuromorphic processor in 180nm process technology with two configurable hierarchy populations. Also, these neurons in the processor can be configured as novel models, integer quadratic integrate-and-fire neuron models, which contain an unsigned 8-bit membrane potential value. The processor can implement intelligent decision making for avoidance in real-time. Moreover, the proposed approach enables the developments of biomimetic neuromorphic system and various low-power, and low-latency inference processing applications. </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 326、时态计算机组织
- [ ] Temporal Computer Organization 
时间：2022年01月19日                         第一作者：James E. Smith                       [链接](https://arxiv.org/abs/2201.07742).                     
## 摘要：本文档重点介绍在使用时间瞬变进行通信和计算的技术中实现的计算系统。虽然用一般术语描述，但脉冲神经网络的实现是主要的兴趣。作为背景，本文总结了一个构造时态网络的代数。然后，描述了由同步段组成的系统组织。这些段在内部进行前馈，并在段之间进行反馈。同步时钟在每个计算步骤或周期结束时重置网络段。在其基本形式中，同步时钟仅执行重置功能。在神经网络的背景下，这满足了生物学的合理性。然而，功能完整性受到限制。通过允许将同步时钟用作作为时间参考值的附加功能输入，消除了该限制。
<details>	<summary>英文摘要</summary>	This document is focused on computing systems implemented in technologies that communicate and compute with temporal transients. Although described in general terms, implementations of spiking neural networks are of primary interest. As background, an algebra for constructing temporal networks is summarized. Then, a system organization consisting of synchronized segments is described. The segments are feedforward internally with feedback between segments. A synchronizing clock resets network segments at the end of each computation step or cycle. In its basic form, the synchronizing clock merely performs a reset function. In the context of neural networks, this satisfies biological plausibility. However, functional completeness is restricted. This restriction is removed by allowing use of the synchronizing clock as an additional function input that acts as a temporal reference value. </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 325、脉冲神经网络的FPGA优化硬件加速
- [ ] FPGA-optimized Hardware acceleration for Spiking Neural Networks 
时间：2022年01月18日                         第一作者：Alessio Carpegna                       [链接](https://arxiv.org/abs/2201.06993).                     
## 摘要：人工智能（AI）在许多不同的任务中获得了成功和重要性。人工智能系统的日益普及和复杂性促使研究人员开发专用硬件加速器。脉冲神经网络（SNN）在这个意义上代表了一个有前途的解决方案，因为它们实现的模型更适合可靠的硬件设计。此外，从神经科学的角度来看，它们更好地模仿人脑。这项工作提出了一种用于SNN的硬件加速器的开发，该加速器带有离线训练，应用于图像识别任务，使用MNIST作为目标数据集。许多技术被用于最小化面积和最大化性能，例如用简单的位移位替换乘法运算，以及最小化花费在非活动脉冲上的时间，这对于更新神经元的内部状态是无用的。该设计以Xilinx Artix-7 FPGA为目标，总共使用了约40%的可用硬件资源，并将分类时间减少了三个数量级，与全精度软件相比，对精度的影响较小，为4.5%。
<details>	<summary>英文摘要</summary>	Artificial intelligence (AI) is gaining success and importance in many different tasks. The growing pervasiveness and complexity of AI systems push researchers towards developing dedicated hardware accelerators. Spiking Neural Networks (SNN) represent a promising solution in this sense since they implement models that are more suitable for a reliable hardware design. Moreover, from a neuroscience perspective, they better emulate a human brain. This work presents the development of a hardware accelerator for an SNN, with off-line training, applied to an image recognition task, using the MNIST as the target dataset. Many techniques are used to minimize the area and to maximize the performance, such as the replacement of the multiplication operation with simple bit shifts and the minimization of the time spent on inactive spikes, useless for the update of neurons' internal state. The design targets a Xilinx Artix-7 FPGA, using in total around the 40% of the available hardware resources and reducing the classification time by three orders of magnitude, with a small 4.5% impact on the accuracy, if compared to its software, full precision counterpart. </details>
<details>	<summary>注释</summary>	6 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2022年01月19日</details>

# 324、利用深度学习的经验教训训练脉冲神经网络
- [ ] Training Spiking Neural Networks Using Lessons From Deep Learning 
时间：2022年01月14日                         第一作者：Jason K. Eshraghian                        [链接](https://arxiv.org/abs/2109.12894).                     
<details>	<summary>邮件日期</summary>	2022年01月19日</details>

# 323、将STDP引入多层递归脉冲神经网络
- [ ] Including STDP to eligibility propagation in multi-layer recurrent spiking neural networks 
时间：2022年01月05日                         第一作者：Werner van der Veen                       [链接](https://arxiv.org/abs/2201.07602).                     
## 摘要：与基于深度学习的方法相比，神经形态系统中的脉冲神经网络（Spiking neural networks，SNN）具有更高的能量效率，但目前还没有明确的竞争学习算法来训练此类SNN。资格传播（e-prop）提供了一种在低功耗神经形态硬件中训练有竞争力的复发性SNN的有效且生物学上合理的方法。在本报告中，再现了e-prop在语音分类任务中的先前表现，并分析了包含STDP样行为的影响。在ALIF神经元模型中加入STDP可以提高分类性能，但Izhikevich e-prop神经元的情况并非如此。最后，我们发现在单层循环SNN中实现的e-prop始终优于多层变体。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) in neuromorphic systems are more energy efficient compared to deep learning-based methods, but there is no clear competitive learning algorithm for training such SNNs. Eligibility propagation (e-prop) offers an efficient and biologically plausible way to train competitive recurrent SNNs in low-power neuromorphic hardware. In this report, previous performance of e-prop on a speech classification task is reproduced, and the effects of including STDP-like behavior are analyzed. Including STDP to the ALIF neuron model improves the classification performance, but this is not the case for the Izhikevich e-prop neuron. Finally, it was found that e-prop implemented in a single-layer recurrent SNN consistently outperforms a multi-layer variant. </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 322、具有时间截断局部反向传播的脉冲神经网络的有效训练
- [ ] Efficient Training of Spiking Neural Networks with Temporally-Truncated Local Backpropagation through Time 
时间：2021年12月13日                         第一作者：Wenzhe Guo                       [链接](https://arxiv.org/abs/2201.07210).                     
## 摘要：由于复杂的神经动力学和触发函数的内在不可微性，直接训练脉冲神经网络（SNN）仍然具有挑战性。为训练SNN而提出的著名的时间反向传播（backpropagation through time，BPTT）算法存在较大的内存占用，并且禁止反向和更新解锁，因此无法利用局部监督训练方法的潜力。本文提出了一种高效、直接的SNN训练算法，该算法将局部监督训练方法与时间截断的BPTT算法相结合。该算法探索了BPTT中的时间和空间局部性，有助于显著降低计算成本，包括GPU内存利用率、主存访问和算术运算。我们深入探讨了与时间截断长度和局部训练块大小有关的设计空间，并测试了它们对运行不同类型任务的不同网络的分类精度的影响。结果表明，时间截断对基于帧的数据集的分类精度有负面影响，但会提高动态视觉传感器（DVS）记录数据集的分类精度。尽管会导致信息丢失，但本地培训能够缓解过度拟合。时间截断和局部训练的联合作用可以减缓精度下降，甚至提高精度。此外，与标准端到端BPTT相比，对AlexNet分类CIFAR10-DVS数据集等深层SNS模型进行训练，可使精确度提高7.26%，GPU内存减少89.94%，内存访问减少10.79%，MAC操作减少99.64%。
<details>	<summary>英文摘要</summary>	Directly training spiking neural networks (SNNs) has remained challenging due to complex neural dynamics and intrinsic non-differentiability in firing functions. The well-known backpropagation through time (BPTT) algorithm proposed to train SNNs suffers from large memory footprint and prohibits backward and update unlocking, making it impossible to exploit the potential of locally-supervised training methods. This work proposes an efficient and direct training algorithm for SNNs that integrates a locally-supervised training method with a temporally-truncated BPTT algorithm. The proposed algorithm explores both temporal and spatial locality in BPTT and contributes to significant reduction in computational cost including GPU memory utilization, main memory access and arithmetic operations. We thoroughly explore the design space concerning temporal truncation length and local training block size and benchmark their impact on classification accuracy of different networks running different types of tasks. The results reveal that temporal truncation has a negative effect on the accuracy of classifying frame-based datasets, but leads to improvement in accuracy on dynamic-vision-sensor (DVS) recorded datasets. In spite of resulting information loss, local training is capable of alleviating overfitting. The combined effect of temporal truncation and local training can lead to the slowdown of accuracy drop and even improvement in accuracy. In addition, training deep SNNs models such as AlexNet classifying CIFAR10-DVS dataset leads to 7.26% increase in accuracy, 89.94% reduction in GPU memory, 10.79% reduction in memory access, and 99.64% reduction in MAC operations compared to the standard end-to-end BPTT. </details>
<details>	<summary>注释</summary>	16 </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 321、通过直接训练的深度脉冲Q网络实现人的水平控制
- [ ] Human-Level Control through Directly-Trained Deep Spiking Q-Networks 
时间：2021年12月13日                         第一作者：Guisong Liu                       [链接](https://arxiv.org/abs/2201.07211).                     
## 摘要：作为第三代神经网络，脉冲神经网络（Spiking neural networks，SNNs）由于具有较高的能量效率，在神经形态硬件方面有着巨大的潜力。然而，由于二进制输出和脉冲函数的不可微性，深度脉冲强化学习（DSRL）即基于SNN的强化学习（RL）仍处于初级阶段。为了解决这些问题，本文提出了一种深度脉冲Q网络（DSQN）。具体来说，我们提出了一种基于泄漏集成与激发（LIF）神经元和深度Q网络（DQN）的直接训练深度脉冲强化学习体系结构。然后，我们对深度脉冲Q网络采用了直接脉冲学习算法。我们进一步从理论上证明了在DSQN中使用LIF神经元的优势。在17款表现最好的Atari游戏上进行了综合实验，将我们的方法与最先进的转换方法进行比较。实验结果证明了该方法在性能、稳定性、鲁棒性和能量效率方面的优越性。据我们所知，我们的工作是第一个通过直接训练的SNN在多个Atari游戏中实现最先进性能的工作。
<details>	<summary>英文摘要</summary>	As the third-generation neural networks, Spiking Neural Networks (SNNs) have great potential on neuromorphic hardware because of their high energy-efficiency. However, Deep Spiking Reinforcement Learning (DSRL), i.e., the Reinforcement Learning (RL) based on SNNs, is still in its preliminary stage due to the binary output and the non-differentiable property of the spiking function. To address these issues, we propose a Deep Spiking Q-Network (DSQN) in this paper. Specifically, we propose a directly-trained deep spiking reinforcement learning architecture based on the Leaky Integrate-and-Fire (LIF) neurons and Deep Q-Network (DQN). Then, we adapt a direct spiking learning algorithm for the Deep Spiking Q-Network. We further demonstrate the advantages of using LIF neurons in DSQN theoretically. Comprehensive experiments have been conducted on 17 top-performing Atari games to compare our method with the state-of-the-art conversion method. The experimental results demonstrate the superiority of our method in terms of performance, stability, robustness and energy-efficiency. To the best of our knowledge, our work is the first one to achieve state-of-the-art performance on multiple Atari games with the directly-trained SNN. </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 320、通过解决脉冲神经网络中的退化问题来推进深度剩余学习
- [ ] Advancing Deep Residual Learning by Solving the Crux of Degradation in Spiking Neural Networks 
时间：2021年12月09日                         第一作者：Yifan Hu                       [链接](https://arxiv.org/abs/2201.07209).                     
## 摘要：尽管神经形态计算的发展很快，但脉冲神经网络（SNN）的深度不足和表现力不足严重限制了其实际应用范围。剩余学习和捷径已被证明是训练深层神经网络的一种重要方法，但之前的工作很少评估它们对基于棘波的通信和时空动力学特征的适用性。这种疏忽导致信息流动受阻，并伴随着降级问题。在本文中，我们确定了关键点，然后提出了一种新的SNN剩余块，它能够显著扩展直接训练的SNN的深度，例如，在CIFAR-10上多达482层，在ImageNet上多达104层，而不会观察到任何轻微的退化问题。我们在基于帧和神经形态的数据集上验证了我们的方法的有效性，我们的SRM-ResNet104在ImageNet上获得了76.02%的优越结果，这是在直接训练的SNN领域中的首次。估计了巨大的能量效率，由此产生的网络平均每个神经元只需要一个脉冲来对输入样本进行分类。我们相信，我们强大且可扩展的建模将为SNN的进一步探索提供强有力的支持。
<details>	<summary>英文摘要</summary>	Despite the rapid progress of neuromorphic computing, the inadequate depth and the resulting insufficient representation power of spiking neural networks (SNNs) severely restrict their application scope in practice. Residual learning and shortcuts have been evidenced as an important approach for training deep neural networks, but rarely did previous work assess their applicability to the characteristics of spike-based communication and spatiotemporal dynamics. This negligence leads to impeded information flow and the accompanying degradation problem. In this paper, we identify the crux and then propose a novel residual block for SNNs, which is able to significantly extend the depth of directly trained SNNs, e.g., up to 482 layers on CIFAR-10 and 104 layers on ImageNet, without observing any slight degradation problem. We validate the effectiveness of our methods on both frame-based and neuromorphic datasets, and our SRM-ResNet104 achieves a superior result of 76.02% accuracy on ImageNet, the first time in the domain of directly trained SNNs. The great energy efficiency is estimated and the resulting networks need on average only one spike per neuron for classifying an input sample. We believe our powerful and scalable modeling will provide a strong support for further exploration of SNNs. </details>
<details>	<summary>注释</summary>	arXiv admin note: substantial text overlap with arXiv:2112.08954 </details>
<details>	<summary>邮件日期</summary>	2022年01月20日</details>

# 319、稀疏脉冲梯度下降
- [ ] Sparse Spiking Gradient Descent 
时间：2022年01月13日                         第一作者：Nicolas Perez-Nieves                        [链接](https://arxiv.org/abs/2105.08810).                     
<details>	<summary>邮件日期</summary>	2022年01月14日</details>

# 318、利用基于时间的神经元提高脉冲神经网络的精度
- [ ] Improving Spiking Neural Network Accuracy Using Time-based Neurons 
时间：2022年01月05日                         第一作者：Hanseok Kim                       [链接](https://arxiv.org/abs/2201.01394).                     
## 摘要：由于von Neumann体系结构下运行深度学习模型在降低功耗方面的基本限制，基于模拟神经元的低功耗脉冲神经网络的神经形态计算系统的研究备受关注。为了集成大量神经元，神经元需要设计成占据一个小的区域，但随着技术规模的缩小，模拟神经元很难扩展，并且它们的电压余量/动态范围和电路非线性都会降低。有鉴于此，本文首先对现有基于电流镜的电压域神经元在28nm工艺中的非线性行为进行了建模，并表明神经元的非线性效应会严重降低SNN推理精度。然后，为了缓解这个问题，我们提出了一种新的神经元，它在时域处理传入的脉冲，并大大提高了线性度，从而与现有的电压域神经元相比提高了推理精度。在MNIST数据集上测试，该神经元的推理错误率与理想神经元的推理错误率相差不到0.1%。
<details>	<summary>英文摘要</summary>	Due to the fundamental limit to reducing power consumption of running deep learning models on von-Neumann architecture, research on neuromorphic computing systems based on low-power spiking neural networks using analog neurons is in the spotlight. In order to integrate a large number of neurons, neurons need to be designed to occupy a small area, but as technology scales down, analog neurons are difficult to scale, and they suffer from reduced voltage headroom/dynamic range and circuit nonlinearities. In light of this, this paper first models the nonlinear behavior of existing current-mirror-based voltage-domain neurons designed in a 28nm process, and show SNN inference accuracy can be severely degraded by the effect of neuron's nonlinearity. Then, to mitigate this problem, we propose a novel neuron, which processes incoming spikes in the time domain and greatly improves the linearity, thereby improving the inference accuracy compared to the existing voltage-domain neuron. Tested on the MNIST dataset, the inference error rate of the proposed neuron differs by less than 0.1% from that of the ideal neuron. </details>
<details>	<summary>邮件日期</summary>	2022年01月06日</details>

# 317、一种基于感受野的鲁棒视觉采样模型
- [ ] A Robust Visual Sampling Model Inspired by Receptive Field 
时间：2022年01月04日                         第一作者：Liwen Hu                       [链接](https://arxiv.org/abs/2201.01030).                     
## 摘要：模拟视网膜中央凹的脉冲相机可以通过发射脉冲来报告每像素亮度强度的累积。作为一种具有高时间分辨率的仿生视觉传感器，它在计算机视觉领域有着巨大的潜力。然而，现有的Spike相机的采样模型容易受到量化和噪声的影响，无法有效地捕捉物体的纹理细节。在这项工作中，受感受野（RVSM）的启发，提出了一种鲁棒的视觉采样模型，该模型使用高斯差分（DoG）和高斯滤波器生成的小波滤波器来模拟感受野。利用类似于小波逆变换的方法，可以将RVSM的峰值数据转换成图像。为了测试性能，我们还提出了一个包含各种运动场景的高速运动峰值数据集（HMD）。通过比较HMD中的重建图像，我们发现RVSM可以大大提高脉冲相机的信息捕获能力。更重要的是，RVSM模仿感受野机制采集区域信息，能够有效滤除高强度噪声，大大改善了脉冲相机对噪声敏感的问题。此外，由于采样结构的强泛化性，RVSM也适用于其他神经形态视觉传感器。以上实验是在一个Spike摄像机模拟器上完成的。
<details>	<summary>英文摘要</summary>	Spike camera mimicking the retina fovea can report per-pixel luminance intensity accumulation by firing spikes. As a bio-inspired vision sensor with high temporal resolution, it has a huge potential for computer vision. However, the sampling model in current Spike camera is so susceptible to quantization and noise that it cannot capture the texture details of objects effectively. In this work, a robust visual sampling model inspired by receptive field (RVSM) is proposed where wavelet filter generated by difference of Gaussian (DoG) and Gaussian filter are used to simulate receptive field. Using corresponding method similar to inverse wavelet transform, spike data from RVSM can be converted into images. To test the performance, we also propose a high-speed motion spike dataset (HMD) including a variety of motion scenes. By comparing reconstructed images in HMD, we find RVSM can improve the ability of capturing information of Spike camera greatly. More importantly, due to mimicking receptive field mechanism to collect regional information, RVSM can filter high intensity noise effectively and improves the problem that Spike camera is sensitive to noise largely. Besides, due to the strong generalization of sampling structure, RVSM is also suitable for other neuromorphic vision sensor. Above experiments are finished in a Spike camera simulator. </details>
<details>	<summary>邮件日期</summary>	2022年01月05日</details>

# 316、通过正则化和归一化改进脉冲神经网络中的代理梯度学习
- [ ] Improving Surrogate Gradient Learning in Spiking Neural Networks via Regularization and Normalization 
时间：2021年12月13日                         第一作者：N                       [链接](https://arxiv.org/abs/2201.02538).                     
## 摘要：脉冲神经网络（SNN）不同于深度学习中使用的经典网络：神经元使用称为脉冲的电脉冲进行通信，就像生物神经元一样。SNN对人工智能技术很有吸引力，因为它们可以在低功耗的神经形态芯片上实现。然而，SNN通常比其模拟对应物精度更低。在本报告中，我们研究了各种正则化和规范化技术，目的是改进SNN中的替代梯度学习。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are different from the classical networks used in deep learning: the neurons communicate using electrical impulses called spikes, just like biological neurons. SNNs are appealing for AI technology, because they could be implemented on low power neuromorphic chips. However, SNNs generally remain less accurate than their analog counterparts. In this report, we examine various regularization and normalization techniques with the goal of improving surrogate gradient learning in SNNs. </details>
<details>	<summary>注释</summary>	Bachelor Thesis </details>
<details>	<summary>邮件日期</summary>	2022年01月10日</details>

# 315、可塑性函数对神经装配的影响
- [ ] Effects of Plasticity Functions on Neural Assemblies 
时间：2021年12月29日                         第一作者：Christodoulos Constantinides                       [链接](https://arxiv.org/abs/2112.14853).                     
## 摘要：我们探讨了各种可塑性功能对神经元组装的影响。为了弥合实验理论和计算理论之间的鸿沟，我们使用了一个概念框架，即组装演算，它是一个基于神经元组装描述大脑功能的正式系统。集合演算包括投射、关联和合并神经元集合的操作。我们的研究重点是用装配演算模拟不同的塑性函数。我们的主要贡献是修改和评估投影操作。我们用Oja和脉冲时间依赖塑性（STDP）规则进行实验，并测试各种超参数的影响。
<details>	<summary>英文摘要</summary>	We explore the effects of various plasticity functions on assemblies of neurons. To bridge the gap between experimental and computational theories we make use of a conceptual framework, the Assembly Calculus, which is a formal system for the description of brain function based on assemblies of neurons. The Assembly Calculus includes operations for projecting, associating, and merging assemblies of neurons. Our research is focused on simulating different plasticity functions with Assembly Calculus. Our main contribution is the modification and evaluation of the projection operation. We experiment with Oja's and Spike Time-Dependent Plasticity (STDP) rules and test the effect of various hyper-parameters. </details>
<details>	<summary>邮件日期</summary>	2022年01月03日</details>

# 314、硅神经元事件计时的可靠性
- [ ] Reliability of Event Timing in Silicon Neurons 
时间：2021年12月28日                         第一作者：Tai Miyazaki Kirby                       [链接](https://arxiv.org/abs/2112.14134).                     
## 摘要：模拟低压电子学在以前所未有的能源效率生产硅神经元（SIN）方面显示出巨大的潜力。然而，它们对工艺、电压和温度（PVT）变化以及噪声的固有高敏感性长期以来被认为是开发有效神经形态解决方案的主要瓶颈。受生物物理新皮质神经元棘波传导研究的启发，我们证明了与生物神经元类似，模拟捷联惯导系统中固有的噪声和可变性可以与可靠的棘波传导共存。我们通过展示三种不同类型的可靠事件传输：单脉冲传输、突发传输和半中心振荡器（HCO）网络的开关控制，在最近的爆破神经元的神经形态模型上说明了这一特性。
<details>	<summary>英文摘要</summary>	Analog, low-voltage electronics show great promise in producing silicon neurons (SiNs) with unprecedented levels of energy efficiency. Yet, their inherently high susceptibility to process, voltage and temperature (PVT) variations, and noise has long been recognised as a major bottleneck in developing effective neuromorphic solutions. Inspired by spike transmission studies in biophysical, neocortical neurons, we demonstrate that the inherent noise and variability can coexist with reliable spike transmission in analog SiNs, similarly to biological neurons. We illustrate this property on a recent neuromorphic model of a bursting neuron by showcasing three different relevant types of reliable event transmission: single spike transmission, burst transmission, and the on-off control of a half-centre oscillator (HCO) network. </details>
<details>	<summary>邮件日期</summary>	2021年12月30日</details>

# 313、N-Omniglot：一个用于时空稀疏少镜头学习的大规模数据集
- [ ] N-Omniglot: a Large-scale Dataset for Spatio-Temporal Sparse Few-shot Learning 
时间：2021年12月25日                         第一作者：Yang Li                       [链接](https://arxiv.org/abs/2112.13230).                     
## 摘要：少镜头学习是人脑最重要的能力之一。然而，目前的人工智能系统在实现这一能力方面遇到了困难，生物上合理的脉冲神经网络（SNN）也是如此。传统少数镜头学习领域的数据集提供的时间信息量很少。而神经形态数据集的缺乏阻碍了SNN少数镜头学习的发展。在这里，我们使用动态视觉传感器（DVS）提供了第一个神经形态数据集：N-Omniglot。它包含1623类手写字符，每类只有20个样本。N-Omniglot消除了对SNN的神经形态数据集的需要，该数据集具有高度稀疏性和巨大的时间一致性。此外，由于笔划的时间顺序信息，该数据集为在少数镜头学习领域开发SNNs算法提供了强大的挑战和合适的基准。我们还提供了改进的最近邻、卷积网络、连体网和元学习算法，用于验证。
<details>	<summary>英文摘要</summary>	Few-shot learning (learning with a few samples) is one of the most important capacities of the human brain. However, the current artificial intelligence systems meet difficulties in achieving this ability, so as the biologically plausible spiking neural networks (SNNs). Datasets for traditional few-shot learning domains provide few amounts of temporal information. And the absence of the neuromorphic datasets has hindered the development of few-shot learning for SNNs. Here, we provide the first neuromorphic dataset: N-Omniglot, using the Dynamic Vision Sensor (DVS). It contains 1623 categories of handwritten characters, with only 20 samples per class. N-Omniglot eliminates the need for a neuromorphic dataset for SNNs with high spareness and tremendous temporal coherence. Additionally, the dataset provides a powerful challenge and a suitable benchmark for developing SNNs algorithm in the few-shot learning domain due to the chronological information of strokes. We also provide the improved nearest neighbor, convolutional network, SiameseNet, and meta-learning algorithm in spiking version for verification. </details>
<details>	<summary>邮件日期</summary>	2021年12月28日</details>

# 312、向强大的深脉冲神经网络推进剩余学习
- [ ] Advancing Residual Learning towards Powerful Deep Spiking Neural Networks 
时间：2021年12月23日                         第一作者：Yifan Hu                       [链接](https://arxiv.org/abs/2112.08954).                     
<details>	<summary>邮件日期</summary>	2021年12月24日</details>

# 311、深度神经网络可以转换为超低延迟脉冲神经网络吗？
- [ ] Can Deep Neural Networks be Converted to Ultra Low-Latency Spiking Neural Networks? 
时间：2021年12月22日                         第一作者：Gourav Datta                        [链接](https://arxiv.org/abs/2112.12133).                     
## 摘要：脉冲神经网络（SNN）通过随时间分布的二进制脉冲进行操作，已成为资源受限设备的一种有前途的节能ML范式。然而，目前最先进的（SOTA）SNN需要多个时间步才能达到可接受的推理精度，从而增加峰值活动，从而增加能耗。SNN的SOTA训练策略涉及非脉冲深度神经网络（DNN）的转换。在本文中，我们确定SOTA转换策略不能产生超低延迟，因为它们错误地假设DNN和SNN预激活值是均匀分布的。我们提出了一种新的训练算法，能够准确地捕获这些分布，最小化DNN和转换后的SNN之间的误差。由此产生的SNN具有超低延迟和高激活稀疏性，从而显著提高了计算效率。特别是，我们在几个VGG和ResNet体系结构上对CIFAR-10和CIFAR-100数据集的图像识别任务评估了我们的框架。与iso体系结构标准DNN相比，我们在CIFAR-100数据集上仅使用2个时间步长就获得了64.19%的顶级精度，计算能量降低了约159.2倍。与其他SOTA SNN模型相比，我们的模型推理速度快2.5-8倍（即，时间步长更少）。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs), that operate via binary spikes distributed over time, have emerged as a promising energy efficient ML paradigm for resource-constrained devices. However, the current state-of-the-art (SOTA) SNNs require multiple time steps for acceptable inference accuracy, increasing spiking activity and, consequently, energy consumption. SOTA training strategies for SNNs involve conversion from a non-spiking deep neural network (DNN). In this paper, we determine that SOTA conversion strategies cannot yield ultra low latency because they incorrectly assume that the DNN and SNN pre-activation values are uniformly distributed. We propose a new training algorithm that accurately captures these distributions, minimizing the error between the DNN and converted SNN. The resulting SNNs have ultra low latency and high activation sparsity, yielding significant improvements in compute efficiency. In particular, we evaluate our framework on image recognition tasks from CIFAR-10 and CIFAR-100 datasets on several VGG and ResNet architectures. We obtain top-1 accuracy of 64.19% with only 2 time steps on the CIFAR-100 dataset with ~159.2x lower compute energy compared to an iso-architecture standard DNN. Compared to other SOTA SNN models, our models perform inference 2.5-8x faster (i.e., with fewer time steps). </details>
<details>	<summary>注释</summary>	Accepted to DATE 2022 </details>
<details>	<summary>邮件日期</summary>	2021年12月23日</details>

# 310、通过时间向前传播实现动态脉冲神经网络的精确在线训练
- [ ] Accurate online training of dynamical spiking neural networks through Forward Propagation Through Time 
时间：2021年12月20日                         第一作者：Bojian Yin                       [链接](https://arxiv.org/abs/2112.11231).                     
## 摘要：大脑中脉冲神经元之间的事件驱动和稀疏通信特性为灵活高效的人工智能带来了巨大的希望。学习算法的最新进展表明，与标准的递归神经网络相比，脉冲神经元的递归网络可以有效地训练，以获得具有竞争力的性能。尽管如此，由于这些学习算法使用时间误差反向传播（BPTT），它们的内存需求很高，训练速度很慢，并且与在线学习不兼容。这限制了这些学习算法在相对较小的网络和有限的时间序列长度上的应用。已经提出了具有较低计算和内存复杂度的BPTT在线近似（e-prop，OSTL），但在实践中也受到内存限制，并且作为近似，其性能并不优于标准BPTT训练。在这里，我们展示了一种新开发的BPTT替代方案，即通过时间的前向传播（FPTT）如何应用于脉冲神经网络。与BPTT不同，FPTT试图最大限度地降低持续的动态规范化损失风险。因此，FPTT可以在线计算，并且相对于序列长度具有固定的复杂性。结合一种新的动态脉冲神经元模型——液体时间常数神经元，我们证明了用FPTT训练的SNN优于在线BPTT近似，在时间分类任务上接近或超过离线BPTT精度。因此，这种方法可以在长序列上以对记忆友好的在线方式训练SNN，并将SNN扩展到新颖复杂的神经结构。
<details>	<summary>英文摘要</summary>	The event-driven and sparse nature of communication between spiking neurons in the brain holds great promise for flexible and energy-efficient AI. Recent advances in learning algorithms have demonstrated that recurrent networks of spiking neurons can be effectively trained to achieve competitive performance compared to standard recurrent neural networks. Still, as these learning algorithms use error-backpropagation through time (BPTT), they suffer from high memory requirements, are slow to train, and are incompatible with online learning. This limits the application of these learning algorithms to relatively small networks and to limited temporal sequence lengths. Online approximations to BPTT with lower computational and memory complexity have been proposed (e-prop, OSTL), but in practice also suffer from memory limitations and, as approximations, do not outperform standard BPTT training. Here, we show how a recently developed alternative to BPTT, Forward Propagation Through Time (FPTT) can be applied in spiking neural networks. Different from BPTT, FPTT attempts to minimize an ongoing dynamically regularized risk on the loss. As a result, FPTT can be computed in an online fashion and has fixed complexity with respect to the sequence length. When combined with a novel dynamic spiking neuron model, the Liquid-Time-Constant neuron, we show that SNNs trained with FPTT outperform online BPTT approximations, and approach or exceed offline BPTT accuracy on temporal classification tasks. This approach thus makes it feasible to train SNNs in a memory-friendly online fashion on long sequences and scale up SNNs to novel and complex neural architectures. </details>
<details>	<summary>注释</summary>	12 pages, 4 figures </details>
<details>	<summary>邮件日期</summary>	2021年12月22日</details>

# 309、基于事件视觉的脉冲卷积网络对抗攻击
- [ ] Adversarial Attacks on Spiking Convolutional Networks for Event-based Vision 
时间：2021年12月20日                         第一作者：Julian B\"uchel                       [链接](https://arxiv.org/abs/2110.02929).                     
<details>	<summary>注释</summary>	16 pages, preprint, submitted to ICLR 2022 </details>
<details>	<summary>邮件日期</summary>	2021年12月21日</details>

# 308、平衡态隐式微分训练反馈脉冲神经网络
- [ ] Training Feedback Spiking Neural Networks by Implicit Differentiation on the Equilibrium State 
时间：2021年12月17日                         第一作者：Mingqing Xiao                       [链接](https://arxiv.org/abs/2109.14247).                     
<details>	<summary>注释</summary>	Accepted by NeurIPS 2021 (Spotlight) </details>
<details>	<summary>邮件日期</summary>	2021年12月21日</details>

# 307、脉冲相机的光流估计
- [ ] Optical Flow Estimation for Spiking Camera 
时间：2021年12月17日                         第一作者：Liwen Hu                       [链接](https://arxiv.org/abs/2110.03916).                     
<details>	<summary>注释</summary>	The first two authors contributed equally </details>
<details>	<summary>邮件日期</summary>	2021年12月20日</details>

# 306、向强大的深脉冲神经网络推进剩余学习
- [ ] Advancing Residual Learning towards Powerful Deep Spiking Neural Networks 
时间：2021年12月15日                         第一作者：Yifan Hu                       [链接](https://arxiv.org/abs/2112.08954).                     
## 摘要：尽管神经形态计算发展迅速，但脉冲神经网络（SNN）的容量和表示能力不足严重限制了其实际应用范围。剩余学习和捷径已被证明是训练深层神经网络的一种重要方法，但以前的工作很少评估它们对基于棘波的通信和时空动力学特征的适用性。在本文中，我们首先发现，这种疏忽导致阻碍信息流，并伴随着退化问题，在以前的剩余SNN。然后，我们提出了一种新的面向SNN的残差块MS ResNet，它能够显著扩展直接训练的SNN的深度，例如，在CIFAR-10上可以扩展到482层，在ImageNet上可以扩展到104层，而不会观察到任何轻微的退化问题。我们在基于帧和神经形态的数据集上验证了MS-ResNet104的有效性，MS-ResNet104在ImageNet上获得了76.02%的准确率，这是在直接训练SNN领域中的首次。我们还观察到，平均每个神经元只需要一个脉冲就可以对输入样本进行分类，这具有很高的能量效率。我们相信，我们强大且可扩展的模型将为SNN的进一步开发提供强大支持。
<details>	<summary>英文摘要</summary>	Despite the rapid progress of neuromorphic computing, inadequate capacity and insufficient representation power of spiking neural networks (SNNs) severely restrict their application scope in practice. Residual learning and shortcuts have been evidenced as an important approach for training deep neural networks, but rarely did previous work assess their applicability to the characteristics of spike-based communication and spatiotemporal dynamics. In this paper, we first identify that this negligence leads to impeded information flow and accompanying degradation problem in previous residual SNNs. Then we propose a novel SNN-oriented residual block, MS-ResNet, which is able to significantly extend the depth of directly trained SNNs, e.g. up to 482 layers on CIFAR-10 and 104 layers on ImageNet, without observing any slight degradation problem. We validate the effectiveness of MS-ResNet on both frame-based and neuromorphic datasets, and MS-ResNet104 achieves a superior result of 76.02% accuracy on ImageNet, the first time in the domain of directly trained SNNs. Great energy efficiency is also observed that on average only one spike per neuron is needed to classify an input sample. We believe our powerful and scalable models will provide a strong support for further exploration of SNNs. </details>
<details>	<summary>邮件日期</summary>	2021年12月17日</details>

# 305、利用生物神经元和突触进行规划
- [ ] Planning with Biological Neurons and Synapses 
时间：2021年12月15日                         第一作者：Francesco d'Amore                       [链接](https://arxiv.org/abs/2112.08186).                     
## 摘要：我们重新讨论了块世界中的规划问题，并为此任务实现了一个已知的启发式方法。重要的是，我们的实现在生物学上是合理的，因为它完全是通过神经元的脉冲来实现的。尽管在过去的五十年里，区块世界已经取得了很多成就，但我们相信这是同类算法中的第一个。输入是编码初始块堆栈集和目标集的符号序列，输出是运动命令序列，如“将顶部块放入表上堆栈1”。该程序是在汇编演算中编写的，汇编演算是最近提出的一种计算框架，旨在通过弥合神经活动和认知功能之间的差距来模拟大脑中的计算。它的基本对象是神经元的集合（稳定的神经元集合，它们的同时放电意味着主体正在思考一个对象、概念、单词等），它的命令包括投射和合并，它的执行模型基于广泛接受的神经科学原理。这个框架中的一个程序基本上建立了一个神经元和突触的动态系统，最终以很高的概率完成了任务。这项工作的目的是从经验上证明，汇编演算中合理的大型程序能够正确可靠地执行；而这种相当现实的——如果理想化的话——更高的认知功能，比如街区世界的规划，可以通过这样的程序成功地实现。
<details>	<summary>英文摘要</summary>	We revisit the planning problem in the blocks world, and we implement a known heuristic for this task. Importantly, our implementation is biologically plausible, in the sense that it is carried out exclusively through the spiking of neurons. Even though much has been accomplished in the blocks world over the past five decades, we believe that this is the first algorithm of its kind. The input is a sequence of symbols encoding an initial set of block stacks as well as a target set, and the output is a sequence of motion commands such as ``put the top block in stack 1 on the table''. The program is written in the Assembly Calculus, a recently proposed computational framework meant to model computation in the brain by bridging the gap between neural activity and cognitive function. Its elementary objects are assemblies of neurons (stable sets of neurons whose simultaneous firing signifies that the subject is thinking of an object, concept, word, etc.), its commands include project and merge, and its execution model is based on widely accepted tenets of neuroscience. A program in this framework essentially sets up a dynamical system of neurons and synapses that eventually, with high probability, accomplishes the task. The purpose of this work is to establish empirically that reasonably large programs in the Assembly Calculus can execute correctly and reliably; and that rather realistic -- if idealized -- higher cognitive functions, such as planning in the blocks world, can be implemented successfully by such programs. </details>
<details>	<summary>邮件日期</summary>	2021年12月16日</details>

# 304、全脉冲变分自动编码器
- [ ] Fully Spiking Variational Autoencoder 
时间：2021年12月14日                         第一作者：Hiromichi Kamata                       [链接](https://arxiv.org/abs/2110.00375).                     
<details>	<summary>注释</summary>	Accepted to AAAI2022 </details>
<details>	<summary>邮件日期</summary>	2021年12月15日</details>

# 303、使用Extoll进行大规模Spike通信
- [ ] BrainScaleS Large Scale Spike Communication using Extoll 
时间：2021年12月14日                         第一作者：Tobias Thommes                       [链接](https://arxiv.org/abs/2111.15296).                     
<details>	<summary>注释</summary>	3 pages, 2 figures, submitted to the Neuro Inspired Computational Elements 2020 (NICE'2020) conference, accepted and presented as a poster in March 2021; 1st replacement: add acknowledgement of DFG (German Research Foundation) </details>
<details>	<summary>邮件日期</summary>	2021年12月15日</details>

# 302、基于事件的卷积神经网络加速器的突触压缩
- [ ] Synapse Compression for Event-Based Convolutional-Neural-Network Accelerators 
时间：2021年12月13日                         第一作者：Lennart Bamberg                       [链接](https://arxiv.org/abs/2112.07019).                     
## 摘要：制造可行的神经形态芯片需要新的计算机架构，以实现大脑轻松支持的大规模并行和高效的信息处理。新兴的基于事件的体系结构使这一梦想成为现实。然而，突触连接的大内存需求阻碍了现代卷积神经网络（CNN）在大规模并行、基于事件（脉冲）结构上的应用。这项工作克服了这一障碍，提供了一个轻量级的硬件方案，将突触内存需求压缩数千倍，使复杂的CNN能够在一个小型芯片上执行。12纳米技术中的硅实现表明，该技术仅增加了系统的实现成本2%，尽管与之前发布的最佳技术相比，总内存占用减少了374倍。
<details>	<summary>英文摘要</summary>	Manufacturing-viable neuromorphic chips require novel computer architectures to achieve the massively parallel and efficient information processing the brain supports so effortlessly. Emerging event-based architectures are making this dream a reality. However, the large memory requirements for synaptic connectivity are a showstopper for the execution of modern convolutional neural networks (CNNs) on massively parallel, event-based (spiking) architectures. This work overcomes this roadblock by contributing a lightweight hardware scheme to compress the synaptic memory requirements by several thousand times, enabling the execution of complex CNNs on a single chip of small form factor. A silicon implementation in a 12-nm technology shows that the technique increases the system's implementation cost by only 2%, despite achieving a total memory-footprint reduction of up to 374x compared to the best previously published technique. </details>
<details>	<summary>注释</summary>	Preprint submitted to IEEE Transactions on Parallel and Distributed Systems </details>
<details>	<summary>邮件日期</summary>	2021年12月15日</details>

# 301、神经形态混合脉冲运动检测器
- [ ] NeuroHSMD: Neuromorphic Hybrid Spiking Motion Detector 
时间：2021年12月12日                         第一作者：Pedro Machado                       [链接](https://arxiv.org/abs/2112.06102).                     
## 摘要：脊椎动物的视网膜在处理诸如检测运动物体等琐碎的视觉任务方面非常高效，但对于现代计算机来说这是一项复杂的任务。物体运动的检测是由名为物体运动敏感神经节细胞（OMS-GC）的特殊视网膜神经节细胞完成的。OMS-GC处理连续信号，并产生视觉皮层后处理的脉冲模式。本文提出的神经形态混合脉冲运动检测器（NeuroHSMD）使用现场可编程门阵列（FPGA）加速了HSMD算法。混合脉冲运动检测器（HSMD）算法是第一个使用定制的3层脉冲神经网络（SNN）增强动态背景减法（DBS）算法的混合算法，该网络生成OMS-GC脉冲样响应。使用相同的2012年变更检测（CDnet2012）和2014年变更检测（CDnet2014）基准数据集，将NeuroHSMD算法与HSMD算法进行比较。结果表明，神经HSMD实时生成与HSMD算法相同的结果，且质量没有下降。此外，本文提出的神经HSMD完全用开放式计算机语言（OpenCL）实现，因此很容易在其他设备中复制，如图形处理器单元（GPU）和中央处理器单元集群（CPU）。
<details>	<summary>英文摘要</summary>	Vertebrate retinas are highly-efficient in processing trivial visual tasks such as detecting moving objects, yet a complex task for modern computers. The detection of object motion is done by specialised retinal ganglion cells named Object-motion-sensitive ganglion cells (OMS-GC). OMS-GC process continuous signals and generate spike patterns that are post-processed by the Visual Cortex. The Neuromorphic Hybrid Spiking Motion Detector (NeuroHSMD) proposed in this work accelerates the HSMD algorithm using Field-Programmable Gate Arrays (FPGAs). The Hybrid Spiking Motion Detector (HSMD) algorithm was the first hybrid algorithm to enhance dynamic background subtraction (DBS) algorithms with a customised 3-layer spiking neural network (SNN) that generates OMS-GC spiking-like responses. The NeuroHSMD algorithm was compared against the HSMD algorithm, using the same 2012 change detection (CDnet2012) and 2014 change detection (CDnet2014) benchmark datasets. The results show that the NeuroHSMD has produced the same results as the HSMD algorithm in real-time without degradation of quality. Moreover, the NeuroHSMD proposed in this paper was completely implemented in Open Computer Language (OpenCL) and therefore is easily replicated in other devices such as Graphical Processor Units (GPUs) and clusters of Central Processor Units (CPUs). </details>
<details>	<summary>邮件日期</summary>	2021年12月14日</details>

# 300、随机STT-MRAM切换的同步无监督STDP学习
- [ ] Synchronous Unsupervised STDP Learning with Stochastic STT-MRAM Switching 
时间：2021年12月10日                         第一作者：Peng Zhou                       [链接](https://arxiv.org/abs/2112.05707).                     
## 摘要：在神经形态系统中，模拟电阻状态用于存储权重受到制造不精确性和限制突触权重精度的设备随机性的阻碍。这一挑战可以通过模拟自旋转移转矩磁阻随机存取存储器（STT-MRAM）二元状态的随机切换来解决。然而，以前基于STT-MRAM的方法是以异步方式运行的，很难通过实验实现。本文提出了一种利用STT-MRAM的随机切换进行无监督学习的带时钟电路的同步脉冲神经网络系统。建议的系统使单层网络能够在MNIST数据集上实现90%的推理精度。
<details>	<summary>英文摘要</summary>	The use of analog resistance states for storing weights in neuromorphic systems is impeded by fabrication imprecision and device stochasticity that limit the precision of synapse weights. This challenge can be resolved by emulating analog behavior with the stochastic switching of the binary states of spin-transfer torque magnetoresistive random-access memory (STT-MRAM). However, previous approaches based on STT-MRAM operate in an asynchronous manner that is difficult to implement experimentally. This paper proposes a synchronous spiking neural network system with clocked circuits that perform unsupervised learning leveraging the stochastic switching of STT-MRAM. The proposed system enables a single-layer network to achieve 90% inference accuracy on the MNIST dataset. </details>
<details>	<summary>邮件日期</summary>	2021年12月13日</details>

# 299、脉冲神经网络中的深度剩余学习
- [ ] Deep Residual Learning in Spiking Neural Networks 
时间：2021年12月07日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2102.04159).                     
<details>	<summary>注释</summary>	Accepted by Advances in Neural Information Processing Systems (NeurIPS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年12月08日</details>

# 298、混合SNN-ANN：基于事件视觉的节能分类和目标检测
- [ ] Hybrid SNN-ANN: Energy-Efficient Classification and Object Detection for Event-Based Vision 
时间：2021年12月06日                         第一作者：Alex                       [链接](https://arxiv.org/abs/2112.03423).                     
## 摘要：基于事件的视觉传感器对事件流而非图像帧中的局部像素亮度变化进行编码，并产生稀疏、节能的场景编码，此外还具有低延迟、高动态范围和缺少运动模糊的特点。基于事件的传感器在目标识别方面的最新进展来自使用反向传播训练的深层神经网络的转换。然而，将这些方法用于事件流需要转换为同步范式，这不仅会损失计算效率，而且还会错过提取时空特征的机会。在本文中，我们提出了一种用于基于事件的模式识别和目标检测的深度神经网络端到端训练的混合体系结构，结合了用于高效基于事件的特征提取的脉冲神经网络（SNN）主干，以及随后的模拟神经网络（ANN）头来解决同步分类和检测任务。这是通过将标准反向传播与代理梯度训练相结合来实现的，以通过SNN传播梯度。混合SNN ANN可以在不进行转换的情况下进行训练，并生成比ANN对应网络更具计算效率的高精度网络。我们展示了基于事件的分类和目标检测数据集的结果，其中只需要调整ANN头的结构以适应任务，并且不需要转换基于事件的输入。由于ANN和SNN需要不同的硬件模式来最大限度地提高其效率，我们设想SNN主干和ANN头部可以在不同的处理单元上执行，从而分析两部分之间通信所需的带宽。混合网络是一种很有前途的体系结构，可以在不影响效率的情况下进一步推进基于事件的视觉的机器学习方法。
<details>	<summary>英文摘要</summary>	Event-based vision sensors encode local pixel-wise brightness changes in streams of events rather than image frames and yield sparse, energy-efficient encodings of scenes, in addition to low latency, high dynamic range, and lack of motion blur. Recent progress in object recognition from event-based sensors has come from conversions of deep neural networks, trained with backpropagation. However, using these approaches for event streams requires a transformation to a synchronous paradigm, which not only loses computational efficiency, but also misses opportunities to extract spatio-temporal features. In this article we propose a hybrid architecture for end-to-end training of deep neural networks for event-based pattern recognition and object detection, combining a spiking neural network (SNN) backbone for efficient event-based feature extraction, and a subsequent analog neural network (ANN) head to solve synchronous classification and detection tasks. This is achieved by combining standard backpropagation with surrogate gradient training to propagate gradients through the SNN. Hybrid SNN-ANNs can be trained without conversion, and result in highly accurate networks that are substantially more computationally efficient than their ANN counterparts. We demonstrate results on event-based classification and object detection datasets, in which only the architecture of the ANN heads need to be adapted to the tasks, and no conversion of the event-based input is necessary. Since ANNs and SNNs require different hardware paradigms to maximize their efficiency, we envision that SNN backbone and ANN head can be executed on different processing units, and thus analyze the necessary bandwidth to communicate between the two parts. Hybrid networks are promising architectures to further advance machine learning approaches for event-based vision, without having to compromise on efficiency. </details>
<details>	<summary>注释</summary>	Accepted at DAGM German Conference on Pattern Recognition (GCPR 2021) </details>
<details>	<summary>邮件日期</summary>	2021年12月08日</details>

# 297、BS4NN：具有时间编码和学习的二值化脉冲神经网络
- [ ] BS4NN: Binarized Spiking Neural Networks with Temporal Coding and Learning 
时间：2021年12月06日                         第一作者：Saeed Reza Kheradpisheh                       [链接](https://arxiv.org/abs/2007.04039).                     
<details>	<summary>邮件日期</summary>	2021年12月07日</details>

# 296、基于代理的脉冲神经网络训练
- [ ] Spiking neural networks trained via proxy 
时间：2021年12月05日                         第一作者：Saeed Reza Kheradpisheh                       [链接](https://arxiv.org/abs/2109.13208).                     
<details>	<summary>邮件日期</summary>	2021年12月07日</details>

# 295、多阈值超低潜伏期脉冲神经网络的反向传播直接训练
- [ ] Direct Training via Backpropagation for Ultra-low Latency Spiking Neural Networks with Multi-threshold 
时间：2021年11月25日                         第一作者：Changqing Xu                       [链接](https://arxiv.org/abs/2112.07426).                     
## 摘要：脉冲神经网络（SNN）可以利用时空信息，具有能量效率的特性，是深度神经网络（DNN）的一个很好的替代方案。事件驱动的信息处理使得SNNs可以减少DNNs的昂贵计算量，节省大量的能量消耗。然而，较高的训练和推理延迟限制了更深层次SNN的发展。snn在训练和推理过程中通常需要数十甚至数百个时间步长，这不仅会增加延迟，而且会造成能量消耗的浪费。为了克服这个问题，我们提出了一种新的基于反向传播（BP）的多阈值超低延迟（1-2个时间步长）SNN训练方法。为了提高每个脉冲的信息容量，我们引入了多阈值泄漏集成和触发（LIF）模型。在我们提出的训练方法中，我们提出了三个近似的脉冲活动导数，以解决基于BP的SNN直接训练中存在的不可微问题。实验结果表明，我们提出的方法在MNIST、FashionMNIST和CIFAR10上的平均准确率分别为99.56%、93.08%和87.90%，只需2个时间步长。对于CIFAR10数据集，我们提出的方法比以前报道的直接训练SNN的精度提高了1.12%，时间步长更少。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) can utilize spatio-temporal information and have a nature of energy efficiency which is a good alternative to deep neural networks(DNNs). The event-driven information processing makes SNNs can reduce the expensive computation of DNNs and save a lot of energy consumption. However, high training and inference latency is a limitation of the development of deeper SNNs. SNNs usually need tens or even hundreds of time steps during the training and inference process which causes not only the increase of latency but also the waste of energy consumption. To overcome this problem, we proposed a novel training method based on backpropagation (BP) for ultra-low latency(1-2 time steps) SNN with multi-threshold. In order to increase the information capacity of each spike, we introduce the multi-threshold Leaky Integrate and Fired (LIF) model. In our proposed training method, we proposed three approximated derivative for spike activity to solve the problem of the non-differentiable issue which cause difficulties for direct training SNNs based on BP. The experimental results show that our proposed method achieves an average accuracy of 99.56%, 93.08%, and 87.90% on MNIST, FashionMNIST, and CIFAR10, respectively with only 2 time steps. For the CIFAR10 dataset, our proposed method achieve 1.12% accuracy improvement over the previously reported direct trained SNNs with fewer time steps. </details>
<details>	<summary>邮件日期</summary>	2021年12月15日</details>

# 294、Nengo上使用脉冲神经网络的稀疏分布记忆
- [ ] Sparse Distributed Memory using Spiking Neural Networks on Nengo 
时间：2021年12月03日                         第一作者：Rohan Deepak Ajwani                       [链接](https://arxiv.org/abs/2109.03111).                     
<details>	<summary>注释</summary>	8 pages, 11 figures, accepted as poster in Bernstein Conference 2021 ACM-class: H.3.2; I.5.5 </details>
<details>	<summary>邮件日期</summary>	2021年12月06日</details>

# 293、BioLCNet：报酬调制局部连接脉冲神经网络
- [ ] BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks 
时间：2021年12月03日                         第一作者：Hafez Ghaemi                       [链接](https://arxiv.org/abs/2109.05539).                     
<details>	<summary>注释</summary>	12 pages, 5 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2021年12月06日</details>

# 292、PrivateSNN：保护隐私的脉冲神经网络
- [ ] PrivateSNN: Privacy-Preserving Spiking Neural Networks 
时间：2021年12月02日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2104.03414).                     
<details>	<summary>注释</summary>	Accepted to AAAI2022 </details>
<details>	<summary>邮件日期</summary>	2021年12月03日</details>

# 291、立体画：用脉冲神经网络进行深度学习
- [ ] StereoSpike: Depth Learning with a Spiking Neural Network 
时间：2021年11月25日                         第一作者：Ulysse Ran\c{c}on                       [链接](https://arxiv.org/abs/2109.13751).                     
<details>	<summary>邮件日期</summary>	2021年11月29日</details>

# 290、脉冲神经形态硬件上量子基态的变分学习
- [ ] Variational learning of quantum ground states on spiking neuromorphic hardware 
时间：2021年11月25日                         第一作者：Robert Klassert                       [链接](https://arxiv.org/abs/2109.15169).                     
<details>	<summary>注释</summary>	14 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年11月29日</details>

# 289、基于信息瓶颈的Hebbian学习规则自然地将工作记忆和突触更新联系起来
- [ ] Information Bottleneck-Based Hebbian Learning Rule Naturally Ties Working Memory and Synaptic Updates 
时间：2021年11月24日                         第一作者：Kyle Daruwalla                        [链接](https://arxiv.org/abs/2111.13187).                     
## 摘要：人工神经网络通过反向传播训练极深的网络，成功地解决了各种各样的问题。反向传播直接应用于脉冲神经网络包含生物学上不可信的成分，如权重传递问题或独立的推理和学习阶段。各种方法分别处理不同的组件，但完整的解决方案仍然是无形的。这里，我们采用另一种方法，完全避免反向传播及其相关问题。最近在深度学习方面的工作建议通过信息瓶颈（IB）对网络的每一层进行独立培训。随后的研究指出，这种分层方法避免了错误在各层之间的传播，从而形成了一种生物学上合理的范式。不幸的是，IB是使用一批样本计算的。之前的工作通过仅使用两个样本（当前和以前的样本）的权重更新解决了这一问题。我们的工作采用不同的方法，将权重更新分解为局部和全局组件。局部分量是Hebbian分量，仅取决于当前样本。全局分量根据一批样本计算分层调制信号。我们证明了这种调制信号可以通过一个像储存器一样具有工作记忆（WM）的辅助电路来学习。因此，我们可以使用大于两个的批量大小，批量大小决定了WM所需的容量。据我们所知，我们的规则是第一个生物学上合理的机制，直接将突触更新与任务的WM相结合。我们在合成数据集和图像分类数据集（如MNIST）上评估了我们的规则，并探讨了WM能力对学习绩效的影响。我们希望我们的工作是理解记忆在学习中的机制作用的第一步。
<details>	<summary>英文摘要</summary>	Artificial neural networks have successfully tackled a large variety of problems by training extremely deep networks via back-propagation. A direct application of back-propagation to spiking neural networks contains biologically implausible components, like the weight transport problem or separate inference and learning phases. Various methods address different components individually, but a complete solution remains intangible. Here, we take an alternate approach that avoids back-propagation and its associated issues entirely. Recent work in deep learning proposed independently training each layer of a network via the information bottleneck (IB). Subsequent studies noted that this layer-wise approach circumvents error propagation across layers, leading to a biologically plausible paradigm. Unfortunately, the IB is computed using a batch of samples. The prior work addresses this with a weight update that only uses two samples (the current and previous sample). Our work takes a different approach by decomposing the weight update into a local and global component. The local component is Hebbian and only depends on the current sample. The global component computes a layer-wise modulatory signal that depends on a batch of samples. We show that this modulatory signal can be learned by an auxiliary circuit with working memory (WM) like a reservoir. Thus, we can use batch sizes greater than two, and the batch size determines the required capacity of the WM. To the best of our knowledge, our rule is the first biologically plausible mechanism to directly couple synaptic updates with a WM of the task. We evaluate our rule on synthetic datasets and image classification datasets like MNIST, and we explore the effect of the WM capacity on learning performance. We hope our work is a first-step towards understanding the mechanistic role of memory in learning. </details>
<details>	<summary>注释</summary>	21 pages, 10 figures, under review </details>
<details>	<summary>邮件日期</summary>	2021年11月29日</details>

# 288、实时智能车辆监控系统
- [ ] Real-time smart vehicle surveillance system 
时间：2021年11月24日                         第一作者：Shantha Kumar S                       [链接](https://arxiv.org/abs/2111.12289).                     
## 摘要：在过去十年中，全球犯罪活动激增。据印度警察局称，车辆盗窃是最难侦破的犯罪之一，所有记录在案的案件中有近19%与机动车盗窃有关。为了战胜这些对手，我们提出了一种实时车辆监控系统，该系统使用CCTV视频源检测和跟踪可疑车辆。该系统提取车辆的各种属性，如品牌、型号、颜色、车牌号和车牌类型。各种图像处理和深度学习算法的使用，以满足该系统的目标。提取的特征可以用作报告违法行为的证据。尽管系统使用了更多的参数，但它仍然能够以最小的延迟和精度损失进行实时预测。
<details>	<summary>英文摘要</summary>	Over the last decade, there has been a spike in criminal activity all around the globe. According to the Indian police department, vehicle theft is one of the least solved offenses, and almost 19% of all recorded cases are related to motor vehicle theft. To overcome these adversaries, we propose a real-time vehicle surveillance system, which detects and tracks the suspect vehicle using the CCTV video feed. The proposed system extracts various attributes of the vehicle such as Make, Model, Color, License plate number, and type of the license plate. Various image processing and deep learning algorithms are employed to meet the objectives of the proposed system. The extracted features can be used as evidence to report violations of law. Although the system uses more parameters, it is still able to make real time predictions with minimal latency and accuracy loss. </details>
<details>	<summary>邮件日期</summary>	2021年11月25日</details>

# 287、脉冲神经形态硬件上量子基态的变分学习
- [ ] Variational learning of quantum ground states on spiking neuromorphic hardware 
时间：2021年11月24日                         第一作者：Robert Klassert                       [链接](https://arxiv.org/abs/2109.15169).                     
<details>	<summary>注释</summary>	14 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年11月25日</details>

# 286、用于节能嵌入式神经形态计算的多核大小μ脑设计
- [ ] Design of Many-Core Big Little \mu Brain for Energy-Efficient Embedded Neuromorphic Computing 
时间：2021年11月23日                         第一作者：M. Lakshmi Varshika                       [链接](https://arxiv.org/abs/2111.11838).                     
## 摘要：随着嵌入式系统中基于脉冲的深度学习推理应用的增加，这些系统倾向于集成神经形态加速器，如$\mu$Brain，以提高能源效率。我们提出了一种基于$\mu$大脑的可扩展多核神经形态硬件设计，以加速脉冲深度卷积神经网络（SDCNN）的计算。为了提高能源效率，内核在神经元和突触容量方面被设计为异构的（大内核的容量比小内核高），并且它们使用并行分段总线互连进行互连，与传统的基于网格的片上网络（NoC）相比，这导致更低的延迟和能量。我们提出了一个名为SentryOS的系统软件框架，将SDCNN推理应用程序映射到所提出的设计。SentryOS由编译器和运行时管理器组成。编译器利用大小$\mu$脑内核的内部架构，将SDCNN应用程序编译成子网。运行时管理器将这些子网络调度到核心上，并通过管道将其执行以提高吞吐量。我们使用五种常用的SDCNN推理应用程序评估了提议的大-小-多核心神经形态设计和系统软件框架，结果表明，提议的解决方案降低了能量（37%到98%），减少了延迟（9%到25%），并提高了应用程序吞吐量（20%到36%）。我们还表明，SentryOS可以很容易地扩展到其他脉冲神经形态加速器。
<details>	<summary>英文摘要</summary>	As spiking-based deep learning inference applications are increasing in embedded systems, these systems tend to integrate neuromorphic accelerators such as $\mu$Brain to improve energy efficiency. We propose a $\mu$Brain-based scalable many-core neuromorphic hardware design to accelerate the computations of spiking deep convolutional neural networks (SDCNNs). To increase energy efficiency, cores are designed to be heterogeneous in terms of their neuron and synapse capacity (big cores have higher capacity than the little ones), and they are interconnected using a parallel segmented bus interconnect, which leads to lower latency and energy compared to a traditional mesh-based Network-on-Chip (NoC). We propose a system software framework called SentryOS to map SDCNN inference applications to the proposed design. SentryOS consists of a compiler and a run-time manager. The compiler compiles an SDCNN application into subnetworks by exploiting the internal architecture of big and little $\mu$Brain cores. The run-time manager schedules these sub-networks onto cores and pipeline their execution to improve throughput. We evaluate the proposed big little many-core neuromorphic design and the system software framework with five commonlyused SDCNN inference applications and show that the proposed solution reduces energy (between 37% and 98%), reduces latency (between 9% and 25%), and increases application throughput (between 20% and 36%). We also show that SentryOS can be easily extended for other spiking neuromorphic accelerators. </details>
<details>	<summary>注释</summary>	Accepted for publication at DATE 2022 </details>
<details>	<summary>邮件日期</summary>	2021年11月24日</details>

# 285、BioLCNet：报酬调制局部连接脉冲神经网络
- [ ] BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks 
时间：2021年11月23日                         第一作者：Hafez Ghaemi                       [链接](https://arxiv.org/abs/2109.05539).                     
<details>	<summary>注释</summary>	12 pages, 5 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2021年11月24日</details>

# 284、BioLeaF：一个用于训练脉冲神经网络的生物似然学习框架
- [ ] BioLeaF: A Bio-plausible Learning Framework for Training of Spiking Neural Networks 
时间：2021年11月14日                         第一作者：Yukun Yang                       [链接](https://arxiv.org/abs/2111.13188).                     
## 摘要：我们的大脑由生物神经元组成，这些神经元通过精确的脉冲计时编码信息，但我们大脑的结构和学习规则在很大程度上仍然未知。与最近发展的基于反向传播（BP）的方法相比，基于反向传播的方法能够以高精度训练脉冲神经网络（SNN），生物学上可行的方法仍处于初级阶段。在这项工作中，我们希望回答这样一个问题：是否有可能通过基于BP的规则和生物似是而非的机制来训练SNN，从而获得可比的准确性。我们提出了一个新的生物似是而非的学习框架，由两个部分组成：一个新的体系结构及其支持的学习规则。通过两种类型的细胞和四种类型的突触连接，所提出的局部微电路结构可以通过局部反馈连接计算和传播错误信号，并支持具有全局定义的脉冲错误函数的多层SNN的训练。在我们的微电路架构下，我们采用了在局部隔室中运行的脉冲时间依赖性可塑性（STDP）规则来更新突触权重，并以生物学上合理的方式实现监督学习。最后，我们从优化的角度对该框架进行了解释，并在特定情况下证明了它与基于BP的规则的等价性。我们的实验表明，所提出的框架显示了与基于BP的规则相当的学习精度，并可能为生物系统中如何协调学习提供新的见解。
<details>	<summary>英文摘要</summary>	Our brain consists of biological neurons encoding information through accurate spike timing, yet both the architecture and learning rules of our brain remain largely unknown. Comparing to the recent development of backpropagation-based (BP-based) methods that are able to train spiking neural networks (SNNs) with high accuracy, biologically plausible methods are still in their infancy. In this work, we wish to answer the question of whether it is possible to attain comparable accuracy of SNNs trained by BP-based rules with bio-plausible mechanisms. We propose a new bio-plausible learning framework, consisting of two components: a new architecture, and its supporting learning rules. With two types of cells and four types of synaptic connections, the proposed local microcircuit architecture can compute and propagate error signals through local feedback connections and support training of multi-layers SNNs with a globally defined spiking error function. Under our microcircuit architecture, we employ the Spike-Timing-Dependent-Plasticity (STDP) rule operating in local compartments to update synaptic weights and achieve supervised learning in a biologically plausible manner. Finally, We interpret the proposed framework from an optimization point of view and show the equivalence between it and the BP-based rules under a special circumstance. Our experiments show that the proposed framework demonstrates learning accuracy comparable to BP-based rules and may provide new insights on how learning is orchestrated in biological systems. </details>
<details>	<summary>邮件日期</summary>	2021年11月29日</details>

# 283、通过乘法突触的脉冲神经网络的非线性计算
- [ ] Nonlinear computations in spiking neural networks through multiplicative synapses 
时间：2021年11月22日                         第一作者：Michele Nardin                       [链接](https://arxiv.org/abs/2009.03857).                     
<details>	<summary>注释</summary>	This article has been peer-reviewed and recommended by Peer Community In Neuroscience Journal-ref: Peer Community In Neuroscience, 2021 DOI: 10.24072/pci.cneuro.100003 </details>
<details>	<summary>邮件日期</summary>	2021年11月23日</details>

# 282、E3NE：一种在FPGA上使用新兴神经编码加速脉冲神经网络的端到端框架
- [ ] E3NE: An End-to-End Framework for Accelerating Spiking Neural Networks with Emerging Neural Encoding on FPGAs 
时间：2021年11月19日                         第一作者：Daniel Gerlinghoff                       [链接](https://arxiv.org/abs/2111.10027).                     
## 摘要：编译器框架对于广泛使用基于FPGA的深度学习加速器至关重要。它们允许不熟悉硬件工程的研究人员和开发人员利用特定领域逻辑所获得的性能。传统的人工神经网络有多种框架。然而，没有太多的研究投入到为脉冲神经网络（SNN）优化的框架的创建上。这种新一代的神经网络对于在边缘设备上部署AI变得越来越有趣，因为边缘设备具有很强的功率和资源约束。我们的端到端框架E3NE自动化了FPGA高效SNN推理逻辑的生成。基于PyTorch模型和用户参数，它应用各种优化并评估基于spike加速器固有的权衡。多层次的并行性和新兴神经编码方案的使用使得效率优于以前的SNN硬件实现。对于类似型号，E3NE使用的硬件资源不到50%，功耗减少20%，同时将延迟降低了一个数量级。此外，可伸缩性和通用性允许部署大规模SNN模型AlexNet和VGG。
<details>	<summary>英文摘要</summary>	Compiler frameworks are crucial for the widespread use of FPGA-based deep learning accelerators. They allow researchers and developers, who are not familiar with hardware engineering, to harness the performance attained by domain-specific logic. There exists a variety of frameworks for conventional artificial neural networks. However, not much research effort has been put into the creation of frameworks optimized for spiking neural networks (SNNs). This new generation of neural networks becomes increasingly interesting for the deployment of AI on edge devices, which have tight power and resource constraints. Our end-to-end framework E3NE automates the generation of efficient SNN inference logic for FPGAs. Based on a PyTorch model and user parameters, it applies various optimizations and assesses trade-offs inherent to spike-based accelerators. Multiple levels of parallelism and the use of an emerging neural encoding scheme result in an efficiency superior to previous SNN hardware implementations. For a similar model, E3NE uses less than 50% of hardware resources and 20% less power, while reducing the latency by an order of magnitude. Furthermore, scalability and generality allowed the deployment of the large-scale SNN models AlexNet and VGG. </details>
<details>	<summary>注释</summary>	Accepted by IEEE Transactions on Parallel and Distributed Systems </details>
<details>	<summary>邮件日期</summary>	2021年11月22日</details>

# 281、用于神经形态信息处理的共振隧穿二极管纳米光电脉冲节点
- [ ] Resonant tunnelling diode nano-optoelectronic spiking nodes for neuromorphic information processing 
时间：2021年11月19日                         第一作者：Mat\v{e}j Hejda                       [链接](https://arxiv.org/abs/2107.06721).                     
<details>	<summary>注释</summary>	Updated with feedback from first round of reviews. Updated figure with 3D model </details>
<details>	<summary>邮件日期</summary>	2021年11月22日</details>

# 280、用局部规则训练的脉冲网络的连续学习
- [ ] Continuous learning of spiking networks trained with local rules 
时间：2021年11月18日                         第一作者：Dmitry Antonov                       [链接](https://arxiv.org/abs/2111.09553).                     
## 摘要：人工神经网络（ANN）在顺序学习过程中会经历灾难性遗忘（CF）。相反，大脑可以持续学习，而不会出现任何灾难性遗忘的迹象。脉冲神经网络（SNN）是下一代人工神经网络，它借鉴了生物神经网络的许多特性。因此，SNN有可能更好地抵抗CF。在本文中，我们研究了SNN对CF的易感性，并测试了几种受生物学启发的缓解灾难性遗忘的方法。SNN的训练采用基于脉冲时间依赖性可塑性（STDP）的生物学上合理的局部训练规则。当地培训禁止直接使用基于全局损失函数梯度的CF预防方法。我们开发并测试了基于随机朗之万动力学确定突触重要性（权重）的方法，无需梯度。此外，还测试了其他几种模拟神经网络的灾难性遗忘预防方法。这些实验是在SpykeTorch环境中的免费数据集上进行的。
<details>	<summary>英文摘要</summary>	Artificial neural networks (ANNs) experience catastrophic forgetting (CF) during sequential learning. In contrast, the brain can learn continuously without any signs of catastrophic forgetting. Spiking neural networks (SNNs) are the next generation of ANNs with many features borrowed from biological neural networks. Thus, SNNs potentially promise better resilience to CF. In this paper, we study the susceptibility of SNNs to CF and test several biologically inspired methods for mitigating catastrophic forgetting. SNNs are trained with biologically plausible local training rules based on spike-timing-dependent plasticity (STDP). Local training prohibits the direct use of CF prevention methods based on gradients of a global loss function. We developed and tested the method to determine the importance of synapses (weights) based on stochastic Langevin dynamics without the need for the gradients. Several other methods of catastrophic forgetting prevention adapted from analog neural networks were tested as well. The experiments were performed on freely available datasets in the SpykeTorch environment. </details>
<details>	<summary>注释</summary>	25 pages </details>
<details>	<summary>邮件日期</summary>	2021年11月19日</details>

# 279、转换脉冲神经网络的L4范数权重调整
- [ ] L4-Norm Weight Adjustments for Converted Spiking Neural Networks 
时间：2021年11月17日                         第一作者：Jason Allred                       [链接](https://arxiv.org/abs/2111.09446).                     
## 摘要：由于稀疏的、事件驱动的计算，脉冲神经网络（SNN）因其潜在的能源效率优势正在被探索。非脉冲人工神经网络通常采用反向传播的随机梯度下降法进行训练。脉冲神经元的不可微放电事件阻碍了反向传播真实梯度的计算。另一方面，使用近似梯度是有效的，但在许多时间步长上计算成本很高。因此，训练脉冲神经网络的一种常用技术是训练拓扑等价的非脉冲网络，然后将其转换为脉冲网络，用比例速率编码的泊松脉冲序列替换实值输入。转换后的SNN功能足够好，因为脉冲神经元的平均预放电膜电位与输入速率向量和神经元权重向量的点积成正比，类似于非脉冲网络的功能。然而，这种转换只考虑膜电位的平均值，而不考虑膜电位的时间变化。由于预放电膜电位的标准偏差与神经元权重向量的L4范数成正比，我们在转换过程中提出了基于L4范数的权重调整，以提高转换网络的分类精度。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) are being explored for their potential energy efficiency benefits due to sparse, event-driven computation. Non-spiking artificial neural networks are typically trained with stochastic gradient descent using backpropagation. The calculation of true gradients for backpropagation in spiking neural networks is impeded by the non-differentiable firing events of spiking neurons. On the other hand, using approximate gradients is effective, but computationally expensive over many time steps. One common technique, then, for training a spiking neural network is to train a topologically-equivalent non-spiking network, and then convert it to an spiking network, replacing real-valued inputs with proportionally rate-encoded Poisson spike trains. Converted SNNs function sufficiently well because the mean pre-firing membrane potential of a spiking neuron is proportional to the dot product of the input rate vector and the neuron weight vector, similar to the functionality of a non-spiking network. However, this conversion only considers the mean and not the temporal variance of the membrane potential. As the standard deviation of the pre-firing membrane potential is proportional to the L4-norm of the neuron weight vector, we propose a weight adjustment based on the L4-norm during the conversion process in order to improve classification accuracy of the converted network. </details>
<details>	<summary>邮件日期</summary>	2021年11月19日</details>

# 278、量子叠加激励的脉冲神经网络
- [ ] Quantum Superposition Inspired Spiking Neural Network 
时间：2021年11月17日                         第一作者：Yinqian Sun                       [链接](https://arxiv.org/abs/2010.12197).                     
<details>	<summary>邮件日期</summary>	2021年11月18日</details>

# 277、从卷积到脉冲：社区目前忽略的环境指标
- [ ] From Convolutions towards Spikes: The Environmental Metric that the Community currently Misses 
时间：2021年11月16日                         第一作者：Aviral Chharia                       [链接](https://arxiv.org/abs/2111.08361).                     
## 摘要：如今，人工智能社区痴迷于将“最先进的”分数（80%的论文在NeurIPS中）作为主要的性能指标，因此一个重要参数，即环境指标，仍然没有报告。十年前，计算能力是一个限制因素；然而，在可预见的未来环境中，挑战将是开发环境友好且节能的算法。人类的大脑已经自我优化了将近一百万年，消耗的能量与典型的笔记本电脑一样。因此，开发受自然启发的算法是一种解决方案。在这项研究中，我们发现目前使用的人工神经网络并不是我们在自然界中发现的，这也是为什么尽管性能较低，但反映哺乳动物视觉皮层的脉冲神经网络却吸引了很多人的兴趣。我们进一步强调限制研究人员使用基于峰值的计算大规模开发神经形态节能微芯片的硬件缺口。使用神经形态处理器代替传统的GPU可能更环保、更高效。这些处理器将使SNN成为解决该问题的理想方案。本文对当前的差距、比较研究的缺乏进行了深入的关注，同时在神经科学和深度学习这两个领域的交叉点提出了新的研究方向。此外，我们定义了一个新的评估指标“自然”，用于报告人工智能模型的碳足迹。
<details>	<summary>英文摘要</summary>	Today, the AI community is obsessed with 'state-of-the-art' scores (80% papers in NeurIPS) as the major performance metrics, due to which an important parameter, i.e., the environmental metric, remains unreported. Computational capabilities were a limiting factor a decade ago; however, in foreseeable future circumstances, the challenge will be to develop environment-friendly and power-efficient algorithms. The human brain, which has been optimizing itself for almost a million years, consumes the same amount of power as a typical laptop. Therefore, developing nature-inspired algorithms is one solution to it. In this study, we show that currently used ANNs are not what we find in nature, and why, although having lower performance, spiking neural networks, which mirror the mammalian visual cortex, have attracted much interest. We further highlight the hardware gaps restricting the researchers from using spike-based computation for developing neuromorphic energy-efficient microchips on a large scale. Using neuromorphic processors instead of traditional GPUs might be more environment friendly and efficient. These processors will turn SNNs into an ideal solution for the problem. This paper presents in-depth attention highlighting the current gaps, the lack of comparative research, while proposing new research directions at the intersection of two fields -- neuroscience and deep learning. Further, we define a new evaluation metric 'NATURE' for reporting the carbon footprint of AI models. </details>
<details>	<summary>注释</summary>	NeurIPS 2021 Human-Centered AI Workshop </details>
<details>	<summary>邮件日期</summary>	2021年11月17日</details>

# 276、Spiking CapsNet：一种在胶囊之间具有生物学上合理的路由规则的Spiking神经网络
- [ ] Spiking CapsNet: A Spiking Neural Network With A Biologically Plausible Routing Rule Between Capsules 
时间：2021年11月15日                         第一作者：Dongcheng Zhao                       [链接](https://arxiv.org/abs/2111.07785).                     
## 摘要：脉冲神经网络（SNN）以其强大的时空信息表示能力而备受关注。胶囊神经网络（CapsNet）在不同层次上具有良好的组装和耦合特性。在这里，我们通过将胶囊引入到脉冲神经网络的建模中，提出了脉冲CapsNet。此外，我们还提出了一种更具生物学意义的依赖于脉冲时间的可塑性路由机制。通过充分考虑低电平脉冲胶囊和高电平脉冲胶囊之间的时空关系，进一步提高了它们之间的耦合能力。我们已经在MNIST和FashionMNIST数据集上验证了实验。与其他优秀的SNN模型相比，我们的算法仍然具有较高的性能。我们的脉冲CapsNet充分结合了SNN和CapsNet的优点，对噪声和仿射变换具有很强的鲁棒性。通过向测试数据集添加不同的椒盐噪声和高斯噪声，实验结果表明，当噪声较大时，我们的峰值CapsNet表现出更稳健的性能，而人工神经网络不能正确地进行澄清。此外，我们的脉冲CapsNet对AffNIST数据集上的仿射变换具有很强的泛化能力。
<details>	<summary>英文摘要</summary>	Spiking neural network (SNN) has attracted much attention due to their powerful spatio-temporal information representation ability. Capsule Neural Network (CapsNet) does well in assembling and coupling features at different levels. Here, we propose Spiking CapsNet by introducing the capsules into the modelling of spiking neural networks. In addition, we propose a more biologically plausible Spike Timing Dependent Plasticity routing mechanism. By fully considering the spatio-temporal relationship between the low-level spiking capsules and the high-level spiking capsules, the coupling ability between them is further improved. We have verified experiments on the MNIST and FashionMNIST datasets. Compared with other excellent SNN models, our algorithm still achieves high performance. Our Spiking CapsNet fully combines the strengthens of SNN and CapsNet, and shows strong robustness to noise and affine transformation. By adding different Salt-Pepper and Gaussian noise to the test dataset, the experimental results demonstrate that our Spiking CapsNet shows a more robust performance when there is more noise, while the artificial neural network can not correctly clarify. As well, our Spiking CapsNet shows strong generalization to affine transformation on the AffNIST dataset. </details>
<details>	<summary>邮件日期</summary>	2021年11月16日</details>

# 275、非监督学习优化的脉冲神经元突触可塑性模型
- [ ] A Spiking Neuron Synaptic Plasticity Model Optimized for Unsupervised Learning 
时间：2021年11月12日                         第一作者：Mikhail Kiselev                       [链接](https://arxiv.org/abs/2111.06768).                     
## 摘要：脉冲神经网络（SNN）被认为是执行各种学习任务（无监督、有监督和强化学习）的基础。SNN中的学习是通过突触可塑性实现的，突触可塑性规则决定突触重量的动态，通常取决于突触前和突触后神经元的活动。不同学习机制的多样性假设不同形式的突触可塑性可能对无监督和有监督学习最有效，因为在活体神经元中观察到，与基本的棘波时间依赖可塑性（STDP）模型存在多种偏差。在本论文中，我们制定了无监督学习问题对塑性规则的具体要求，并构建了一个新的塑性模型来推广STDP并满足这些要求。这种可塑性模型是本文提出的一种新的监督学习算法SCoBUL（Spike-Correlation-Based Unsupervised learning）的主要逻辑组成部分。我们还提供了计算机模拟实验的结果，证实了这些突触可塑性规则和SCoBUL算法的有效性。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNN) are considered as a perspective basis for performing all kinds of learning tasks - unsupervised, supervised and reinforcement learning. Learning in SNN is implemented through synaptic plasticity - the rules which determine dynamics of synaptic weights depending usually on activity of the pre- and post-synaptic neurons. Diversity of various learning regimes assumes that different forms of synaptic plasticity may be most efficient for, for example, unsupervised and supervised learning, as it is observed in living neurons demonstrating many kinds of deviations from the basic spike timing dependent plasticity (STDP) model. In the present paper, we formulate specific requirements to plasticity rules imposed by unsupervised learning problems and construct a novel plasticity model generalizing STDP and satisfying these requirements. This plasticity model serves as main logical component of the novel supervised learning algorithm called SCoBUL (Spike Correlation Based Unsupervised Learning) proposed in this work. We also present the results of computer simulation experiments confirming efficiency of these synaptic plasticity rules and the algorithm SCoBUL. </details>
<details>	<summary>邮件日期</summary>	2021年11月15日</details>

# 274、基于STDP的事件数据无监督Spiking实例分割
- [ ] Unsupervised Spiking Instance Segmentation on Event Data using STDP 
时间：2021年11月12日                         第一作者：Paul Kirkl                       [链接](https://arxiv.org/abs/2111.05283).                     
<details>	<summary>注释</summary>	20 Pages, 13 Figures </details>
<details>	<summary>邮件日期</summary>	2021年11月15日</details>

# 273、利用剩余脉冲神经网络进行精确特征提取的关键
- [ ] Keys to Accurate Feature Extraction Using Residual Spiking Neural Networks 
时间：2021年11月12日                         第一作者：Alex Vicente-Sola                       [链接](https://arxiv.org/abs/2111.05955).                     
<details>	<summary>注释</summary>	13 pages, 5 figures, 14 tables ACM-class: I.2.6; I.2.10; I.4.8; I.5.2; D.2.13 </details>
<details>	<summary>邮件日期</summary>	2021年11月15日</details>

# 272、利用剩余脉冲神经网络进行精确特征提取的关键
- [ ] Keys to Accurate Feature Extraction Using Residual Spiking Neural Networks 
时间：2021年11月10日                         第一作者：Alex Vicente-Sola (1)                       [链接](https://arxiv.org/abs/2111.05955).                     
## 摘要：脉冲神经网络（SNN）已成为传统人工神经网络（ANN）的一种有趣的替代方案，这得益于其时间处理能力以及在神经形态硬件中的低交换（大小、重量和功率）和节能实现。然而，训练SNN所涉及的挑战限制了它们在准确性方面的性能，从而限制了它们的应用。因此，改进学习算法和神经网络结构以获得更精确的特征提取是当前SNN研究的重点之一。在这篇文章中，我们对现代扣球体系结构的关键组件进行了研究。我们对从性能最好的网络中获取的图像分类数据集中的不同技术进行了经验比较。我们设计了一个成功剩余网络（ResNet）体系结构的脉冲版本，并在其上测试了不同的组件和训练策略。我们的研究结果为SNN设计提供了最先进的指导，在尝试构建最佳视觉特征提取器时，可以做出明智的选择。最后，我们的网络在CIFAR-10（94.1%）和CIFAR-100（74.5%）数据集中的性能优于以前的SNN体系结构，并与DVS-CIFAR10（71.3%）中的最新技术相匹配，参数比以前的最新技术更少，并且不需要ANN-SNN转换。代码可在https://github.com/VicenteAlex/Spiking_ResNet.
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have become an interesting alternative to conventional artificial neural networks (ANN) thanks to their temporal processing capabilities and their low-SWaP (Size, Weight, and Power) and energy efficient implementations in neuromorphic hardware. However the challenges involved in training SNNs have limited their performance in terms of accuracy and thus their applications. Improving learning algorithms and neural architectures for a more accurate feature extraction is therefore one of the current priorities in SNN research. In this paper we present a study on the key components of modern spiking architectures. We empirically compare different techniques in image classification datasets taken from the best performing networks. We design a spiking version of the successful residual network (ResNet) architecture and test different components and training strategies on it. Our results provide a state of the art guide to SNN design, which allows to make informed choices when trying to build the optimal visual feature extractor. Finally, our network outperforms previous SNN architectures in CIFAR-10 (94.1%) and CIFAR-100 (74.5%) datasets and matches the state of the art in DVS-CIFAR10 (71.3%), with less parameters than the previous state of the art and without the need for ANN-SNN conversion. Code available at https://github.com/VicenteAlex/Spiking_ResNet. </details>
<details>	<summary>注释</summary>	13 pages, 5 figures, 14 tables ACM-class: I.2.6; I.2.10; I.4.8; I.5.2; D.2.13 </details>
<details>	<summary>邮件日期</summary>	2021年11月12日</details>

# 271、从头开始训练低潜伏期深脉冲神经网络的批标准化方法
- [ ] Revisiting Batch Normalization for Training Low-latency Deep Spiking Neural Networks from Scratch 
时间：2021年11月10日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2010.01729).                     
<details>	<summary>注释</summary>	Accepted to Frontiers in Neuroscience (2021) </details>
<details>	<summary>邮件日期</summary>	2021年11月12日</details>

# 270、基于STDP的事件数据无监督Spiking实例分割
- [ ] Unsupervised Spiking Instance Segmentation on Event Data using STDP 
时间：2021年11月09日                         第一作者：Paul Kirkl                       [链接](https://arxiv.org/abs/2111.05283).                     
## 摘要：脉冲神经网络（SNN）和神经形态工程领域带来了如何处理机器学习（ML）和计算机视觉（CV）问题的范式转变。这种范式的转变来自基于事件的感知和处理的适应。基于事件的视觉传感器允许生成与场景动态相关的稀疏和异步事件。不仅可以捕获空间信息，还可以捕获高保真的时间信息。同时避免了传统高帧速率方法的额外开销和冗余。然而，随着范式的变化，传统的CV和ML中的许多技术都不适用于这些基于事件的时空视觉流。因此，存在数量有限的识别、检测和分割方法。在本文中，我们提出了一种新的方法，只需使用训练用于对象识别的脉冲时间相关可塑性训练脉冲卷积神经网络的权值即可执行实例分割。这利用了网络内部特征表示的空间和时间方面，增加了这种新的鉴别能力。我们通过成功地将用于人脸检测的单类无监督网络转换为多人人脸识别和实例分割网络，突出了这一新功能。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNN) and the field of Neuromorphic Engineering has brought about a paradigm shift in how to approach Machine Learning (ML) and Computer Vision (CV) problem. This paradigm shift comes from the adaption of event-based sensing and processing. An event-based vision sensor allows for sparse and asynchronous events to be produced that are dynamically related to the scene. Allowing not only the spatial information but a high-fidelity of temporal information to be captured. Meanwhile avoiding the extra overhead and redundancy of conventional high frame rate approaches. However, with this change in paradigm, many techniques from traditional CV and ML are not applicable to these event-based spatial-temporal visual streams. As such a limited number of recognition, detection and segmentation approaches exist. In this paper, we present a novel approach that can perform instance segmentation using just the weights of a Spike Time Dependent Plasticity trained Spiking Convolutional Neural Network that was trained for object recognition. This exploits the spatial and temporal aspects of the network's internal feature representations adding this new discriminative capability. We highlight the new capability by successfully transforming a single class unsupervised network for face detection into a multi-person face recognition and instance segmentation network. </details>
<details>	<summary>注释</summary>	20 Pages, 13 Figures </details>
<details>	<summary>邮件日期</summary>	2021年11月10日</details>

# 269、稳定的终身学习：用脉冲神经元解决塑性神经网络的不稳定性
- [ ] Stable Lifelong Learning: Spiking neurons as a solution to instability in plastic neural networks 
时间：2021年11月07日                         第一作者：Samuel Schmidgall                       [链接](https://arxiv.org/abs/2111.04113).                     
## 摘要：突触可塑性是神经网络中自我调节的无监督学习的有力方法。最近，人们对利用人工神经网络（ANN）和突触可塑性进行终生学习的兴趣又重新兴起。可塑性已被证明可以提高这些网络的学习能力，从而推广到新的环境环境中。然而，这些经过训练的网络的长期稳定性尚未得到检验。这项工作表明，将可塑性与人工神经网络结合使用会导致不稳定性超过训练期间使用的预定寿命。这种不稳定性可能导致寻求回报行为的急剧下降，或迅速导致达到环境终端状态。这种行为被证明在两种不同的环境下，在许多训练时间范围内，对几种塑性规则是一致的：手推车杆平衡问题和四足运动问题。我们通过使用脉冲神经元来解决这种不稳定性。
<details>	<summary>英文摘要</summary>	Synaptic plasticity poses itself as a powerful method of self-regulated unsupervised learning in neural networks. A recent resurgence of interest has developed in utilizing Artificial Neural Networks (ANNs) together with synaptic plasticity for intra-lifetime learning. Plasticity has been shown to improve the learning capabilities of these networks in generalizing to novel environmental circumstances. However, the long-term stability of these trained networks has yet to be examined. This work demonstrates that utilizing plasticity together with ANNs leads to instability beyond the pre-specified lifespan used during training. This instability can lead to the dramatic decline of reward seeking behavior, or quickly lead to reaching environment terminal states. This behavior is shown to hold consistent for several plasticity rules on two different environments across many training time-horizons: a cart-pole balancing problem and a quadrupedal locomotion problem. We present a solution to this instability through the use of spiking neurons. </details>
<details>	<summary>邮件日期</summary>	2021年11月09日</details>

# 268、一种用于人工智能的长短时记忆在基于脉冲的神经形态硬件中的应用
- [ ] A Long Short-Term Memory for AI Applications in Spike-based Neuromorphic Hardware 
时间：2021年11月07日                         第一作者：Philipp Plank                       [链接](https://arxiv.org/abs/2107.03992).                     
<details>	<summary>注释</summary>	Philipp Plank and Arjun Rao have contributed equally to this work as first authors </details>
<details>	<summary>邮件日期</summary>	2021年11月09日</details>

# 267、WaveSense：用于关键词识别的带脉冲神经网络的高效时间卷积
- [ ] WaveSense: Efficient Temporal Convolutions with Spiking Neural Networks for Keyword Spotting 
时间：2021年11月02日                         第一作者：Philipp Weidel                       [链接](https://arxiv.org/abs/2111.01456).                     
## 摘要：超低功耗局部信号处理是常开设备边缘应用的一个重要方面。模拟脉冲神经网络的神经形态处理器显示出强大的计算能力，同时满足该领域所需的有限功率预算。在这项工作中，我们提出脉冲神经动力学作为扩张的时间卷积的自然替代。我们将这一想法扩展到WaveSense，这是一种受WaveNet架构启发的脉冲神经网络。WaveSense使用简单的神经动力学、固定的时间常数和简单的前馈结构，因此特别适合于神经形态实现。我们在几个数据集上测试了该模型的关键字识别能力。结果表明，所提出的网络优于其他脉冲神经网络，达到了CNN和LSTMs等人工神经网络的最新性能。
<details>	<summary>英文摘要</summary>	Ultra-low power local signal processing is a crucial aspect for edge applications on always-on devices. Neuromorphic processors emulating spiking neural networks show great computational power while fulfilling the limited power budget as needed in this domain. In this work we propose spiking neural dynamics as a natural alternative to dilated temporal convolutions. We extend this idea to WaveSense, a spiking neural network inspired by the WaveNet architecture. WaveSense uses simple neural dynamics, fixed time-constants and a simple feed-forward architecture and hence is particularly well suited for a neuromorphic implementation. We test the capabilities of this model on several datasets for keyword-spotting. The results show that the proposed network beats the state of the art of other spiking neural networks and reaches near state-of-the-art performance of artificial neural networks such as CNNs and LSTMs. </details>
<details>	<summary>邮件日期</summary>	2021年11月03日</details>

# 266、带神经网络鉴别器的脉冲生成对抗网络：局部训练、贝叶斯模型和持续元学习
- [ ] Spiking Generative Adversarial Networks With a Neural Network Discriminator: Local Training, Bayesian Models, and Continual Meta-Learning 
时间：2021年11月02日                         第一作者：Bleema Rosenfeld                       [链接](https://arxiv.org/abs/2111.01750).                     
## 摘要：神经形态数据以由脉冲编码的时空模式携带信息。因此，神经形态计算中的一个中心问题是训练脉冲神经网络（SNN）以再现响应给定脉冲刺激的时空脉冲模式。大多数现有方法通过将每个输入分配给特定的期望输出脉冲序列，以确定性方式对SNN的输入-输出行为进行建模。相比之下，为了充分利用脉冲的时间编码能力，本工作建议训练SNN以匹配脉冲信号的分布，而不是单个脉冲信号。为此，本文介绍了一种新的混合结构，包括通过SNN实现的条件生成器和通过常规人工神经网络（ANN）实现的鉴别器。ANN的作用是在训练期间，按照生成性对抗网络（GANs）的原则，在对抗性迭代学习策略中向SNN提供反馈。为了更好地捕捉多模态时空分布，所提出的方法（称为SpikeGAN）被进一步扩展以支持生成器权重的贝叶斯学习。最后，通过提出Spikgan的在线元学习变体，解决了具有时变统计的设置问题。与基于（静态）信念网络和最大似然（或经验风险最小化）的现有解决方案相比，实验深入了解了所提出方法的优点。
<details>	<summary>英文摘要</summary>	Neuromorphic data carries information in spatio-temporal patterns encoded by spikes. Accordingly, a central problem in neuromorphic computing is training spiking neural networks (SNNs) to reproduce spatio-temporal spiking patterns in response to given spiking stimuli. Most existing approaches model the input-output behavior of an SNN in a deterministic fashion by assigning each input to a specific desired output spiking sequence. In contrast, in order to fully leverage the time-encoding capacity of spikes, this work proposes to train SNNs so as to match distributions of spiking signals rather than individual spiking signals. To this end, the paper introduces a novel hybrid architecture comprising a conditional generator, implemented via an SNN, and a discriminator, implemented by a conventional artificial neural network (ANN). The role of the ANN is to provide feedback during training to the SNN within an adversarial iterative learning strategy that follows the principle of generative adversarial network (GANs). In order to better capture multi-modal spatio-temporal distribution, the proposed approach -- termed SpikeGAN -- is further extended to support Bayesian learning of the generator's weight. Finally, settings with time-varying statistics are addressed by proposing an online meta-learning variant of SpikeGAN. Experiments bring insights into the merits of the proposed approach as compared to existing solutions based on (static) belief networks and maximum likelihood (or empirical risk minimization). </details>
<details>	<summary>邮件日期</summary>	2021年11月03日</details>

# 265、通过局部突触可塑性学习基于事件的时空特征描述符：计算机视觉的生物学现实视角
- [ ] Learning Event-based Spatio-Temporal Feature Descriptors via Local Synaptic Plasticity: A Biologically-realistic Perspective of Computer Vision 
时间：2021年11月01日                         第一作者：Ali Safa                       [链接](https://arxiv.org/abs/2111.00791).                     
## 摘要：我们提出了一个基于优化的理论，描述了在视觉皮层中经验观察到的具有棘波时间依赖性可塑性（STDP）学习的棘波皮层集合。使用我们的方法，我们为基于事件的摄像机构建了一类完全连接、卷积和基于动作的特征描述符，我们分别在N-MNIST、挑战性CIFAR10-DVS和IBM DVS128手势数据集上对其进行评估。与传统的基于事件的最先进特征描述符相比，我们报告了显著的准确性改进（在CIFAR10-DVS上为+8%）。我们报告，与最先进的基于STDP的系统相比，精确度有了很大提高（N-MNIST为+10%，IBM DVS128为+7.74%）。除了在神经形态边缘设备中进行超低功耗学习外，我们的工作还有助于为实现生物现实、基于优化的皮层视觉理论铺平道路。
<details>	<summary>英文摘要</summary>	We present an optimization-based theory describing spiking cortical ensembles equipped with Spike-Timing-Dependent Plasticity (STDP) learning, as empirically observed in the visual cortex. Using our methods, we build a class of fully-connected, convolutional and action-based feature descriptors for event-based camera that we respectively assess on N-MNIST, challenging CIFAR10-DVS and on the IBM DVS128 gesture dataset. We report significant accuracy improvements compared to conventional state-of-the-art event-based feature descriptors (+8% on CIFAR10-DVS). We report large improvements in accuracy compared to state-of-the-art STDP-based systems (+10% on N-MNIST, +7.74% on IBM DVS128 Gesture). In addition to ultra-low-power learning in neuromorphic edge devices, our work helps paving the way towards a biologically-realistic, optimization-based theory of cortical vision. </details>
<details>	<summary>邮件日期</summary>	2021年11月02日</details>

# 264、BioGrad：基于生物似有理梯度的脉冲神经网络学习
- [ ] BioGrad: Biologically Plausible Gradient-Based Learning for Spiking Neural Networks 
时间：2021年10月27日                         第一作者：Guangzhi Tang                       [链接](https://arxiv.org/abs/2110.14092).                     
## 摘要：在新兴的神经形态芯片的推动下，脉冲神经网络（SNN）正在为人工智能问题提供节能、大规模并行和低延迟的解决方案。为了利用这些计算优势，SNN需要通过遵循大脑启发的神经形态原理的学习算法进行训练，即基于事件、局部和在线计算。然而，最先进的SNN训练算法是基于backprop的，并不遵循上述原则。由于其有限的生物学合理性，将backprop应用于SNN需要非局部反馈路径来传输连续值误差，并且依赖于未来时间步长的梯度。对backprop引入生物学上合理的修改有助于克服其一些局限性，但限制了backprop的近似程度，从而妨碍了其性能。我们为SNN提出了一个生物学上合理的基于梯度的学习算法，该算法在功能上等同于backprop，同时遵守所有三个神经形态原则。我们引入了具有局部资格跟踪的多室脉冲神经元来计算学习所需的梯度，并引入了一个周期性的“睡眠”阶段来进一步改进对backprop的近似，在此期间，局部Hebbian规则将反馈和前馈权重对齐。我们的方法在MNIST（98.13%）和基于事件的N-MNIST（97.59%）数据集上实现了与backprop相同的性能水平。我们在Intel的Loihi上部署了我们的学习算法来为MNIST训练一个1隐藏层网络，获得了93.32%的测试精度，同时每个训练样本消耗的能量比GPU上的BioGrad少400倍。我们的工作表明，最佳学习在神经形态计算中是可行的，进一步追求其生物学合理性可以更好地抓住这一新兴计算范式的好处。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNN) are delivering energy-efficient, massively parallel, and low-latency solutions to AI problems, facilitated by the emerging neuromorphic chips. To harness these computational benefits, SNN need to be trained by learning algorithms that adhere to brain-inspired neuromorphic principles, namely event-based, local, and online computations. Yet, the state-of-the-art SNN training algorithms are based on backprop that does not follow the above principles. Due to its limited biological plausibility, the application of backprop to SNN requires non-local feedback pathways for transmitting continuous-valued errors, and relies on gradients from future timesteps. The introduction of biologically plausible modifications to backprop has helped overcome several of its limitations, but limits the degree to which backprop is approximated, which hinders its performance. We propose a biologically plausible gradient-based learning algorithm for SNN that is functionally equivalent to backprop, while adhering to all three neuromorphic principles. We introduced multi-compartment spiking neurons with local eligibility traces to compute the gradients required for learning, and a periodic "sleep" phase to further improve the approximation to backprop during which a local Hebbian rule aligns the feedback and feedforward weights. Our method achieved the same level of performance as backprop with multi-layer fully connected SNN on MNIST (98.13%) and the event-based N-MNIST (97.59%) datasets. We deployed our learning algorithm on Intel's Loihi to train a 1-hidden-layer network for MNIST, and obtained 93.32% test accuracy while consuming 400 times less energy per training sample than BioGrad on GPU. Our work shows that optimal learning is feasible in neuromorphic computing, and further pursuing its biological plausibility can better capture the benefits of this emerging computing paradigm. </details>
<details>	<summary>注释</summary>	14 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年10月28日</details>

# 263、利用星形胶质细胞调制塑性组织的混沌动力学边缘提高液体状态机性能
- [ ] Increasing Liquid State Machine Performance with Edge-of-Chaos Dynamics Organized by Astrocyte-modulated Plasticity 
时间：2021年10月26日                         第一作者：Vladimir A. Ivanov                       [链接](https://arxiv.org/abs/2111.01760).                     
## 摘要：液体状态机（LSM）结合了低训练复杂度和生物合理性，这使得它成为边缘和神经形态计算范式的一个有吸引力的机器学习框架。LSM最初是作为大脑计算模型提出的，它调整其内部权值时没有梯度的反向传播，这导致与多层神经网络相比性能较低。神经科学的最新发现表明，星形胶质细胞，一种长期被忽视的非神经性脑细胞，调节突触可塑性和大脑动力学，将大脑网络调整到有序和混沌之间计算最优临界相变附近。受这种对大脑网络如何自我调节的颠覆性理解的启发，我们提出了神经元-星形胶质细胞液体状态机（NALSM），它通过自组织的近临界动力学来解决性能低下的问题。与生物模型类似，星形胶质细胞模型整合了神经元活动，并向棘波时间依赖性可塑性（STDP）提供全局反馈，STDP围绕与混沌边缘相关的临界分支因子自组织NALSM动力学。我们证明，与可比的LSM方法相比，NALSM实现了最先进的精度，而不需要数据特定的手动调整。NALSM在MNIST、N-MNIST和Fashion MNIST上的最高准确率分别为97.61%、97.51%和85.84%，与当前通过反向传播训练的全连接多层脉冲神经网络相比，NALSM取得了相当的性能。我们的研究结果表明，大脑启发的机器学习方法的进一步发展有可能达到深度学习的性能，并在边缘支持鲁棒和节能的神经形态计算。
<details>	<summary>英文摘要</summary>	The liquid state machine (LSM) combines low training complexity and biological plausibility, which has made it an attractive machine learning framework for edge and neuromorphic computing paradigms. Originally proposed as a model of brain computation, the LSM tunes its internal weights without backpropagation of gradients, which results in lower performance compared to multi-layer neural networks. Recent findings in neuroscience suggest that astrocytes, a long-neglected non-neuronal brain cell, modulate synaptic plasticity and brain dynamics, tuning brain networks to the vicinity of the computationally optimal critical phase transition between order and chaos. Inspired by this disruptive understanding of how brain networks self-tune, we propose the neuron-astrocyte liquid state machine (NALSM) that addresses under-performance through self-organized near-critical dynamics. Similar to its biological counterpart, the astrocyte model integrates neuronal activity and provides global feedback to spike-timing-dependent plasticity (STDP), which self-organizes NALSM dynamics around a critical branching factor that is associated with the edge-of-chaos. We demonstrate that NALSM achieves state-of-the-art accuracy versus comparable LSM methods, without the need for data-specific hand-tuning. With a top accuracy of 97.61% on MNIST, 97.51% on N-MNIST, and 85.84% on Fashion-MNIST, NALSM achieved comparable performance to current fully-connected multi-layer spiking neural networks trained via backpropagation. Our findings suggest that the further development of brain-inspired machine learning methods has the potential to reach the performance of deep learning, with the added benefits of supporting robust and energy-efficient neuromorphic computing on the edge. </details>
<details>	<summary>注释</summary>	23 pages, 9 figures, NeurIPS 2021 Journal-ref: 35th Conference on Neural Information Processing Systems (NeurIPS 2021) </details>
<details>	<summary>邮件日期</summary>	2021年11月03日</details>

# 262、基于事件的脉冲神经网络光流自监督学习
- [ ] Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks 
时间：2021年10月25日                         第一作者：Jesse Hagenaars                       [链接](https://arxiv.org/abs/2106.01862).                     
<details>	<summary>注释</summary>	Accepted at NeurIPS 2021; code and additional material can be found at https://mavlab.tudelft.nl/event_flow/ </details>
<details>	<summary>邮件日期</summary>	2021年10月27日</details>

# 261、脉冲神经网络中的深度剩余学习
- [ ] Deep Residual Learning in Spiking Neural Networks 
时间：2021年10月25日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2102.04159).                     
<details>	<summary>注释</summary>	Accepted by Advances in Neural Information Processing Systems (NeurIPS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年10月26日</details>

# 260、脉冲神经形态芯片学习纠缠量子态
- [ ] Spiking neuromorphic chip learns entangled quantum states 
时间：2021年10月25日                         第一作者：Stefanie Czischek                       [链接](https://arxiv.org/abs/2008.01039).                     
<details>	<summary>注释</summary>	9+13 pages, 4+2 figures; Submission to SciPost </details>
<details>	<summary>邮件日期</summary>	2021年10月26日</details>

# 259、平衡态隐式微分训练反馈脉冲神经网络
- [ ] Training Feedback Spiking Neural Networks by Implicit Differentiation on the Equilibrium State 
时间：2021年10月24日                         第一作者：Mingqing Xiao                       [链接](https://arxiv.org/abs/2109.14247).                     
<details>	<summary>注释</summary>	Accepted by NeurIPS 2021 (Spotlight) </details>
<details>	<summary>邮件日期</summary>	2021年10月26日</details>

# 258、ES ImageNet：一个用于脉冲神经网络的百万事件流分类数据集
- [ ] ES-ImageNet: A Million Event-Stream Classification Dataset for Spiking Neural Networks 
时间：2021年10月23日                         第一作者：Yihan Lin                       [链接](https://arxiv.org/abs/2110.12211).                     
## 摘要：随着事件驱动算法，特别是脉冲神经网络（SNN）在神经形态视觉处理方面的不断改进，迫切需要一个更具挑战性的事件流数据集。然而，众所周知，使用像动态视觉传感器（DVS）这样的神经形态摄像机创建ES数据集既耗时又昂贵。在这项工作中，我们提出了一种称为全向离散梯度（ODG）的快速有效算法，将流行的计算机视觉数据集ILSVRC2012转换为其事件流（ES）版本，将大约1300000帧图像生成1000个类别的ES样本。这样，我们提出了一个称为ES ImageNet的ES数据集，它比目前其他神经形态分类数据集大几十倍，完全由软件生成。ODG算法实现图像运动，利用不同方向上的离散梯度信息生成局部值变化，为将基于帧的图像转换为事件流提供了一种低成本、高速的方法，并使用边缘积分从事件流重建高质量图像。此外，我们以多种方式分析了ES ImageNet的统计数据，并使用著名的深度神经网络算法和脉冲神经网络算法提供了数据集的性能基准。我们相信这项工作将为SNN和神经形态视觉提供一个新的大规模基准数据集。
<details>	<summary>英文摘要</summary>	With event-driven algorithms, especially the spiking neural networks (SNNs), achieving continuous improvement in neuromorphic vision processing, a more challenging event-stream-dataset is urgently needed. However, it is well known that creating an ES-dataset is a time-consuming and costly task with neuromorphic cameras like dynamic vision sensors (DVS). In this work, we propose a fast and effective algorithm termed Omnidirectional Discrete Gradient (ODG) to convert the popular computer vision dataset ILSVRC2012 into its event-stream (ES) version, generating about 1,300,000 frame-based images into ES-samples in 1000 categories. In this way, we propose an ES-dataset called ES-ImageNet, which is dozens of times larger than other neuromorphic classification datasets at present and completely generated by the software. The ODG algorithm implements an image motion to generate local value changes with discrete gradient information in different directions, providing a low-cost and high-speed way for converting frame-based images into event streams, along with Edge-Integral to reconstruct the high-quality images from event streams. Furthermore, we analyze the statistics of the ES-ImageNet in multiple ways, and a performance benchmark of the dataset is also provided using both famous deep neural network algorithms and spiking neural network algorithms. We believe that this work shall provide a new large-scale benchmark dataset for SNNs and neuromorphic vision. </details>
<details>	<summary>邮件日期</summary>	2021年10月26日</details>

# 257、基于时间到第一脉冲编码的脉冲神经网络的硬件实现
- [ ] Hardware Implementation of Spiking Neural Networks Using Time-To-First-Spike Encoding 
时间：2021年10月22日                         第一作者：Seongbin Oh                       [链接](https://arxiv.org/abs/2006.05033).                     
<details>	<summary>邮件日期</summary>	2021年10月25日</details>

# 256、基于生物似然时空调整的反向传播训练深脉冲神经网络
- [ ] Backpropagation with Biologically Plausible Spatio-Temporal Adjustment For Training Deep Spiking Neural Networks 
时间：2021年10月22日                         第一作者：Guobin Shen                       [链接](https://arxiv.org/abs/2110.08858).                     
<details>	<summary>邮件日期</summary>	2021年10月25日</details>

# 255、一种用于脉冲神经网络静态图像编码的自适应采样和边缘检测方法
- [ ] An Adaptive Sampling and Edge Detection Approach for Encoding Static Images for Spiking Neural Networks 
时间：2021年10月19日                         第一作者：Peyton Ch                       [链接](https://arxiv.org/abs/2110.10217).                     
## 摘要：目前使用卷积神经网络进行图像分类的最新方法通常受到延迟和功耗的限制。这就限制了可以采用这些方法的设备，尤其是低功耗边缘设备。脉冲神经网络（SNN）被认为是第三代人工神经网络，其目的是通过从生物神经元通信过程中获得灵感来解决这些延迟和功率限制。然而，在将图像等数据输入SNN之前，必须先将其编码到脉冲序列中。在此，我们提出了一种使用边缘检测将静态图像编码为时间脉冲序列的方法和一种用于SNN的自适应信号采样方法。边缘检测过程包括首先对2D静态图像执行Canny边缘检测，然后使用图像-信号转换方法将检测到的边缘图像转换为两个X和Y信号。自适应信令方法包括对信号进行采样，使信号保持足够的细节，并对信号中的突变敏感。然后，诸如基于阈值的表示（TBR）和前向步进（SF）之类的时间编码机制能够用于将采样信号转换为脉冲序列。我们使用各种错误和指标来优化和评估所提出的图像编码方法的效率和精度。与传统SF和TBR编码相比，使用边缘检测和自适应时间编码机制生成的脉冲序列的原始信号和重构信号之间的比较结果显示，在用于编码MNIST数据集时，平均均方根误差（RMSE）分别减少了18倍和7倍。
<details>	<summary>英文摘要</summary>	Current state-of-the-art methods of image classification using convolutional neural networks are often constrained by both latency and power consumption. This places a limit on the devices, particularly low-power edge devices, that can employ these methods. Spiking neural networks (SNNs) are considered to be the third generation of artificial neural networks which aim to address these latency and power constraints by taking inspiration from biological neuronal communication processes. Before data such as images can be input into an SNN, however, they must be first encoded into spike trains. Herein, we propose a method for encoding static images into temporal spike trains using edge detection and an adaptive signal sampling method for use in SNNs. The edge detection process consists of first performing Canny edge detection on the 2D static images and then converting the edge detected images into two X and Y signals using an image-to-signal conversion method. The adaptive signaling approach consists of sampling the signals such that the signals maintain enough detail and are sensitive to abrupt changes in the signal. Temporal encoding mechanisms such as threshold-based representation (TBR) and step-forward (SF) are then able to be used to convert the sampled signals into spike trains. We use various error and indicator metrics to optimize and evaluate the efficiency and precision of the proposed image encoding approach. Comparison results between the original and reconstructed signals from spike trains generated using edge-detection and adaptive temporal encoding mechanism exhibit 18x and 7x reduction in average root mean square error (RMSE) compared to the conventional SF and TBR encoding, respectively, while used for encoding MNIST dataset. </details>
<details>	<summary>邮件日期</summary>	2021年10月22日</details>

# 254、HIRE-SNN：通过使用精心设计的输入噪声进行训练，利用节能深脉冲神经网络固有的鲁棒性
- [ ] HIRE-SNN: Harnessing the Inherent Robustness of Energy-Efficient Deep Spiking Neural Networks by Training with Crafted Input Noise 
时间：2021年10月06日                         第一作者：Souvik Kundu                       [链接](https://arxiv.org/abs/2110.11417).                     
## 摘要：低潜伏期深脉冲神经网络（SNN）已成为传统人工神经网络（ANN）的一种很有前途的替代方案，因为它们在事件驱动的神经形态硬件上具有提高能量效率的潜力。然而，包括SNN在内的神经网络会受到各种对抗性攻击，因此必须对其进行训练，以便在许多应用中保持对此类攻击的弹性。然而，由于与SNN相关的高昂训练成本，在各种对抗性攻击下对深层SNN的分析和优化在很大程度上被忽略了。在本文中，我们首先详细分析了低延迟SNN对流行的基于梯度的攻击的固有鲁棒性，即快速梯度符号法（FGSM）和投影梯度下降法（PGD）。受此分析的启发，为了利用模型的鲁棒性抵御这些攻击，我们提出了一种SNN训练算法，该算法使用精心编制的输入噪声，并且不需要额外的训练时间。为了评估我们算法的优点，我们在CIFAR-10和CIFAR-100数据集上对VGG和ResNet的变体进行了广泛的实验。与标准训练的直接输入SNN相比，我们训练的模型在FGSM和PGD攻击生成的图像上的分类精度分别提高了13.7%和10.1%，而干净图像的精度损失可以忽略不计。我们的模型也优于在速率编码输入上训练的固有鲁棒SNN，在攻击生成的图像上具有改进的或类似的分类性能，同时延迟和计算能量分别降低25倍和4.6倍。
<details>	<summary>英文摘要</summary>	Low-latency deep spiking neural networks (SNNs) have become a promising alternative to conventional artificial neural networks (ANNs) because of their potential for increased energy efficiency on event-driven neuromorphic hardware. Neural networks, including SNNs, however, are subject to various adversarial attacks and must be trained to remain resilient against such attacks for many applications. Nevertheless, due to prohibitively high training costs associated with SNNs, analysis, and optimization of deep SNNs under various adversarial attacks have been largely overlooked. In this paper, we first present a detailed analysis of the inherent robustness of low-latency SNNs against popular gradient-based attacks, namely fast gradient sign method (FGSM) and projected gradient descent (PGD). Motivated by this analysis, to harness the model robustness against these attacks we present an SNN training algorithm that uses crafted input noise and incurs no additional training time. To evaluate the merits of our algorithm, we conducted extensive experiments with variants of VGG and ResNet on both CIFAR-10 and CIFAR-100 datasets. Compared to standard trained direct input SNNs, our trained models yield improved classification accuracy of up to 13.7% and 10.1% on FGSM and PGD attack-generated images, respectively, with negligible loss in clean image accuracy. Our models also outperform inherently robust SNNs trained on rate-coded inputs with improved or similar classification performance on attack-generated images while having up to 25x and 4.6x lower latency and computation energy, respectively. </details>
<details>	<summary>注释</summary>	10 pages, 11 figures, 7 tables, International Conference on Computer Vision </details>
<details>	<summary>邮件日期</summary>	2021年10月25日</details>

# 253、基于生物似然时空调整的反向传播训练深脉冲神经网络
- [ ] Backpropagation with Biologically Plausible Spatio-Temporal Adjustment For Training Deep Spiking Neural Networks 
时间：2021年10月17日                         第一作者：Guobin Shen                       [链接](https://arxiv.org/abs/2110.08858).                     
## 摘要：脉冲神经网络（SNN）模拟人脑中的信息处理操作，在包含丰富时空信息的脉冲序列中表示和传输信息，在许多认知任务中表现出优异的性能。此外，事件驱动的信息处理使得在神经形态芯片上实现节能。深度学习的成功离不开反向传播。由于离散的信息传输，直接将反向传播应用于SNN的训练与传统的深度神经网络相比仍存在性能差距。此外，为了获得更好的性能，需要大量的模拟时间，这会导致较高的延迟。为了解决这些问题，我们提出了一种生物学上合理的空间调整方法，它重新考虑了膜电位和峰值之间的关系，并实现了对不同时间步长梯度的合理调整。它精确地控制误差沿空间维度的反向传播。其次，我们提出了一种生物学上合理的时间调整方法，使误差在时间维度上跨棘波传播，从而克服了传统棘波神经元在单个棘波周期内的时间依赖性问题。我们已经在多个数据集上验证了我们的算法，实验结果表明，我们的算法大大减少了网络延迟和能耗，同时也提高了网络性能。我们在神经形态数据集N-MNIST、DVS手势和DVS-CIFAR10上取得了最先进的性能。对于静态数据集MNIST和CIFAR10，我们已经超过了大多数传统的SNN反向传播训练算法，并取得了相对优越的性能。
<details>	<summary>英文摘要</summary>	The spiking neural network (SNN) mimics the information processing operation in the human brain, represents and transmits information in spike trains containing wealthy spatial and temporal information, and shows superior performance on many cognitive tasks. In addition, the event-driven information processing enables the energy-efficient implementation on neuromorphic chips. The success of deep learning is inseparable from backpropagation. Due to the discrete information transmission, directly applying the backpropagation to the training of the SNN still has a performance gap compared with the traditional deep neural networks. Also, a large simulation time is required to achieve better performance, which results in high latency. To address the problems, we propose a biological plausible spatial adjustment, which rethinks the relationship between membrane potential and spikes and realizes a reasonable adjustment of gradients to different time steps. And it precisely controls the backpropagation of the error along the spatial dimension. Secondly, we propose a biologically plausible temporal adjustment making the error propagate across the spikes in the temporal dimension, which overcomes the problem of the temporal dependency within a single spike period of the traditional spiking neurons. We have verified our algorithm on several datasets, and the experimental results have shown that our algorithm greatly reduces the network latency and energy consumption while also improving network performance. We have achieved state-of-the-art performance on the neuromorphic datasets N-MNIST, DVS-Gesture, and DVS-CIFAR10. For the static datasets MNIST and CIFAR10, we have surpassed most of the traditional SNN backpropagation training algorithm and achieved relatively superior performance. </details>
<details>	<summary>邮件日期</summary>	2021年10月19日</details>

# 252、HyperSeed：矢量符号体系结构的无监督学习
- [ ] HyperSeed: Unsupervised Learning with Vector Symbolic Architectures 
时间：2021年10月15日                         第一作者：Evgeny Osipov                       [链接](https://arxiv.org/abs/2110.08343).                     
## 摘要：受生物神经形态硬件最新创新的启发，本文提出了一种新的无监督机器学习方法Hyperseed，该方法利用向量符号体系结构（VSA）快速学习未标记数据的拓扑保持特征映射。它依赖于VSAs的两个主要功能：绑定操作和叠加计算。在本文中，我们介绍了在傅里叶全息约化表示VSA模型中表示的Hyperseed的算法部分，它特别适合于在脉冲神经形态硬件上实现。Hyperseed算法有两个独特的新颖之处：1）仅从少量输入数据样本进行学习；2）基于单个向量运算的学习规则。这些特性在合成数据集以及示例性基准用例、虹膜分类和使用n-gram统计的语言识别任务上得到了演示。
<details>	<summary>英文摘要</summary>	Motivated by recent innovations in biologically-inspired neuromorphic hardware, this paper presents a novel unsupervised machine learning approach named Hyperseed that leverages Vector Symbolic Architectures (VSA) for fast learning a topology preserving feature map of unlabelled data. It relies on two major capabilities of VSAs: the binding operation and computing in superposition. In this paper, we introduce the algorithmic part of Hyperseed expressed within Fourier Holographic Reduced Representations VSA model, which is specifically suited for implementation on spiking neuromorphic hardware. The two distinctive novelties of the Hyperseed algorithm are: 1) Learning from only few input data samples and 2) A learning rule based on a single vector operation. These properties are demonstrated on synthetic datasets as well as on illustrative benchmark use-cases, IRIS classification and a language identification task using n-gram statistics. </details>
<details>	<summary>邮件日期</summary>	2021年10月19日</details>

# 251、进化脉冲神经元细胞自动机和网络模拟体外神经元活动
- [ ] Evolving spiking neuron cellular automata and networks to emulate in vitro neuronal activity 
时间：2021年10月15日                         第一作者：J{\o}rgen Jensen Farner                       [链接](https://arxiv.org/abs/2110.08242).                     
## 摘要：神经启发模型和系统在非传统计算中有着巨大的应用潜力。通常，生物神经元的机制在模拟或物理系统中被建模或模仿，试图利用大脑的一些计算能力。然而，在神经系统中起作用的生物机制是复杂的，很难捕捉和设计；因此，采用数据驱动的方法将神经行为的特征转移到人工基底上会更简单。在本研究中，我们使用进化算法（EA）在体外产生模拟生物神经元行为模式的脉冲神经系统。该方法的目的是开发一种生成能够显示复杂行为的模型的方法，该模型可能适合用作计算基础。我们的模型能够产生一定程度的网络范围的同步性，并根据用于其进化的目标数据（来自一系列神经元培养密度和成熟度）显示出一系列行为。顶级模型的基因组表明，模型中连接的兴奋性和密度在决定所产生活动的复杂性方面起着重要作用。
<details>	<summary>英文摘要</summary>	Neuro-inspired models and systems have great potential for applications in unconventional computing. Often, the mechanisms of biological neurons are modeled or mimicked in simulated or physical systems in an attempt to harness some of the computational power of the brain. However, the biological mechanisms at play in neural systems are complicated and challenging to capture and engineer; thus, it can be simpler to turn to a data-driven approach to transfer features of neural behavior to artificial substrates. In the present study, we used an evolutionary algorithm (EA) to produce spiking neural systems that emulate the patterns of behavior of biological neurons in vitro. The aim of this approach was to develop a method of producing models capable of exhibiting complex behavior that may be suitable for use as computational substrates. Our models were able to produce a level of network-wide synchrony and showed a range of behaviors depending on the target data used for their evolution, which was from a range of neuronal culture densities and maturities. The genomes of the top-performing models indicate the excitability and density of connections in the model play an important role in determining the complexity of the produced activity. </details>
<details>	<summary>注释</summary>	To be published in proceedings of IEEE SSCI 2021 as part of ICES symposium (International Conference on Evolvable Systems, IEEE Symposium Series on Computational Intelligence 2021) </details>
<details>	<summary>邮件日期</summary>	2021年10月18日</details>

# 250、超越分类：直接训练用于语义分割的脉冲神经网络
- [ ] Beyond Classification: Directly Training Spiking Neural Networks for Semantic Segmentation 
时间：2021年10月14日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2110.07742).                     
## 摘要：脉冲神经网络（SNN）由于其稀疏、异步和二进制事件驱动的处理，最近成为人工神经网络（ANN）的低功耗替代品。由于其能源效率，SNN很有可能被部署到现实世界中资源受限的系统中，如自动驾驶车辆和无人机。然而，由于SNN的不可微性和复杂的神经元动力学特性，以往的SNN优化方法大多局限于图像识别。在本文中，我们探讨了SNN在分类之外的应用，并给出了配置有脉冲神经元的语义分割网络。具体来说，我们首先研究了两种有代表性的SNN优化技术（即ANN-SNN转换和代理梯度学习）在语义分割数据集上的识别任务。我们观察到，当从ANN转换时，SNN由于特征的空间差异而遭受高延迟和低性能。因此，我们使用替代梯度学习直接训练网络，比ANN-SNN转换具有更低的延迟和更高的性能。此外，我们为SNN域重新设计了两种基本的ANN分段结构（即完全卷积网络和DeepLab）。我们在两个公共语义分割基准上进行了实验，包括PASCAL VOC2012数据集和DDD17基于事件的数据集。除了说明SNN用于语义分割的可行性外，我们还表明SNN在该领域比ANN更具鲁棒性和能量效率。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have recently emerged as the low-power alternative to Artificial Neural Networks (ANNs) because of their sparse, asynchronous, and binary event-driven processing. Due to their energy efficiency, SNNs have a high possibility of being deployed for real-world, resource-constrained systems such as autonomous vehicles and drones. However, owing to their non-differentiable and complex neuronal dynamics, most previous SNN optimization methods have been limited to image recognition. In this paper, we explore the SNN applications beyond classification and present semantic segmentation networks configured with spiking neurons. Specifically, we first investigate two representative SNN optimization techniques for recognition tasks (i.e., ANN-SNN conversion and surrogate gradient learning) on semantic segmentation datasets. We observe that, when converted from ANNs, SNNs suffer from high latency and low performance due to the spatial variance of features. Therefore, we directly train networks with surrogate gradient learning, resulting in lower latency and higher performance than ANN-SNN conversion. Moreover, we redesign two fundamental ANN segmentation architectures (i.e., Fully Convolutional Networks and DeepLab) for the SNN domain. We conduct experiments on two public semantic segmentation benchmarks including the PASCAL VOC2012 dataset and the DDD17 event-based dataset. In addition to showing the feasibility of SNNs for semantic segmentation, we show that SNNs can be more robust and energy-efficient compared to their ANN counterparts in this domain. </details>
<details>	<summary>邮件日期</summary>	2021年10月18日</details>

# 249、脉冲神经网络中的深度剩余学习
- [ ] Deep Residual Learning in Spiking Neural Networks 
时间：2021年10月14日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2102.04159).                     
<details>	<summary>注释</summary>	Accepted by Advances in Neural Information Processing Systems (NeurIPS) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年10月18日</details>

# 248、一种训练脉冲神经网络的时间编码方法
- [ ] A Time Encoding approach to training Spiking Neural Networks 
时间：2021年10月13日                         第一作者：Karen Adam                       [链接](https://arxiv.org/abs/2110.06735).                     
## 摘要：虽然脉冲神经网络（SNN）越来越流行，但用于训练它们的算法似乎不足以解决与经典人工神经网络（ANN）相同的任务。在本文中，我们提供了一个额外的工具来帮助我们理解和训练SNN使用理论从时间编码领域。时间编码机（TEM）可以用来对神经元进行建模、整合和激发，并且具有很好的重建特性。我们将看到如何从TEM领域获得灵感，将SNN的峰值时间解释为SNN权重矩阵的约束。更具体地说，我们研究如何通过解决一组线性约束来训练单层SNN，以及如何通过利用SNN发出的脉冲的全部或无和异步特性来训练双层SNN。脉冲的这些特性导致了反向传播的替代方案，这在经典ANN中同时和分级激活的情况下是不可能的。
<details>	<summary>英文摘要</summary>	While Spiking Neural Networks (SNNs) have been gaining in popularity, it seems that the algorithms used to train them are not powerful enough to solve the same tasks as those tackled by classical Artificial Neural Networks (ANNs). In this paper, we provide an extra tool to help us understand and train SNNs by using theory from the field of time encoding. Time encoding machines (TEMs) can be used to model integrate-and-fire neurons and have well-understood reconstruction properties. We will see how one can take inspiration from the field of TEMs to interpret the spike times of SNNs as constraints on the SNNs' weight matrices. More specifically, we study how to train one-layer SNNs by solving a set of linear constraints, and how to train two-layer SNNs by leveraging the all-or-none and asynchronous properties of the spikes emitted by SNNs. These properties of spikes result in an alternative to backpropagation which is not possible in the case of simultaneous and graded activations as in classical ANNs. </details>
<details>	<summary>注释</summary>	5 pages, 5 figures, submitted to IEEE ICASSP 2022 </details>
<details>	<summary>邮件日期</summary>	2021年10月14日</details>

# 247、一种优化的无梯度深脉冲神经网络结构
- [ ] An optimised deep spiking neural network architecture without gradients 
时间：2021年10月12日                         第一作者：Yeshwanth Bethi                       [链接](https://arxiv.org/abs/2109.12813).                     
<details>	<summary>注释</summary>	18 pages, 6 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2021年10月13日</details>

# 246、SCFlow：用于脉冲相机的光流估计
- [ ] SCFlow: Optical Flow Estimation for Spiking Camera 
时间：2021年10月08日                         第一作者：Liwen Hu                       [链接](https://arxiv.org/abs/2110.03916).                     
## 摘要：作为一种具有高时间分辨率的仿生传感器，脉冲相机在实际应用中有着巨大的潜力，特别是在高速场景中的运动估计方面。光流估计在基于图像和基于事件的视觉中取得了显著的成功，但现有的方法不能直接应用于脉冲摄像机的脉冲流。传统的光流算法不能很好地匹配脉冲流数据。本文提出了一种新的用于脉冲相机光流估计的深度学习管道SCFlow。重要的是，我们引入了给定脉冲流的适当输入表示，该脉冲流作为唯一输入馈入SCFlow。我们介绍\textit{first}脉冲相机模拟器（SPCS）。此外，基于SPCS，我们首先提出了两个用于脉冲相机的光流数据集（脉冲飞行物和照片真实感高速运动，分别表示为SPIFT和PHM），对应于随机高速和精心设计的场景。实验结果表明，SCFlow可以预测不同高速场景中脉冲流的光流，并在数据集上表现出优于现有方法的优势\textit{所有代码和构造的数据集将在发布后发布}。
<details>	<summary>英文摘要</summary>	As a bio-inspired sensor with high temporal resolution, Spiking camera has an enormous potential in real applications, especially for motion estimation in high-speed scenes. Optical flow estimation has achieved remarkable success in image-based and event-based vision, but % existing methods cannot be directly applied in spike stream from spiking camera. conventional optical flow algorithms are not well matched to the spike stream data. This paper presents, SCFlow, a novel deep learning pipeline for optical flow estimation for spiking camera. Importantly, we introduce an proper input representation of a given spike stream, which is fed into SCFlow as the sole input. We introduce the \textit{first} spiking camera simulator (SPCS). Furthermore, based on SPCS, we first propose two optical flow datasets for spiking camera (SPIkingly Flying Things and Photo-realistic High-speed Motion, denoted as SPIFT and PHM respectively) corresponding to random high-speed and well-designed scenes. Empirically, we show that the SCFlow can predict optical flow from spike stream in different high-speed scenes, and express superiority to existing methods on the datasets. \textit{All codes and constructed datasets will be released after publication}. </details>
<details>	<summary>注释</summary>	The first two authors contributed equally </details>
<details>	<summary>邮件日期</summary>	2021年10月11日</details>

# 245、你只需要一个时间步：训练超低延迟的脉冲神经网络
- [ ] One Timestep is All You Need: Training Spiking Neural Networks with Ultra Low Latency 
时间：2021年10月01日                         第一作者：Sayeed Shafayet Chowdhury                       [链接](https://arxiv.org/abs/2110.05929).                     
## 摘要：脉冲神经网络（SNN）是常用深度神经网络（DNN）的节能替代方案。通过事件驱动的信息处理，SNN可以大大降低DNN昂贵的计算需求，同时实现相当的性能。然而，高推断延迟是深度SNN边缘部署的一个重要障碍。在多个时间步上进行计算不仅会增加延迟以及由于操作数量增加而导致的总体能量预算，而且还会导致获取膜电位的内存访问开销，这两种情况都会降低SNN的能量效益。为了克服这一瓶颈并充分利用SNN的潜力，我们提出了一种迭代初始化和重新训练SNN的方法（IIR-SNN），在时间轴上执行单次激发推理。该方法从使用T个时间步长（T>1）训练的SNN开始。然后，在减少延迟的每个阶段，使用前一阶段训练的具有较高时间步长的网络作为具有较低时间步长的后续训练的初始化。这是一种压缩方法，因为网络在时域中逐渐缩小。在本文中，我们使用直接输入编码并选择T=5，因为根据文献，它是在ImageNet上实现令人满意的性能所需的最小延迟。提出的方案允许我们获得高达单位延迟的SNN，在推理过程中需要一次前向传递。使用VGG16，我们在CIFAR-10、CIFAR-100和ImageNet上分别实现了93.05%、70.15%和67.71%的顶级精度，只需1个时间步。此外，与其他最先进的SNN相比，IIR SNN执行推理的延迟减少了5-2500倍，保持了相当甚至更好的准确性。此外，与标准DNN相比，建议的IIR SNN提供了25-33倍的能源效率，同时在分类性能方面与标准DNN相当。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) are energy efficient alternatives to commonly used deep neural networks (DNNs). Through event-driven information processing, SNNs can reduce the expensive compute requirements of DNNs considerably, while achieving comparable performance. However, high inference latency is a significant hindrance to the edge deployment of deep SNNs. Computation over multiple timesteps not only increases latency as well as overall energy budget due to higher number of operations, but also incurs memory access overhead of fetching membrane potentials, both of which lessen the energy benefits of SNNs. To overcome this bottleneck and leverage the full potential of SNNs, we propose an Iterative Initialization and Retraining method for SNNs (IIR-SNN) to perform single shot inference in the temporal axis. The method starts with an SNN trained with T timesteps (T>1). Then at each stage of latency reduction, the network trained at previous stage with higher timestep is utilized as initialization for subsequent training with lower timestep. This acts as a compression method, as the network is gradually shrunk in the temporal domain. In this paper, we use direct input encoding and choose T=5, since as per literature, it is the minimum required latency to achieve satisfactory performance on ImageNet. The proposed scheme allows us to obtain SNNs with up to unit latency, requiring a single forward pass during inference. We achieve top-1 accuracy of 93.05%, 70.15% and 67.71% on CIFAR-10, CIFAR-100 and ImageNet, respectively using VGG16, with just 1 timestep. In addition, IIR-SNNs perform inference with 5-2500X reduced latency compared to other state-of-the-art SNNs, maintaining comparable or even better accuracy. Furthermore, in comparison with standard DNNs, the proposed IIR-SNNs provide25-33X higher energy efficiency, while being comparable to them in classification performance. </details>
<details>	<summary>邮件日期</summary>	2021年10月13日</details>

# 244、基于事件视觉的脉冲卷积网络对抗攻击
- [ ] Adversarial Attacks on Spiking Convolutional Networks for Event-based Vision 
时间：2021年10月06日                         第一作者：Julian B\"uchel                       [链接](https://arxiv.org/abs/2110.02929).                     
## 摘要：使用动态视觉传感器的基于事件的传感在低功耗视觉应用中获得了发展。脉冲神经网络能够很好地处理基于事件的数据的稀疏性，适合部署在低功耗的神经形态硬件上。作为一个新兴领域，脉冲神经网络对潜在恶意对手攻击的敏感性迄今为止很少受到关注。在这项工作中，我们展示了白盒对抗攻击算法如何适应基于事件的视觉数据的离散和稀疏性质，以及如何适应脉冲神经网络的连续时间设置。我们在N-MNIST和IBM手势神经形态视觉数据集上测试了我们的方法，结果表明，通过注入相对较少的适当位置的事件，对抗性干扰可以获得较高的成功率。我们还首次验证了这些扰动直接作用于神经形态硬件的有效性。最后，我们讨论了由此产生的扰动的性质和未来可能的方向。
<details>	<summary>英文摘要</summary>	Event-based sensing using dynamic vision sensors is gaining traction in low-power vision applications. Spiking neural networks work well with the sparse nature of event-based data and suit deployment on low-power neuromorphic hardware. Being a nascent field, the sensitivity of spiking neural networks to potentially malicious adversarial attacks has received very little attention so far. In this work, we show how white-box adversarial attack algorithms can be adapted to the discrete and sparse nature of event-based visual data, and to the continuous-time setting of spiking neural networks. We test our methods on the N-MNIST and IBM Gestures neuromorphic vision datasets and show adversarial perturbations achieve a high success rate, by injecting a relatively small number of appropriately placed events. We also verify, for the first time, the effectiveness of these perturbations directly on neuromorphic hardware. Finally, we discuss the properties of the resulting perturbations and possible future directions. </details>
<details>	<summary>注释</summary>	16 pages, preprint, submitted to ICLR 2022 </details>
<details>	<summary>邮件日期</summary>	2021年10月07日</details>

# 243、快速精确递归神经网络的脉冲激励秩编码
- [ ] Spike-inspired Rank Coding for Fast and Accurate Recurrent Neural Networks 
时间：2021年10月06日                         第一作者：Alan Jeffares                       [链接](https://arxiv.org/abs/2110.02865).                     
## 摘要：生物脉冲神经网络（SNN）可以在其输出中对信息进行时间编码，例如按照神经元激发的顺序，而人工神经网络（ANN）通常不这样做。因此，在处理时间输入时，神经形态计算的SNN模型被认为可能比ANN更快速有效。另一方面，人工神经网络更易于训练，通常可获得优异的性能。在这里，我们表明，时态编码（如SNN启发的秩编码（RC））也可以应用于传统的人工神经网络（如LSTM），并导致计算节省和加速。在ANN的RC中，我们使用标准实值激活通过时间应用反向传播，但仅从每个顺序输入示例的策略性早期时间步开始，由阈值交叉事件决定。然后，学习自然也会在不改变模型或算法的情况下产生输出。通过跳过第一个事件后剩余的输入序列，向前和向后的训练过程都可以显著缩短。RC培训还显著缩短了推理过程中的洞察时间，但准确性的降低幅度最小。期望的速度-精度权衡可通过改变阈值或奖励输出熵的正则化参数进行调整。我们在序列分类的两个玩具问题中以及在时间编码的MNIST数据集中演示了这些问题，其中我们的RC模型在第一个输入时间步后达到99.19%的准确率，在使用SNN的时间编码以及Google语音命令的口语词分类方面优于最新技术，优于非RC训练的LSTM早期推理。
<details>	<summary>英文摘要</summary>	Biological spiking neural networks (SNNs) can temporally encode information in their outputs, e.g. in the rank order in which neurons fire, whereas artificial neural networks (ANNs) conventionally do not. As a result, models of SNNs for neuromorphic computing are regarded as potentially more rapid and efficient than ANNs when dealing with temporal input. On the other hand, ANNs are simpler to train, and usually achieve superior performance. Here we show that temporal coding such as rank coding (RC) inspired by SNNs can also be applied to conventional ANNs such as LSTMs, and leads to computational savings and speedups. In our RC for ANNs, we apply backpropagation through time using the standard real-valued activations, but only from a strategically early time step of each sequential input example, decided by a threshold-crossing event. Learning then incorporates naturally also _when_ to produce an output, without other changes to the model or the algorithm. Both the forward and the backward training pass can be significantly shortened by skipping the remaining input sequence after that first event. RC-training also significantly reduces time-to-insight during inference, with a minimal decrease in accuracy. The desired speed-accuracy trade-off is tunable by varying the threshold or a regularization parameter that rewards output entropy. We demonstrate these in two toy problems of sequence classification, and in a temporally-encoded MNIST dataset where our RC model achieves 99.19% accuracy after the first input time-step, outperforming the state of the art in temporal coding with SNNs, as well as in spoken-word classification of Google Speech Commands, outperforming non-RC-trained early inference with LSTMs. </details>
<details>	<summary>邮件日期</summary>	2021年10月07日</details>

# 242、全脉冲变分自动编码器
- [ ] Fully Spiking Variational Autoencoder 
时间：2021年10月05日                         第一作者：Hiromichi Kamata                       [链接](https://arxiv.org/abs/2110.00375).                     
<details>	<summary>注释</summary>	https://github.com/kamata1729/FullySpikingVAE </details>
<details>	<summary>邮件日期</summary>	2021年10月06日</details>

# 241、基于生物神经网络的端到端语音识别
- [ ] Towards efficient end-to-end speech recognition with biologically-inspired neural networks 
时间：2021年10月04日                         第一作者：Thomas Bohnstingl                       [链接](https://arxiv.org/abs/2110.02743).                     
## 摘要：自动语音识别（ASR）是一种使程序能够将人类语音处理成书面形式的能力。人工智能（AI）的最新发展导致了基于深层神经网络的高精度ASR系统，如递归神经网络传感器（RNN-T）。然而，这些方法的核心组件和执行的操作与强大的生物对应物，即人脑不同。另一方面，目前基于脉冲神经网络（SNN）的生物启发ASR模型的发展在准确性方面落后，主要集中在小规模应用上。在这项工作中，我们回顾了将生物学上合理的模型纳入深度学习的过程，并从大脑中发现的各种神经和突触动力学中获得灵感，从而大大增强了这些模型的能力。特别是，我们介绍了模拟轴-体突触和轴-轴突触的神经连接概念。基于此，我们提出了具有丰富神经突触动力学的新型深度学习单元，并将其集成到RNN-T架构中。我们首次证明，与现有的深度学习模型相比，大规模ASR模型的生物现实实现可以产生具有竞争力的性能水平。具体地说，我们证明了这种实现具有一些优势，例如降低了计算成本和较低的延迟，这对于语音识别应用至关重要。
<details>	<summary>英文摘要</summary>	Automatic speech recognition (ASR) is a capability which enables a program to process human speech into a written form. Recent developments in artificial intelligence (AI) have led to high-accuracy ASR systems based on deep neural networks, such as the recurrent neural network transducer (RNN-T). However, the core components and the performed operations of these approaches depart from the powerful biological counterpart, i.e., the human brain. On the other hand, the current developments in biologically-inspired ASR models, based on spiking neural networks (SNNs), lag behind in terms of accuracy and focus primarily on small scale applications. In this work, we revisit the incorporation of biologically-plausible models into deep learning and we substantially enhance their capabilities, by taking inspiration from the diverse neural and synaptic dynamics found in the brain. In particular, we introduce neural connectivity concepts emulating the axo-somatic and the axo-axonic synapses. Based on this, we propose novel deep learning units with enriched neuro-synaptic dynamics and integrate them into the RNN-T architecture. We demonstrate for the first time, that a biologically realistic implementation of a large-scale ASR model can yield competitive performance levels compared to the existing deep learning models. Specifically, we show that such an implementation bears several advantages, such as a reduced computational cost and a lower latency, which are critical for speech recognition applications. </details>
<details>	<summary>邮件日期</summary>	2021年10月07日</details>

# 240、脉冲超维网络：结合记忆框架的神经形态模型
- [ ] Spiking Hyperdimensional Network: Neuromorphic Models Integrated with Memory-Inspired Framework 
时间：2021年10月01日                         第一作者：Zhuowen Zou                       [链接](https://arxiv.org/abs/2110.00214).                     
## 摘要：最近，受大脑启发的计算模型在鲁棒性和能源效率方面表现出了超越当今深度学习解决方案的巨大潜力。特别是，脉冲神经网络（SNN）和超维计算（HDC）在实现高效和稳健的认知学习方面显示了良好的结果。尽管取得了成功，但这两种大脑启发模型有着不同的优势。SNN模仿人脑的物理特性，而HDC则在更抽象和功能的层面上对大脑进行建模。他们的设计理念展示了激励他们组合的互补模式。借助于经典的记忆心理学模型，我们提出了SpikeHD，这是第一个从根本上结合了脉冲神经网络和超维计算的框架。SpikeHD生成了一个可扩展且强大的认知学习系统，可以更好地模拟大脑功能。SpikeHD利用脉冲神经网络通过保留原始事件脉冲数据的空间和时间相关性来提取低级特征。然后，通过将信号映射到高维空间，学习抽象信息，并对数据进行分类，利用HDC对SNN输出进行操作。我们对一组基准分类问题的广泛评估表明，与SNN体系结构相比，SpikeHD具有以下优点：（1）通过利用两阶段信息处理显著增强学习能力，（2）对噪声和故障具有较强的鲁棒性，以及（3）减少网络规模和学习复杂信息所需的参数。
<details>	<summary>英文摘要</summary>	Recently, brain-inspired computing models have shown great potential to outperform today's deep learning solutions in terms of robustness and energy efficiency. Particularly, Spiking Neural Networks (SNNs) and HyperDimensional Computing (HDC) have shown promising results in enabling efficient and robust cognitive learning. Despite the success, these two brain-inspired models have different strengths. While SNN mimics the physical properties of the human brain, HDC models the brain on a more abstract and functional level. Their design philosophies demonstrate complementary patterns that motivate their combination. With the help of the classical psychological model on memory, we propose SpikeHD, the first framework that fundamentally combines Spiking neural network and hyperdimensional computing. SpikeHD generates a scalable and strong cognitive learning system that better mimics brain functionality. SpikeHD exploits spiking neural networks to extract low-level features by preserving the spatial and temporal correlation of raw event-based spike data. Then, it utilizes HDC to operate over SNN output by mapping the signal into high-dimensional space, learning the abstract information, and classifying the data. Our extensive evaluation on a set of benchmark classification problems shows that SpikeHD provides the following benefit compared to SNN architecture: (1) significantly enhance learning capability by exploiting two-stage information processing, (2) enables substantial robustness to noise and failure, and (3) reduces the network size and required parameters to learn complex information. </details>
<details>	<summary>邮件日期</summary>	2021年10月04日</details>

# 239、利用深度学习的经验教训训练脉冲神经网络
- [ ] Training Spiking Neural Networks Using Lessons From Deep Learning 
时间：2021年10月01日                         第一作者：Jason K. Eshraghian                        [链接](https://arxiv.org/abs/2109.12894).                     
<details>	<summary>邮件日期</summary>	2021年10月04日</details>

# 238、基于进化神经形态雷达的自主开源飞艇高度控制器
- [ ] Evolved neuromorphic radar-based altitude controller for an autonomous open-source blimp 
时间：2021年10月01日                         第一作者：Marina Gonz\'alez-\'Alvarez                       [链接](https://arxiv.org/abs/2110.00646).                     
## 摘要：机器人飞艇在安全性、机动性和延长飞行时间方面具有显著优势。然而，它们的高度限制性权重约束对执行所需控制任务的可用计算能力提出了重大挑战。脉冲神经网络（SNN）是解决这一问题的一个很有前途的研究方向。通过模仿神经元之间使用脉冲或脉冲传输信息的生物过程，它们允许低功耗和异步事件驱动处理。在本文中，我们提出了一种基于SNN的机器人飞艇进化高度控制器，该控制器完全依赖于机载雷达提供的传感器反馈。从轻量级、低成本、开源飞艇的设计出发，我们还提出了一种基于SNN的控制器体系结构，一种在模拟环境中训练网络的进化框架，以及一种改善与现实差距的控制方案。通过实际实验对系统的性能进行了评估，并与人工神经网络和线性控制器进行了比较，证明了该方法的优越性。结果表明，通过有效的控制，可以精确跟踪高度指令。
<details>	<summary>英文摘要</summary>	Robotic airships offer significant advantages in terms of safety, mobility, and extended flight times. However, their highly restrictive weight constraints pose a major challenge regarding the available computational power to perform the required control tasks. Spiking neural networks (SNNs) are a promising research direction for addressing this problem. By mimicking the biological process for transferring information between neurons using spikes or impulses, they allow for low power consumption and asynchronous event-driven processing. In this paper, we propose an evolved altitude controller based on a SNN for a robotic airship which relies solely on the sensory feedback provided by an airborne radar. Starting from the design of a lightweight, low-cost, open-source airship, we also present a SNN-based controller architecture, an evolutionary framework for training the network in a simulated environment, and a control scheme for ameliorating the gap with reality. The system's performance is evaluated through real-world experiments, demonstrating the advantages of our approach by comparing it with an artificial neural network and a linear controller. The results show an accurate tracking of the altitude command with an efficient control effort. </details>
<details>	<summary>邮件日期</summary>	2021年10月05日</details>

# 237、脉冲神经形态硬件上量子基态的变分学习
- [ ] Variational learning of quantum ground states on spiking neuromorphic hardware 
时间：2021年10月01日                         第一作者：Robert Klassert                       [链接](https://arxiv.org/abs/2109.15169).                     
<details>	<summary>注释</summary>	12 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年10月05日</details>

# 236、脉冲神经形态硬件上量子基态的变分学习
- [ ] Variational learning of quantum ground states on spiking neuromorphic hardware 
时间：2021年09月30日                         第一作者：Robert Klassert                       [链接](https://arxiv.org/abs/2109.15169).                     
## 摘要：我们训练了一个神经形态硬件芯片，通过变分能量最小化来近似量子自旋模型的基态。与使用马尔可夫链蒙特卡罗生成样本的变分人工神经网络相比，该方法的优点是神经形态设备以快速且固有的并行方式生成样本。我们开发了一种训练算法，并将其应用于横向场伊辛模型，在中等系统规模（$N\leq 10$）下表现出良好的性能。一项系统的超参数研究表明，对更大系统规模的可扩展性主要取决于样本质量，而样本质量受到模拟神经形态芯片上参数漂移的限制。学习性能显示了阈值行为作为ansatz变分参数数量的函数，大约$50$隐藏神经元足以表示高达$N=10$的临界基态。网络参数的6+1位分辨率不限制当前设置中可达到的近似质量。我们的工作为利用神经形态硬件的能力解决量子多体问题中的维度诅咒迈出了重要的一步。
<details>	<summary>英文摘要</summary>	We train a neuromorphic hardware chip to approximate the ground states of quantum spin models by variational energy minimization. Compared to variational artificial neural networks using Markov chain Monte Carlo for sample generation, this approach has the advantage that the neuromorphic device generates samples in a fast and inherently parallel fashion. We develop a training algorithm and apply it to the transverse field Ising model, showing good performance at moderate system sizes ($N\leq 10$). A systematic hyperparameter study shows that scalability to larger system sizes mainly depends on sample quality which is limited by parameter drifts on the analog neuromorphic chip. The learning performance shows a threshold behavior as a function of the number of variational parameters of the ansatz, with approximately $50$ hidden neurons being sufficient for representing critical ground states up to $N=10$. The 6+1-bit resolution of the network parameters does not limit the reachable approximation quality in the current setup. Our work provides an important step towards harnessing the capabilities of neuromorphic hardware for tackling the curse of dimensionality in quantum many-body problems. </details>
<details>	<summary>注释</summary>	12 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年10月01日</details>

# 235、全脉冲变分自动编码器
- [ ] Fully Spiking Variational Autoencoder 
时间：2021年09月26日                         第一作者：Hiromichi Kamata                       [链接](https://arxiv.org/abs/2110.00375).                     
## 摘要：脉冲神经网络（SNN）由于其二进制和事件驱动的特性，可以在具有超高速和超低能耗的神经形态设备上运行。因此，SNN有望有各种应用，包括在边缘设备上运行生成模型，以创建高质量图像。在这项研究中，我们构建了一个带有SNN的变分自动编码器（VAE）来实现图像生成。VAE以其在生成模型中的稳定性而闻名；最近，它的质量提高了。在VAE中，潜在空间表示为正态分布，采样时需要进行浮点计算。然而，这在SNN中是不可能的，因为所有特征必须是二进制时间序列数据。因此，我们使用自回归SNN模型构建了潜在空间，并从其输出中随机选取样本对潜在变量进行采样。这允许潜在变量遵循伯努利过程，并允许变分学习。因此，我们构建了全脉冲变分自动编码器，其中所有模块都用SNN构造。据我们所知，我们是第一个仅使用SNN层构建VAE的公司。我们对几个数据集进行了实验，并确认它可以生成与传统人工神经网络相同或更好质量的图像。代码很快就会发布。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) can be run on neuromorphic devices with ultra-high speed and ultra-low energy consumption because of their binary and event-driven nature. Therefore, SNNs are expected to have various applications, including as generative models being running on edge devices to create high-quality images. In this study, we build a variational autoencoder (VAE) with SNN to enable image generation. VAE is known for its stability among generative models; recently, its quality advanced. In vanilla VAE, the latent space is represented as a normal distribution, and floating-point calculations are required in sampling. However, this is not possible in SNNs because all features must be binary time series data. Therefore, we constructed the latent space with an autoregressive SNN model, and randomly selected samples from its output to sample the latent variables. This allows the latent variables to follow the Bernoulli process and allows variational learning. Thus, we build the Fully Spiking Variational Autoencoder where all modules are constructed with SNN. To the best of our knowledge, we are the first to build a VAE only with SNN layers. We experimented with several datasets, and confirmed that it can generate images with the same or better quality compared to conventional ANNs. The code will be available soon. </details>
<details>	<summary>邮件日期</summary>	2021年10月04日</details>

# 234、平衡态隐式微分训练反馈脉冲神经网络
- [ ] Training Feedback Spiking Neural Networks by Implicit Differentiation on the Equilibrium State 
时间：2021年09月29日                         第一作者：Mingqing Xiao                       [链接](https://arxiv.org/abs/2109.14247).                     
## 摘要：脉冲神经网络（SNN）是受大脑启发的模型，能够在神经形态硬件上实现节能。然而，由于脉冲神经元模型的不连续性，SNN的监督训练仍然是一个难题。大多数现有的方法模仿人工神经网络的反向传播框架和前馈结构，并使用替代导数或计算关于脉冲时间的梯度来处理该问题。这些方法要么累积近似误差，要么仅通过现有脉冲有限地传播信息，通常需要信息沿时间步传播，具有较大的内存开销和生物不可信性。在这项工作中，我们考虑反馈脉冲神经网络，这是更像大脑，并提出了一种新的训练方法，不依赖于准确的反向正向计算。首先，我们证明了具有反馈连接的SNN的平均放电速率会随着时间逐渐演化到一个平衡状态，这遵循一个不动点方程。然后，通过将反馈SNN的正向计算视为该方程的黑箱解算器，并利用方程上的隐式微分，我们可以计算参数的梯度，而无需考虑精确的正向过程。这样，前向和后向过程被解耦，从而避免了不可微脉冲函数的问题。我们还简要讨论了内隐分化的生物学合理性，它只需要计算另一个平衡。在MNIST、Fashion MNIST、N-MNIST、CIFAR-10和CIFAR-100上进行的大量实验表明，我们的方法对于在少量时间步长内具有较少神经元和参数的反馈模型具有优越的性能。我们的代码可在https://github.com/pkuxmq/IDE-FSNN.
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are brain-inspired models that enable energy-efficient implementation on neuromorphic hardware. However, the supervised training of SNNs remains a hard problem due to the discontinuity of the spiking neuron model. Most existing methods imitate the backpropagation framework and feedforward architectures for artificial neural networks, and use surrogate derivatives or compute gradients with respect to the spiking time to deal with the problem. These approaches either accumulate approximation errors or only propagate information limitedly through existing spikes, and usually require information propagation along time steps with large memory costs and biological implausibility. In this work, we consider feedback spiking neural networks, which are more brain-like, and propose a novel training method that does not rely on the exact reverse of the forward computation. First, we show that the average firing rates of SNNs with feedback connections would gradually evolve to an equilibrium state along time, which follows a fixed-point equation. Then by viewing the forward computation of feedback SNNs as a black-box solver for this equation, and leveraging the implicit differentiation on the equation, we can compute the gradient for parameters without considering the exact forward procedure. In this way, the forward and backward procedures are decoupled and therefore the problem of non-differentiable spiking functions is avoided. We also briefly discuss the biological plausibility of implicit differentiation, which only requires computing another equilibrium. Extensive experiments on MNIST, Fashion-MNIST, N-MNIST, CIFAR-10, and CIFAR-100 demonstrate the superior performance of our method for feedback models with fewer neurons and parameters in a small number of time steps. Our code is avaiable at https://github.com/pkuxmq/IDE-FSNN. </details>
<details>	<summary>注释</summary>	Accepted by NeurIPS 2021 (Spotlight) </details>
<details>	<summary>邮件日期</summary>	2021年09月30日</details>

# 233、通过信息瓶颈学习脉冲神经网络的时间解码
- [ ] Learning to Time-Decode in Spiking Neural Networks Through the Information Bottleneck 
时间：2021年09月29日                         第一作者：Nicolas Skatchkovsky                       [链接](https://arxiv.org/abs/2106.01177).                     
<details>	<summary>注释</summary>	Accepted at NeuRIPS 2021 </details>
<details>	<summary>邮件日期</summary>	2021年09月30日</details>

# 232、利用深度学习的经验教训训练脉冲神经网络
- [ ] Training Spiking Neural Networks Using Lessons From Deep Learning 
时间：2021年09月29日                         第一作者：Jason K. Eshraghian                        [链接](https://arxiv.org/abs/2109.12894).                     
<details>	<summary>邮件日期</summary>	2021年09月30日</details>

# 231、立体画：用脉冲神经网络进行深度学习
- [ ] StereoSpike: Depth Learning with a Spiking Neural Network 
时间：2021年09月28日                         第一作者：Ulysse Ran\c{c}on                       [链接](https://arxiv.org/abs/2109.13751).                     
## 摘要：深度估计是一项重要的计算机视觉任务，特别适用于自动驾驶车辆的导航或机器人的目标操纵。在这里，我们使用端到端的神经形态方法解决了这个问题，将两个基于事件的摄像头和一个脉冲神经网络（SNN）与一个稍加修改的类似U网络的编码器-解码器架构相结合，我们将其命名为立体派克。更具体地说，我们使用了多车辆立体事件摄影机数据集（MVSEC）。它提供了一个深度-地面真实值，用于使用代理梯度下降以有监督的方式训练立体派克。我们提出了一种新的读出模式，从解码器的峰值中获得密集的模拟预测——每个像素的深度。我们证明了该体系结构的通用性非常好，甚至比其非脉冲对应结构更好，从而实现了最先进的测试精度。据我们所知，这是第一次用完全脉冲网络解决如此大规模的回归问题。最后，我们证明了通过正则化可以获得低的发射率（<10%），并且在精度上的代价最小。这意味着可以在神经形态芯片上高效地实现立体派克，为低功耗和实时嵌入式系统打开了大门。
<details>	<summary>英文摘要</summary>	Depth estimation is an important computer vision task, useful in particular for navigation in autonomous vehicles, or for object manipulation in robotics. Here we solved it using an end-to-end neuromorphic approach, combining two event-based cameras and a Spiking Neural Network (SNN) with a slightly modified U-Net-like encoder-decoder architecture, that we named StereoSpike. More specifically, we used the Multi Vehicle Stereo Event Camera Dataset (MVSEC). It provides a depth ground-truth, which was used to train StereoSpike in a supervised manner, using surrogate gradient descent. We propose a novel readout paradigm to obtain a dense analog prediction -- the depth of each pixel -- from the spikes of the decoder. We demonstrate that this architecture generalizes very well, even better than its non-spiking counterparts, leading to state-of-the-art test accuracy. To the best of our knowledge, it is the first time that such a large-scale regression problem is solved by a fully spiking network. Finally, we show that low firing rates (<10%) can be obtained via regularization, with a minimal cost in accuracy. This means that StereoSpike could be efficiently implemented on neuromorphic chips, opening the door for low power and real time embedded systems. </details>
<details>	<summary>邮件日期</summary>	2021年09月29日</details>

# 230、深相量网络：连接传统神经网络和脉冲神经网络
- [ ] Deep Phasor Networks: Connecting Conventional and Spiking Neural Networks 
时间：2021年09月28日                         第一作者：Wilkie Olin-Ammentorp                       [链接](https://arxiv.org/abs/2106.11908).                     
<details>	<summary>注释</summary>	24 pages, 8 figures </details>
<details>	<summary>邮件日期</summary>	2021年09月29日</details>

# 229、一种优化的无梯度深脉冲神经网络结构
- [ ] An optimised deep spiking neural network architecture without gradients 
时间：2021年09月27日                         第一作者：Yeshwanth Bethi                       [链接](https://arxiv.org/abs/2109.12813).                     
## 摘要：我们提出了一种端到端可训练的模块化事件驱动神经结构，该结构使用局部突触和阈值适应规则来执行任意时空脉冲模式之间的转换。该体系结构代表了现有脉冲神经网络（SNN）体系结构的高度抽象模型。所提出的优化深度事件驱动脉冲神经网络结构（ODSA）可以同时学习多个任意时间尺度的分层时空特征。ODSA在不使用误差反向传播或梯度计算的情况下执行在线学习。通过在每个节点上使用简单的局部自适应选择阈值，网络可以快速学习在每一层为任何给定问题适当分配其神经元资源，而无需使用实值误差度量。这些自适应选择阈值是ODSA的核心特征，确保了网络的稳定性和对噪声以及初始系统参数选择的显著鲁棒性。由于每一层的硬赢家通吃（WTA）约束，网络激活本质上是稀疏的。我们评估了现有时空数据集的体系结构，包括spike编码的IRIS和TIDIGITS数据集，以及基于我们创建的国际莫尔斯电码的一组新任务。这些测试证明了ODSA的分层时空学习能力。通过这些测试，我们证明了ODSA能够以尽可能少的计算节点数最优地解决实际且极具挑战性的分层时空学习任务。
<details>	<summary>英文摘要</summary>	We present an end-to-end trainable modular event-driven neural architecture that uses local synaptic and threshold adaptation rules to perform transformations between arbitrary spatio-temporal spike patterns. The architecture represents a highly abstracted model of existing Spiking Neural Network (SNN) architectures. The proposed Optimized Deep Event-driven Spiking neural network Architecture (ODESA) can simultaneously learn hierarchical spatio-temporal features at multiple arbitrary time scales. ODESA performs online learning without the use of error back-propagation or the calculation of gradients. Through the use of simple local adaptive selection thresholds at each node, the network rapidly learns to appropriately allocate its neuronal resources at each layer for any given problem without using a real-valued error measure. These adaptive selection thresholds are the central feature of ODESA, ensuring network stability and remarkable robustness to noise as well as to the selection of initial system parameters. Network activations are inherently sparse due to a hard Winner-Take-All (WTA) constraint at each layer. We evaluate the architecture on existing spatio-temporal datasets, including the spike-encoded IRIS and TIDIGITS datasets, as well as a novel set of tasks based on International Morse Code that we created. These tests demonstrate the hierarchical spatio-temporal learning capabilities of ODESA. Through these tests, we demonstrate ODESA can optimally solve practical and highly challenging hierarchical spatio-temporal learning tasks with the minimum possible number of computing nodes. </details>
<details>	<summary>注释</summary>	18 pages, 6 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 228、利用深度学习的经验教训训练脉冲神经网络
- [ ] Training Spiking Neural Networks Using Lessons From Deep Learning 
时间：2021年09月27日                         第一作者：Jason K. Eshraghian                        [链接](https://arxiv.org/abs/2109.12894).                     
## 摘要：大脑是寻找灵感以开发更高效的神经网络的完美场所。我们的突触和神经元的内部运作让我们得以窥见深度学习的未来。本文展示了如何将几十年来在深度学习、梯度下降、反向传播和神经科学方面的研究成果应用于生物学上合理的脉冲神经网络。本文探讨了将数据编码为脉冲与学习过程之间微妙的相互作用；将梯度学习应用于脉冲神经网络的挑战和解决方案；时间反向传播和脉冲时间依赖性可塑性之间的微妙联系，以及深度学习如何向生物学上合理的在线学习发展。一些想法在神经形态工程界被广泛接受和使用，而另一些想法在这里首次被提出或证明是正确的。
<details>	<summary>英文摘要</summary>	The brain is the perfect place to look for inspiration to develop more efficient neural networks. The inner workings of our synapses and neurons provide a glimpse at what the future of deep learning might look like. This paper shows how to apply the lessons learnt from several decades of research in deep learning, gradient descent, backpropagation and neuroscience to biologically plausible spiking neural neural networks. This paper explores the delicate interplay between encoding data as spikes and the learning process; the challenges and solutions of applying gradient-based learning to spiking neural networks; the subtle link between temporal backpropagation and spike timing dependent plasticity, and how deep learning might move towards biologically plausible online learning. Some ideas are well accepted and commonly used amongst the neuromorphic engineering community, while others are presented or justified for the first time here. </details>
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 227、基于代理的脉冲神经网络训练
- [ ] Spiking neural networks trained via proxy 
时间：2021年09月27日                         第一作者：Saeed Reza Kheradpisheh                       [链接](https://arxiv.org/abs/2109.13208).                     
## 摘要：我们提出了一种新的学习算法来训练以传统人工神经网络（ANN）为代理的脉冲神经网络（SNN）。我们分别耦合了两个SNN和ANN网络，这两个网络由具有相同网络结构和共享突触权重的Integrated and fire（IF）和ReLU神经元组成。两个网络的前向传递是完全独立的。通过假设带有速率编码的IF神经元作为ReLU的近似值，我们在代理ANN中反向传播SNN的错误以更新共享权重，只需将ANN的最终输出替换为SNN的最终输出。我们将提出的代理学习应用于深度卷积SNN，并在Fahion MNIST和Cifar10两个基准数据集上对其进行评估，分类准确率分别为94.56%和93.11%。所提出的网络可以优于其他通过串联学习、替代梯度学习或从深度ANN转换而来的深度SNN。转换后的SNN需要很长的模拟时间才能达到合理的精度，而我们的代理学习可以使有效的SNN具有更短的模拟时间。
<details>	<summary>英文摘要</summary>	We propose a new learning algorithm to train spiking neural networks (SNN) using conventional artificial neural networks (ANN) as proxy. We couple two SNN and ANN networks, respectively, made of integrate-and-fire (IF) and ReLU neurons with the same network architectures and shared synaptic weights. The forward passes of the two networks are totally independent. By assuming IF neuron with rate-coding as an approximation of ReLU, we backpropagate the error of the SNN in the proxy ANN to update the shared weights, simply by replacing the ANN final output with that of the SNN. We applied the proposed proxy learning to deep convolutional SNNs and evaluated it on two benchmarked datasets of Fahion-MNIST and Cifar10 with 94.56% and 93.11% classification accuracy, respectively. The proposed networks could outperform other deep SNNs trained with tandem learning, surrogate gradient learning, or converted from deep ANNs. Converted SNNs require long simulation times to reach reasonable accuracies while our proxy learning leads to efficient SNNs with much shorter simulation times. </details>
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 226、Brian2Loihi：神经形态芯片Loihi的仿真器，使用脉冲神经网络仿真器Brian
- [ ] Brian2Loihi: An emulator for the neuromorphic chip Loihi using the spiking neural network simulator Brian 
时间：2021年09月25日                         第一作者：Carlo Michaelis                       [链接](https://arxiv.org/abs/2109.12308).                     
## 摘要：开发智能神经形态解决方案仍然是一项具有挑战性的工作。它需要对硬件的基本构建块有坚实的概念性理解。除此之外，可访问且用户友好的原型设计对于加快设计流程至关重要。我们开发了一个基于神经网络模拟器Brian的开源Loihi模拟器，该模拟器可以很容易地整合到现有的仿真工作流中。我们在软件中演示了单个神经元和循环连接的脉冲神经网络的无错误Loihi仿真。片上学习也进行了审查和实施，由于随机舍入，存在合理的差异。这项工作对Loihi的计算单元进行了连贯的介绍，并介绍了一个新的、易于使用的Loihi原型软件包，旨在帮助简化新算法的概念化和部署。
<details>	<summary>英文摘要</summary>	Developing intelligent neuromorphic solutions remains a challenging endeavour. It requires a solid conceptual understanding of the hardware's fundamental building blocks. Beyond this, accessible and user-friendly prototyping is crucial to speed up the design pipeline. We developed an open source Loihi emulator based on the neural network simulator Brian that can easily be incorporated into existing simulation workflows. We demonstrate errorless Loihi emulation in software for a single neuron and for a recurrently connected spiking neural network. On-chip learning is also reviewed and implemented, with reasonable discrepancy due to stochastic rounding. This work provides a coherent presentation of Loihi's computational unit and introduces a new, easy-to-use Loihi prototyping package with the aim to help streamline conceptualisation and deployment of new algorithms. </details>
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 225、具有相变记忆突触的脉冲递归神经网络的在线训练
- [ ] Online Training of Spiking Recurrent Neural Networks with Phase-Change Memory Synapses 
时间：2021年09月25日                         第一作者：Yigit Demirag                       [链接](https://arxiv.org/abs/2108.01804).                     
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 224、最大化相互信息的感知系统的生物学上合理的学习规则
- [ ] Biologically Plausible Learning Rules for Perceptual Systems that Maximize Mutual Information 
时间：2021年09月07日                         第一作者：Tao Liu                       [链接](https://arxiv.org/abs/2109.13102).                     
## 摘要：人们普遍认为，生物体的感知系统是针对其所处环境的特性而优化的。这一原理的一个具体例子称为Infomax原理，认为早期感知处理的目的是最大化神经编码和传入感觉信号之间的互信息。在本文中，我们展示了一个利用时空局部、基于脉冲和连续时间学习规则精确实现这一原理的模型。
<details>	<summary>英文摘要</summary>	It is widely believed that the perceptual system of an organism is optimized for the properties of the environment to which it is exposed. A specific instance of this principle known as the Infomax principle holds that the purpose of early perceptual processing is to maximize the mutual information between the neural coding and the incoming sensory signal. In this article, we show a model to implement this principle accurately with spatio-temporal local, spike-based, and continuous-time learning rules. </details>
<details>	<summary>邮件日期</summary>	2021年09月28日</details>

# 223、通过正则化训练无神经元破裂或死亡的深脉冲自动编码器
- [ ] Training Deep Spiking Auto-encoders without Bursting or Dying Neurons through Regularization 
时间：2021年09月22日                         第一作者：Justus F. H\"ubotter                       [链接](https://arxiv.org/abs/2109.11045).                     
## 摘要：脉冲神经网络是计算神经科学中下一代大脑模型的一种很有前途的方法。此外，与经典的人工神经网络相比，它们可以在专门的神经形态硬件中实现快速计算，从而成为AI的节能部署。然而，训练深度脉冲神经网络，特别是以无监督的方式训练，是一项挑战，而且脉冲模型的性能受到死亡或爆裂神经元的显著阻碍。在这里，我们将基于膜电位的反向传播的端到端学习应用于具有多个可训练的漏积分和激发神经元层的脉冲卷积自动编码器。我们提出了仿生正则化方法来控制潜在表征中的脉冲密度。在实验中，我们发现对膜电位和脉冲输出应用正则化成功地避免了神经元死亡和破裂，并显著降低了脉冲自动编码器的重建误差。在MNIST数据集上训练正则化网络可产生与非脉冲基线模型（确定性和变分自动编码器）相当的图像重建质量，并表明比早期方法有所改进。重要的是，我们表明，与变分自动编码器不同，脉冲潜在表示显示与图像类相关的结构。
<details>	<summary>英文摘要</summary>	Spiking neural networks are a promising approach towards next-generation models of the brain in computational neuroscience. Moreover, compared to classic artificial neural networks, they could serve as an energy-efficient deployment of AI by enabling fast computation in specialized neuromorphic hardware. However, training deep spiking neural networks, especially in an unsupervised manner, is challenging and the performance of a spiking model is significantly hindered by dead or bursting neurons. Here, we apply end-to-end learning with membrane potential-based backpropagation to a spiking convolutional auto-encoder with multiple trainable layers of leaky integrate-and-fire neurons. We propose bio-inspired regularization methods to control the spike density in latent representations. In the experiments, we show that applying regularization on membrane potential and spiking output successfully avoids both dead and bursting neurons and significantly decreases the reconstruction error of the spiking auto-encoder. Training regularized networks on the MNIST dataset yields image reconstruction quality comparable to non-spiking baseline models (deterministic and variational auto-encoder) and indicates improvement upon earlier approaches. Importantly, we show that, unlike the variational auto-encoder, the spiking latent representations display structure associated with the image class. </details>
<details>	<summary>注释</summary>	Under review </details>
<details>	<summary>邮件日期</summary>	2021年09月24日</details>

# 222、在Intel的神经形态硬件Loihi上映射和验证点神经元模型
- [ ] Mapping and Validating a Point Neuron Model on Intel's Neuromorphic Hardware Loihi 
时间：2021年09月22日                         第一作者：Srijanie Dey                        [链接](https://arxiv.org/abs/2109.10835).                     
## 摘要：神经形态硬件基于模拟大脑的自然生物结构。由于其计算模型与标准神经模型相似，因此它可以作为神经科学和人工智能领域（包括生物医学应用）研究项目的计算加速工具。然而，为了开发这一新一代计算机芯片，必须对基于大脑的实验数据进行严格的模拟和随后的验证。在这项工作中，我们研究了英特尔第五代神经形态芯片“Loihi”的潜力，该芯片基于脉冲神经网络（SNN）模拟大脑神经元的新思想。这项工作是在模拟基于与丰富的解剖、生理和行为约束数据集相匹配的小鼠初级视觉皮层的泄漏集成和火灾（LIF）模型的背景下进行的。经典硬件上的仿真作为神经形态实现的验证平台。我们发现，Loihi非常有效地复制了经典模拟，并且随着网络变大，它在时间和能量性能方面的可扩展性显著提高。
<details>	<summary>英文摘要</summary>	Neuromorphic hardware is based on emulating the natural biological structure of the brain. Since its computational model is similar to standard neural models, it could serve as a computational acceleration for research projects in the field of neuroscience and artificial intelligence, including biomedical applications. However, in order to exploit this new generation of computer chips, rigorous simulation and consequent validation of brain-based experimental data is imperative. In this work, we investigate the potential of Intel's fifth generation neuromorphic chip - `Loihi', which is based on the novel idea of Spiking Neural Networks (SNNs) emulating the neurons in the brain. The work is implemented in context of simulating the Leaky Integrate and Fire (LIF) models based on the mouse primary visual cortex matched to a rich data set of anatomical, physiological and behavioral constraints. Simulations on the classical hardware serve as the validation platform for the neuromorphic implementation. We find that Loihi replicates classical simulations very efficiently and scales notably well in terms of both time and energy performance as the networks get larger. </details>
<details>	<summary>邮件日期</summary>	2021年09月23日</details>

# 221、通过结构学习：深入神经形态知识图嵌入
- [ ] Learning through structure: towards deep neuromorphic knowledge graph embeddings 
时间：2021年09月21日                         第一作者：Victor Caceres Chian                       [链接](https://arxiv.org/abs/2109.10376).                     
## 摘要：在从分子合成到社会网络分析和推荐系统的许多工业和学术应用中，计算图结构数据的潜在表示是一项普遍存在的学习任务。知识图是与语义Web相关的最流行和最广泛使用的数据表示形式之一。除了以机器可读的格式构造事实知识之外，知识图还充当许多人工智能应用程序的主干，并允许将上下文信息吸收到各种学习算法中。图神经网络试图通过相邻节点之间的消息传递启发式在低维向量空间中编码图结构。近年来，许多不同的图形神经网络结构在许多学习任务中表现出开创性的性能。在这项工作中，我们提出了一种策略，将用于知识图推理的深度图学习体系结构映射到神经形态体系结构。基于随机初始化和未训练（即冻结）图神经网络能够保持局部图结构的认识，我们构造了一个具有浅知识图嵌入模型的冻结神经网络。我们的实验表明，在传统的计算硬件上，这会导致显著的加速和内存减少，同时保持有竞争力的性能水平。此外，我们将冻结结构扩展到脉冲神经网络，引入了一种新的、基于事件的、高度稀疏的知识图嵌入算法，该算法适合在神经形态硬件中实现。
<details>	<summary>英文摘要</summary>	Computing latent representations for graph-structured data is an ubiquitous learning task in many industrial and academic applications ranging from molecule synthetization to social network analysis and recommender systems. Knowledge graphs are among the most popular and widely used data representations related to the Semantic Web. Next to structuring factual knowledge in a machine-readable format, knowledge graphs serve as the backbone of many artificial intelligence applications and allow the ingestion of context information into various learning algorithms. Graph neural networks attempt to encode graph structures in low-dimensional vector spaces via a message passing heuristic between neighboring nodes. Over the recent years, a multitude of different graph neural network architectures demonstrated ground-breaking performances in many learning tasks. In this work, we propose a strategy to map deep graph learning architectures for knowledge graph reasoning to neuromorphic architectures. Based on the insight that randomly initialized and untrained (i.e., frozen) graph neural networks are able to preserve local graph structures, we compose a frozen neural network with shallow knowledge graph embedding models. We experimentally show that already on conventional computing hardware, this leads to a significant speedup and memory reduction while maintaining a competitive performance level. Moreover, we extend the frozen architecture to spiking neural networks, introducing a novel, event-based and highly sparse knowledge graph embedding algorithm that is suitable for implementation in neuromorphic hardware. </details>
<details>	<summary>注释</summary>	Accepted for publication at the International Conference on Neuromorphic Computing (ICNC 2021) </details>
<details>	<summary>邮件日期</summary>	2021年09月23日</details>

# 220、基于神经形态处理器的微型飞行器机载高度控制简约神经形态PID的设计与实现
- [ ] Design and implementation of a parsimonious neuromorphic PID for onboard altitude control for MAVs using neuromorphic processors 
时间：2021年09月21日                         第一作者：Stein Stroobants                       [链接](https://arxiv.org/abs/2109.10199).                     
## 摘要：机器人的神经形态传感和处理技术的巨大潜力促使研究人员和工程师们研究用于自主机器人（导航、障碍物检测和回避等）鲁棒可靠控制的新模型，特别是在无人机竞赛和攻击性机动等挑战性环境中的四旋翼机器人。使用脉冲神经网络，这些模型可以在神经形态硬件上运行，从而受益于出色的更新率和高能效。然而，低级控制器往往被忽略，并停留在神经形态环路之外。设计低级神经形态控制器对于消除标准PID至关重要，因此可以受益于关闭神经形态回路的所有优点。在本文中，我们提出了一种简约可调的神经形态PID控制器，该控制器具有最少数量的93个稀疏连接的神经元，以实现配备Intel Loihi神经形态芯片的四旋翼的自主机载高度控制。我们在一组实验中成功地证明了我们提出的网络的鲁棒性，其中要求四旋翼从起飞时达到目标高度。我们的结果证实了这种低级神经形态控制器的适用性，最终具有非常高的更新频率。
<details>	<summary>英文摘要</summary>	The great promises of neuromorphic sensing and processing for robotics have led researchers and engineers to investigate novel models for robust and reliable control of autonomous robots (navigation, obstacle detection and avoidance, etc.), especially for quadrotors in challenging contexts such as drone racing and aggressive maneuvers. Using spiking neural networks, these models can be run on neuromorphic hardware to benefit from outstanding update rates and high energy efficiency. Yet, low-level controllers are often neglected and remain outside of the neuromorphic loop. Designing low-level neuromorphic controllers is crucial to remove the standard PID, and therefore benefit from all the advantages of closing the neuromorphic loop. In this paper, we propose a parsimonious and adjustable neuromorphic PID controller, endowed with a minimal number of 93 neurons sparsely connected to achieve autonomous, onboard altitude control of a quadrotor equipped with Intel's Loihi neuromorphic chip. We successfully demonstrate the robustness of our proposed network in a set of experiments where the quadrotor is requested to reach a target altitude from take-off. Our results confirm the suitability of such low-level neuromorphic controllers, ultimately with a very high update frequency. </details>
<details>	<summary>注释</summary>	7 pages, 9 figures, conference </details>
<details>	<summary>邮件日期</summary>	2021年09月22日</details>

# 219、面向节能安全的边缘人工智能：一种跨层框架
- [ ] Towards Energy-Efficient and Secure Edge AI: A Cross-Layer Framework 
时间：2021年09月20日                         第一作者：Muhammad Shafique                       [链接](https://arxiv.org/abs/2109.09829).                     
## 摘要：安全和隐私问题以及需要定期处理的数据量将处理推到了计算系统的边缘。由于严格的内存和功率/能量限制，在资源受限的边缘设备上部署高级神经网络（NN），如深度神经网络（DNN）和脉冲神经网络（SNN），提供最先进的结果是一项挑战。此外，这些系统需要在各种安全和可靠性威胁下保持正确的功能。本文首先讨论了在不同系统层，即硬件（HW）和软件（SW）解决能源效率、可靠性和安全问题的现有方法。然后，我们讨论了如何通过硬件/软件级优化（如修剪、量化和近似）进一步提高边缘人工智能系统的性能（延迟）和能效。为了解决可靠性威胁（如永久性和瞬时性故障），我们强调了成本效益高的缓解技术，如故障感知培训和映射。此外，我们还简要讨论了解决安全威胁（如模型和数据损坏）的有效检测和保护技术。最后，我们将讨论如何将这些技术结合到一个集成的跨层框架中，以实现健壮且节能的边缘人工智能系统。
<details>	<summary>英文摘要</summary>	The security and privacy concerns along with the amount of data that is required to be processed on regular basis has pushed processing to the edge of the computing systems. Deploying advanced Neural Networks (NN), such as deep neural networks (DNNs) and spiking neural networks (SNNs), that offer state-of-the-art results on resource-constrained edge devices is challenging due to the stringent memory and power/energy constraints. Moreover, these systems are required to maintain correct functionality under diverse security and reliability threats. This paper first discusses existing approaches to address energy efficiency, reliability, and security issues at different system layers, i.e., hardware (HW) and software (SW). Afterward, we discuss how to further improve the performance (latency) and the energy efficiency of Edge AI systems through HW/SW-level optimizations, such as pruning, quantization, and approximation. To address reliability threats (like permanent and transient faults), we highlight cost-effective mitigation techniques, like fault-aware training and mapping. Moreover, we briefly discuss effective detection and protection techniques to address security threats (like model and data corruption). Towards the end, we discuss how these techniques can be combined in an integrated cross-layer framework for realizing robust and energy-efficient Edge AI systems. </details>
<details>	<summary>注释</summary>	To appear at the 40th IEEE/ACM International Conference on Computer-Aided Design (ICCAD), November 2021, Virtual Event </details>
<details>	<summary>邮件日期</summary>	2021年09月22日</details>

# 218、具有易训练性和鲁棒性的时间编码深脉冲神经网络
- [ ] Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance 
时间：2021年09月17日                         第一作者：Shibo Zhou                       [链接](https://arxiv.org/abs/1909.10837).                     
<details>	<summary>邮件日期</summary>	2021年09月20日</details>

# 217、具有共振和激发神经元的深脉冲神经网络
- [ ] Deep Spiking Neural Networks with Resonate-and-Fire Neurons 
时间：2021年09月16日                         第一作者：Badr AlKhamissi                       [链接](https://arxiv.org/abs/2109.08234).                     
## 摘要：在这项工作中，我们探索了一种新的脉冲神经网络（SNN）公式，其中共振和激发（RAF）神经元（Izhikevich，2001）通过反向传播进行梯度下降训练。RAF-SNN虽然在生物学上更合理，但在不同的网络配置中，使用相似或更少的参数，其性能可与机器学习文献中的传统模型相比或更高。引人注目的是，RAF-SNN在静态和动态条件下，对测试/训练时产生的噪声具有鲁棒性。与MNIST上的CNN相比，我们在测试时显示N（0，0.2）诱导噪声的绝对准确度高出25%。与N-MNIST上的LSTM相比，我们在训练时显示了70%的绝对准确度和20%的诱导噪声。
<details>	<summary>英文摘要</summary>	In this work, we explore a new Spiking Neural Network (SNN) formulation with Resonate-and-Fire (RAF) neurons (Izhikevich, 2001) trained with gradient descent via back-propagation. The RAF-SNN, while more biologically plausible, achieves performance comparable to or higher than conventional models in the Machine Learning literature across different network configurations, using similar or fewer parameters. Strikingly, the RAF-SNN proves robust against noise induced at testing/training time, under both static and dynamic conditions. Against CNN on MNIST, we show 25% higher absolute accuracy with N(0, 0.2) induced noise at testing time. Against LSTM on N-MNIST, we show 70% higher absolute accuracy with 20% induced noise at training time. </details>
<details>	<summary>注释</summary>	Preprint </details>
<details>	<summary>邮件日期</summary>	2021年09月20日</details>

# 216、基于加权神经元分配的脉冲神经网络视觉位置识别
- [ ] Spiking Neural Networks for Visual Place Recognition via Weighted Neuronal Assignments 
时间：2021年09月14日                         第一作者：Somayeh Hussaini                       [链接](https://arxiv.org/abs/2109.06452).                     
## 摘要：脉冲神经网络（SNN）既有令人信服的潜在优势，包括能源效率和低延迟，也有挑战，包括事件脉冲的不可微性。该领域的许多初步研究已经将深度神经网络转换为等效的SNN，但这种转换方法可能会否定从头开发的基于SNN的方法的一些潜在优势。高性能SNN的一个有希望的领域是模板匹配和图像识别。本研究介绍了第一个用于视觉位置识别（VPR）任务的高性能SNN：给定查询图像，SNN必须从参考图像列表中找到最接近的匹配。这一新系统的核心是一种新的分配方案，它通过对单个位置编码神经元进行上加权和对多个不同参考位置作出响应的“模糊”神经元进行下加权，实现了一种形式的模糊信息显著性。在具有挑战性的牛津RobotCar和Nordland数据集上的一系列实验中，我们表明，我们的SNN实现了与最先进和经典技术相当的VPR性能，并且随着参考位置数量的增加，性能逐渐下降。我们的研究结果为SNN提供了一个重要的里程碑，SNN可以提供健壮、节能和低延迟的机器人定位。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) offer both compelling potential advantages, including energy efficiency and low latencies, and challenges including the non-differentiable nature of event spikes. Much of the initial research in this area has converted deep neural networks to equivalent SNNs, but this conversion approach potentially negates some of the potential advantages of SNN-based approaches developed from scratch. One promising area for high performance SNNs is template matching and image recognition. This research introduces the first high performance SNN for the Visual Place Recognition (VPR) task: given a query image, the SNN has to find the closest match out of a list of reference images. At the core of this new system is a novel assignment scheme that implements a form of ambiguity-informed salience, by up-weighting single-place-encoding neurons and down-weighting "ambiguous" neurons that respond to multiple different reference places. In a range of experiments on the challenging Oxford RobotCar and Nordland datasets, we show that our SNN achieves comparable VPR performance to state-of-the-art and classical techniques, and degrades gracefully in performance with an increasing number of reference places. Our results provide a significant milestone towards SNNs that can provide robust, energy-efficient and low latency robot localization. </details>
<details>	<summary>注释</summary>	8 pages, 6 figures, under review </details>
<details>	<summary>邮件日期</summary>	2021年09月15日</details>

# 215、BioLCNet：报酬调制局部连接脉冲神经网络
- [ ] BioLCNet: Reward-modulated Locally Connected Spiking Neural Networks 
时间：2021年09月12日                         第一作者：Hafez Ghaemi                       [链接](https://arxiv.org/abs/2109.05539).                     
## 摘要：最近的研究表明，卷积神经网络（CNN）并不是唯一可行的图像分类方法。此外，CNN中使用的权重共享和反向传播并不符合灵长类视觉系统中存在的机制。为了提出一个生物学上更合理的解决方案，我们设计了一个局部连接的脉冲神经网络（SNN），该网络使用脉冲时间依赖性可塑性（STDP）及其报酬调制变量（R-STDP）学习规则进行训练。通过使用脉冲神经元和局部连接以及强化学习（RL），我们为我们提出的架构命名了BioLCNet。我们的网络由速率编码输入层、本地连接的隐藏层和解码输出层组成。输出层采用基于脉冲总体的投票方案进行解码。我们使用MNIST数据集获得图像分类精度，并评估奖励系统对不同目标响应的鲁棒性。
<details>	<summary>英文摘要</summary>	Recent studies have shown that convolutional neural networks (CNNs) are not the only feasible solution for image classification. Furthermore, weight sharing and backpropagation used in CNNs do not correspond to the mechanisms present in the primate visual system. To propose a more biologically plausible solution, we designed a locally connected spiking neural network (SNN) trained using spike-timing-dependent plasticity (STDP) and its reward-modulated variant (R-STDP) learning rules. The use of spiking neurons and local connections along with reinforcement learning (RL) led us to the nomenclature BioLCNet for our proposed architecture. Our network consists of a rate-coded input layer followed by a locally connected hidden layer and a decoding output layer. A spike population-based voting scheme is adopted for decoding in the output layer. We used the MNIST dataset to obtain image classification accuracy and to assess the robustness of our rewarding system to varying target responses. </details>
<details>	<summary>注释</summary>	8 pages, 5 figures ACM-class: I.2.6; I.5.1 </details>
<details>	<summary>邮件日期</summary>	2021年09月14日</details>

# 214、用于基于事件的光流估计的时空递归网络
- [ ] Spatio-Temporal Recurrent Networks for Event-Based Optical Flow Estimation 
时间：2021年09月10日                         第一作者：Ziluo Ding                       [链接](https://arxiv.org/abs/2109.04871).                     
## 摘要：事件摄像机为视觉感知提供了很有希望的替代方案，特别是在高速和高动态范围的场景中。最近，许多深度学习方法在为许多基于事件的问题（如光流估计）提供无模型解决方案方面取得了巨大成功。然而，现有的深度学习方法并没有从建筑设计的角度很好地解决时间信息的重要性，也不能有效地提取时空特征。另一个利用脉冲神经网络的研究领域存在更深层次架构的训练问题。为了解决这些问题，提出了一种新的输入表示法，用于捕获事件的时间分布以增强信号。此外，我们还介绍了一种用于基于事件的光流估计的时空递归编码-解码神经网络结构，该结构利用卷积选通递归单元从一系列事件图像中提取特征映射。此外，我们的架构允许合并一些传统的基于帧的核心模块，如相关层和迭代残差细化方案。该网络在多车辆立体事件摄像机数据集上通过自监督学习进行端到端训练。我们已经证明，它大大优于所有现有的最先进的方法。
<details>	<summary>英文摘要</summary>	Event camera has offered promising alternative for visual perception, especially in high speed and high dynamic range scenes. Recently, many deep learning methods have shown great success in providing model-free solutions to many event-based problems, such as optical flow estimation. However, existing deep learning methods did not address the importance of temporal information well from the perspective of architecture design and cannot effectively extract spatio-temporal features. Another line of research that utilizes Spiking Neural Network suffers from training issues for deeper architecture. To address these points, a novel input representation is proposed that captures the events temporal distribution for signal enhancement. Moreover, we introduce a spatio-temporal recurrent encoding-decoding neural network architecture for event-based optical flow estimation, which utilizes Convolutional Gated Recurrent Units to extract feature maps from a series of event images. Besides, our architecture allows some traditional frame-based core modules, such as correlation layer and iterative residual refine scheme, to be incorporated. The network is end-to-end trained with self-supervised learning on the Multi-Vehicle Stereo Event Camera dataset. We have shown that it outperforms all the existing state-of-the-art methods by a large margin. </details>
<details>	<summary>邮件日期</summary>	2021年09月13日</details>

# 213、HSMD：一种基于混合脉冲神经网络结构的目标运动检测算法
- [ ] HSMD: An object motion detection algorithm using a Hybrid Spiking Neural Network Architecture 
时间：2021年09月09日                         第一作者：Pedro Machado                       [链接](https://arxiv.org/abs/2109.04119).                     
## 摘要：运动物体的检测是脊椎动物视网膜执行的一项琐碎任务，但也是一项复杂的计算机视觉任务。物体运动敏感神经节细胞（OMS-GC）是视网膜中感知运动物体的特殊细胞。OMS-GC以连续信号作为输入，产生棘波模式作为输出，通过视神经传输到视皮层。本文提出的混合敏感运动检测器（HSMD）算法通过定制的3层脉冲神经网络（SNN）增强了GSOC动态背景减法（DBS）算法，该网络输出类似于OMS-GC的脉冲响应。该算法与OpenCV库中现有的背景减法（BS）方法进行了比较，特别是2012年变化检测（CDnet2012）和2014年变化检测（CDnet2014）基准数据集。结果表明，HSMD在所有竞争方法中总体排名第一，并且在所有八个测试指标中的四个类别上的性能优于所有其他算法。此外，本文提出的HSMD是第一个使用SNN来增强现有最先进的DBS（GSOC）算法的HSMD，结果表明SNN在实际应用中提供了接近实时的性能。
<details>	<summary>英文摘要</summary>	The detection of moving objects is a trivial task performed by vertebrate retinas, yet a complex computer vision task. Object-motion-sensitive ganglion cells (OMS-GC) are specialised cells in the retina that sense moving objects. OMS-GC take as input continuous signals and produce spike patterns as output, that are transmitted to the Visual Cortex via the optic nerve. The Hybrid Sensitive Motion Detector (HSMD) algorithm proposed in this work enhances the GSOC dynamic background subtraction (DBS) algorithm with a customised 3-layer spiking neural network (SNN) that outputs spiking responses akin to the OMS-GC. The algorithm was compared against existing background subtraction (BS) approaches, available on the OpenCV library, specifically on the 2012 change detection (CDnet2012) and the 2014 change detection (CDnet2014) benchmark datasets. The results show that the HSMD was ranked overall first among the competing approaches and has performed better than all the other algorithms on four of the categories across all the eight test metrics. Furthermore, the HSMD proposed in this paper is the first to use an SNN to enhance an existing state of the art DBS (GSOC) algorithm and the results demonstrate that the SNN provides near real-time performance in realistic applications. </details>
<details>	<summary>邮件日期</summary>	2021年09月10日</details>

# 212、Nengo上使用脉冲神经网络的稀疏分布记忆
- [ ] Sparse Distributed Memory using Spiking Neural Networks on Nengo 
时间：2021年09月07日                         第一作者：Rohan Deepak Ajwani                       [链接](https://arxiv.org/abs/2109.03111).                     
## 摘要：我们提出了一种基于脉冲神经网络（SNN）的稀疏分布内存（SDM）实现的Nengo框架。我们的工作基于Furber等人2004年以前的工作，使用N-of-M代码实现SDM。作为SDM设计的一个组成部分，我们在Nengo上使用SNN实现了相关矩阵存储器（CMM）。我们的SNN实现在Nengo上使用泄漏集成和火灾（LIF）脉冲神经元模型。我们的目标是了解基于SNN的SDM与传统SDM相比的性能。为此，我们在Nengo上模拟了传统和基于SNN的SDM和CMM。我们观察到，基于SNN的模型的性能与传统模型类似。为了评估不同SNN的性能，我们使用自适应LIF、脉冲校正线性单元和Izhikevich模型重复了实验，并获得了类似的结果。我们的结论是，使用脉冲神经元开发某些类型的联想记忆确实是可行的，这些神经元的记忆容量和其他特征与没有SNN时的性能相似。最后，我们实现了一个应用程序，其中使用N-of-M代码编码的MNIST图像与其标签关联并存储在基于SNN的SDM中。
<details>	<summary>英文摘要</summary>	We present a Spiking Neural Network (SNN) based Sparse Distributed Memory (SDM) implemented on the Nengo framework. We have based our work on previous work by Furber et al, 2004, implementing SDM using N-of-M codes. As an integral part of the SDM design, we have implemented Correlation Matrix Memory (CMM) using SNN on Nengo. Our SNN implementation uses Leaky Integrate and Fire (LIF) spiking neuron models on Nengo. Our objective is to understand how well SNN-based SDMs perform in comparison to conventional SDMs. Towards this, we have simulated both conventional and SNN-based SDM and CMM on Nengo. We observe that SNN-based models perform similarly as the conventional ones. In order to evaluate the performance of different SNNs, we repeated the experiment using Adaptive-LIF, Spiking Rectified Linear Unit, and Izhikevich models and obtained similar results. We conclude that it is indeed feasible to develop some types of associative memories using spiking neurons whose memory capacity and other features are similar to the performance without SNNs. Finally we have implemented an application where MNIST images, encoded with N-of-M codes, are associated with their labels and stored in the SNN-based SDM. </details>
<details>	<summary>注释</summary>	8 pages, 11 figures, accepted as poster in Bernstein Conference 2021 ACM-class: H.3.2; I.5.5 </details>
<details>	<summary>邮件日期</summary>	2021年09月08日</details>

# 211、具有易训练性和鲁棒性的时间编码深脉冲神经网络
- [ ] Temporal-Coded Deep Spiking Neural Network with Easy Training and Robust Performance 
时间：2021年09月06日                         第一作者：Shibo Zhou                       [链接](https://arxiv.org/abs/1909.10837).                     
<details>	<summary>邮件日期</summary>	2021年09月07日</details>

# 210、脉冲神经网络集成
- [ ] Ensembles of Spiking Neural Networks 
时间：2021年09月06日                         第一作者：Georgiana Neculae                       [链接](https://arxiv.org/abs/2010.14619).                     
<details>	<summary>注释</summary>	16 pages, 3 tables, 5 figures MSC-class: 68T07, 62J12 </details>
<details>	<summary>邮件日期</summary>	2021年09月07日</details>

# 209、用于序列学习的具有改进固有递归动力学的脉冲神经网络
- [ ] Spiking Neural Networks with Improved Inherent Recurrence Dynamics for Sequential Learning 
时间：2021年09月04日                         第一作者：Wachirawit Ponghiran                        [链接](https://arxiv.org/abs/2109.01905).                     
## 摘要：具有泄漏集成和激发（LIF）神经元的脉冲神经网络（SNN）可以以事件驱动的方式运行，并具有随时间保留信息的内部状态，为节能的神经形态计算提供了机会，特别是在边缘设备上。然而，值得注意的是，许多关于SNN的有代表性的工作并没有充分证明其固有的重复性（保留过去信息的膜电位）对于顺序学习的有用性。大多数工作训练SNN识别静态图像通过人工扩展输入表示时间通过率编码。我们证明了SNN可以被训练用于序列任务，并提出了对LIF神经元网络的修改，使内部状态能够学习长序列，并使其固有的重现性能够适应消失梯度问题。然后，我们开发了一个训练方案，用改进的固有递归动力学来训练所提出的SNN。我们的训练方案允许脉冲神经元产生多位输出（与二进制脉冲相反），这有助于缓解脉冲神经元激活函数的导数与用于克服脉冲神经元不可微性的替代导数之间的失配。我们的实验结果表明，在TIMIT和LibriSpeech 100h数据集上提出的SNN体系结构的精度与LSTMs相当（分别在1.10%和0.36%范围内），但参数比LSTMs少2倍。在TIMIT和LibriSpeech 100h数据集上，与GRU相比，稀疏SNN输出还导致乘法运算节省10.13倍和11.14倍，GRU通常被视为LSTM的轻量级替代品。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) with leaky integrate and fire (LIF) neurons, can be operated in an event-driven manner and have internal states to retain information over time, providing opportunities for energy-efficient neuromorphic computing, especially on edge devices. Note, however, many representative works on SNNs do not fully demonstrate the usefulness of their inherent recurrence (membrane potentials retaining information about the past) for sequential learning. Most of the works train SNNs to recognize static images by artificially expanded input representation in time through rate coding. We show that SNNs can be trained for sequential tasks and propose modifications to a network of LIF neurons that enable internal states to learn long sequences and make their inherent recurrence resilient to the vanishing gradient problem. We then develop a training scheme to train the proposed SNNs with improved inherent recurrence dynamics. Our training scheme allows spiking neurons to produce multi-bit outputs (as opposed to binary spikes) which help mitigate the mismatch between a derivative of spiking neurons' activation function and a surrogate derivative used to overcome spiking neurons' non-differentiability. Our experimental results indicate that the proposed SNN architecture on TIMIT and LibriSpeech 100h dataset yields accuracy comparable to that of LSTMs (within 1.10% and 0.36%, respectively), but with 2x fewer parameters than LSTMs. The sparse SNN outputs also lead to 10.13x and 11.14x savings in multiplication operations compared to GRUs, which is generally con-sidered as a lightweight alternative to LSTMs, on TIMIT and LibriSpeech 100h datasets, respectively. </details>
<details>	<summary>邮件日期</summary>	2021年09月07日</details>

# 208、卷积脉冲神经网络中基于脉冲时间位移的误差反向传播
- [ ] Spike time displacement based error backpropagation in convolutional spiking neural networks 
时间：2021年08月31日                         第一作者：Maryam Mirsadeghi                       [链接](https://arxiv.org/abs/2108.13621).                     
## 摘要：我们最近提出了STiDi BP算法，该算法避免了向后递归梯度计算，用于训练具有单脉冲时间编码的多层脉冲神经网络（SNN）。该算法采用线性近似来计算脉冲潜伏期相对于膜电位的导数，并使用具有分段线性突触后电位的脉冲神经元来降低计算成本和神经处理的复杂性。在本文中，我们扩展了STiDi-BP算法，将其应用于更深层次的卷积结构。基于两个流行基准MNIST和Fashion MNIST数据集的图像分类任务的评估结果表明，该算法的准确率分别为99.2%和92.8%，证实了该算法在深度SNN中的适用性。我们考虑的另一个问题是内存存储和计算成本的减少。要做到这一点，我们考虑卷积SNN（CSNN）与两组权重：实值权重，更新在后向传递和它们的符号，二进制权重，在前馈过程中使用。我们在两个数据集MNIST和Fashion MNIST上对二进制CSNN进行了评估，并获得了可接受的性能，与实值权重相关的精度下降可以忽略不计（分别约为$0.6%$和$0.8%$）。
<details>	<summary>英文摘要</summary>	We recently proposed the STiDi-BP algorithm, which avoids backward recursive gradient computation, for training multi-layer spiking neural networks (SNNs) with single-spike-based temporal coding. The algorithm employs a linear approximation to compute the derivative of the spike latency with respect to the membrane potential and it uses spiking neurons with piecewise linear postsynaptic potential to reduce the computational cost and the complexity of neural processing. In this paper, we extend the STiDi-BP algorithm to employ it in deeper and convolutional architectures. The evaluation results on the image classification task based on two popular benchmarks, MNIST and Fashion-MNIST datasets with the accuracies of respectively 99.2% and 92.8%, confirm that this algorithm has been applicable in deep SNNs. Another issue we consider is the reduction of memory storage and computational cost. To do so, we consider a convolutional SNN (CSNN) with two sets of weights: real-valued weights that are updated in the backward pass and their signs, binary weights, that are employed in the feedforward process. We evaluate the binary CSNN on two datasets of MNIST and Fashion-MNIST and obtain acceptable performance with a negligible accuracy drop with respect to real-valued weights (about $0.6%$ and $0.8%$ drops, respectively). </details>
<details>	<summary>邮件日期</summary>	2021年09月01日</details>

# 207、星形胶质细胞介导多层神经元-星形胶质细胞网络中的类似记忆
- [ ] Astrocytes mediate analogous memory in a multi-layer neuron-astrocytic network 
时间：2021年08月31日                         第一作者：Yuliya Tsybina                       [链接](https://arxiv.org/abs/2108.13414).                     
## 摘要：短期工作记忆的神经过程建模一直是神经科学许多理论研究的重点。在这里，我们提出了一个脉冲神经元网络（SNN）的数学模型，演示了一段信息如何作为一种健壮的活动模式保持几秒钟，然后在没有其他刺激的情况下完全消失。由于伴随SNN的星形胶质细胞的激活，这种短期记忆痕迹得以保留。星形胶质细胞以秒的时间尺度呈现钙瞬变。这些瞬变进一步调节突触传递的效率，从而通过释放胶质传递素在不同时间尺度上调节相邻神经元的放电频率。我们展示了这种瞬变如何持续编码神经元放电的频率，并提供了类似信息的可靠短期存储。这种短期记忆可以将操作信息保留几秒钟，然后完全忘记，以避免与即将出现的模式重叠。SNN通过局部细胞间扩散连接与星形细胞层相互连接。星形胶质细胞只有在相邻神经元同步激发时才被激活，例如当信息模式被加载时。为了举例说明，我们拍摄了人们脸部的灰度照片，其中的灰度编码了刺激神经元的外加电流水平。星形胶质细胞反馈通过改变神经元放电频率来调节（促进）突触传递。我们展示了如何加载任意模式，然后将其存储一定的时间间隔，并在将适当的线索模式应用于输入时进行检索。
<details>	<summary>英文摘要</summary>	Modeling the neuronal processes underlying short-term working memory remains the focus of many theoretical studies in neuroscience. Here we propose a mathematical model of spiking neuron network (SNN) demonstrating how a piece of information can be maintained as a robust activity pattern for several seconds then completely disappear if no other stimuli come. Such short-term memory traces are preserved due to the activation of astrocytes accompanying the SNN. The astrocytes exhibit calcium transients at a time scale of seconds. These transients further modulate the efficiency of synaptic transmission and, hence, the firing rate of neighboring neurons at diverse timescales through gliotransmitter release. We show how such transients continuously encode frequencies of neuronal discharges and provide robust short-term storage of analogous information. This kind of short-term memory can keep operative information for seconds, then completely forget it to avoid overlapping with forthcoming patterns. The SNN is inter-connected with the astrocytic layer by local inter-cellular diffusive connections. The astrocytes are activated only when the neighboring neurons fire quite synchronously, e.g. when an information pattern is loaded. For illustration, we took greyscale photos of people's faces where the grey level encoded the level of applied current stimulating the neurons. The astrocyte feedback modulates (facilitates) synaptic transmission by varying the frequency of neuronal firing. We show how arbitrary patterns can be loaded, then stored for a certain interval of time, and retrieved if the appropriate clue pattern is applied to the input. </details>
<details>	<summary>注释</summary>	18 pages, 6 figures, 1 table, Appendix </details>
<details>	<summary>邮件日期</summary>	2021年09月01日</details>

# 206、基于旋转不变卷积的大规模点云位置识别
- [ ] Attentive Rotation Invariant Convolution for Point Cloud-based Large Scale Place Recognition 
时间：2021年08月29日                         第一作者：Zhaoxin Fan                       [链接](https://arxiv.org/abs/2108.12790).                     
## 摘要：自动驾驶和同步定位与地图（SLAM）在现实世界中变得越来越重要，基于点云的大规模地点识别是其中的一个亮点。以往的位置识别方法都将该任务视为一个点云检索问题，取得了良好的性能。然而，它们都有一个共同的缺陷：它们无法处理旋转点云时的情况，这是常见的，例如，当视点或摩托车类型发生变化时。为了解决这个问题，本文提出了一种注意旋转不变卷积（ARIConv）。ARIConv在其结构中采用了三种旋转不变特征（RIF）：球形信号（SS）、单个局部旋转不变特征（ILRIF）和组局部旋转不变特征（GLRIF）来学习旋转不变卷积核，这些特征对旋转不变点云特征的学习具有鲁棒性。更重要的是，为了突出关键的RIF，我们在ARIConv中注入了一个关注模块，在学习内核时赋予不同的RIF不同的重要性。最后，利用ARIConv，我们构建了一个类似DenseNet的网络结构来学习用于检索的旋转不敏感全局描述符。我们的实验表明，当旋转点云扫描时，我们的模型可以在大规模位置识别任务中实现最先进的性能，并且可以在原始非旋转数据集上实现与大多数现有方法相当的结果。
<details>	<summary>英文摘要</summary>	Autonomous Driving and Simultaneous Localization and Mapping(SLAM) are becoming increasingly important in real world, where point cloud-based large scale place recognition is the spike of them. Previous place recognition methods have achieved acceptable performances by regarding the task as a point cloud retrieval problem. However, all of them are suffered from a common defect: they can't handle the situation when the point clouds are rotated, which is common, e.g, when viewpoints or motorcycle types are changed. To tackle this issue, we propose an Attentive Rotation Invariant Convolution (ARIConv) in this paper. The ARIConv adopts three kind of Rotation Invariant Features (RIFs): Spherical Signals (SS), Individual-Local Rotation Invariant Features (ILRIF) and Group-Local Rotation Invariant features (GLRIF) in its structure to learn rotation invariant convolutional kernels, which are robust for learning rotation invariant point cloud features. What's more, to highlight pivotal RIFs, we inject an attentive module in ARIConv to give different RIFs different importance when learning kernels. Finally, utilizing ARIConv, we build a DenseNet-like network architecture to learn rotation-insensitive global descriptors used for retrieving. We experimentally demonstrate that our model can achieve state-of-the-art performance on large scale place recognition task when the point cloud scans are rotated and can achieve comparable results with most of existing methods on the original non-rotated datasets. </details>
<details>	<summary>注释</summary>	10 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年08月31日</details>

# 205、热动力学介导的随机磁性隧道结的内在脉冲时间依赖塑性
- [ ] Intrinsic Spike Timing Dependent Plasticity in Stochastic Magnetic Tunnel Junctions Mediated by Heat Dynamics 
时间：2021年08月28日                         第一作者：Humberto Inzunza Velarde                       [链接](https://arxiv.org/abs/2108.12684).                     
## 摘要：对高效认知计算的追求引起了神经形态计算领域的广泛研究兴趣。神经形态计算旨在利用固态设备和电路模拟生物神经元和突触的行为。在各种方法中，新兴的非易失性记忆技术对模拟神经突触行为特别感兴趣。这些设备可以将生物神经元和突触的丰富动态映射到其固有的设备物理上。在这封信中，我们重点研究了生物突触的脉冲时间依赖性可塑性（STDP）行为，并提出了一种在磁隧道结（MTJ）器件中实现STDP行为的方法。具体而言，我们利用与时间相关的热动力学和MTJ对瞬时温度的响应来模拟STDP行为。我们的模拟基于磁化动力学的宏观自旋模型，结果表明，通过将简单的电压波形作为MTJ器件前后神经元的脉冲响应，可以在随机磁性隧道结中模拟STDP。
<details>	<summary>英文摘要</summary>	The quest for highly efficient cognitive computing has led to extensive research interest for the field of neuromorphic computing. Neuromorphic computing aims to mimic the behavior of biological neurons and synapses using solid-state devices and circuits. Among various approaches, emerging non-volatile memory technologies are of special interest for mimicking neuro-synaptic behavior. These devices allow the mapping of the rich dynamics of biological neurons and synapses onto their intrinsic device physics. In this letter, we focus on Spike Timing Dependent Plasticity (STDP) behavior of biological synapses and propose a method to implement the STDP behavior in Magnetic Tunnel Junction (MTJ) devices. Specifically, we exploit the time-dependent heat dynamics and the response of an MTJ to the instantaneous temperature to imitate the STDP behavior. Our simulations, based on a macro-spin model for magnetization dynamics, show that, STDP can be imitated in stochastic magnetic tunnel junctions by applying simple voltage waveforms as the spiking response of pre- and post-neurons across an MTJ device. </details>
<details>	<summary>邮件日期</summary>	2021年08月31日</details>

# 204、一种将脉冲神经网络映射到多核神经形态硬件的设计流程
- [ ] A Design Flow for Mapping Spiking Neural Networks to Many-Core Neuromorphic Hardware 
时间：2021年08月27日                         第一作者：Shihao Song                       [链接](https://arxiv.org/abs/2108.12444).                     
## 摘要：许多核心神经形态硬件的设计正变得越来越复杂，因为这些系统预计将执行大型机器学习模型。为了解决设计的复杂性，需要一个可预测的设计流程来保证实时性能，例如延迟和吞吐量，而不会显著增加计算核心的缓冲区需求。同步数据流图（SDFG）用于流应用程序到多处理器系统的可预测映射。我们提出了一种基于SDFG的设计流程，用于将脉冲神经网络（SNN）映射到多个核心神经形态硬件，目的是探索吞吐量和缓冲区大小之间的权衡。提出的设计流程集成了一种基于Kernighan-Lin图划分启发式的迭代划分方法，创建SNN集群，使每个集群都可以映射到硬件的核心。分区方法最小化了集群间的脉冲通信，从而提高了硬件共享互连的延迟。接下来，设计流程使用粒子群优化（PSO）的一个实例（一种进化算法）将集群映射到核心，探索吞吐量和缓冲区大小的设计空间。Pareto最优映射保留在设计流中，允许系统设计者选择满足设计的吞吐量和缓冲区大小要求的Pareto映射。我们使用五个大型卷积神经网络（CNN）模型评估了设计流程。结果表明，与最先进的基于数据流的映射解决方案相比，最大吞吐量提高63%，缓冲区大小要求降低10%。
<details>	<summary>英文摘要</summary>	The design of many-core neuromorphic hardware is getting more and more complex as these systems are expected to execute large machine learning models. To deal with the design complexity, a predictable design flow is needed to guarantee real-time performance such as latency and throughput without significantly increasing the buffer requirement of computing cores. Synchronous Data Flow Graphs (SDFGs) are used for predictable mapping of streaming applications to multiprocessor systems. We propose an SDFG-based design flow for mapping spiking neural networks (SNNs) to many-core neuromorphic hardware with the objective of exploring the tradeoff between throughput and buffer size. The proposed design flow integrates an iterative partitioning approach, based on Kernighan-Lin graph partitioning heuristic, creating SNN clusters such that each cluster can be mapped to a core of the hardware. The partitioning approach minimizes the inter-cluster spike communication, which improves latency on the shared interconnect of the hardware. Next, the design flow maps clusters to cores using an instance of the Particle Swarm Optimization (PSO), an evolutionary algorithm, exploring the design space of throughput and buffer size. Pareto optimal mappings are retained from the design flow, allowing system designers to select a Pareto mapping that satisfies throughput and buffer size requirements of the design. We evaluated the design flow using five large-scale convolutional neural network (CNN) models. Results demonstrate 63% higher maximum throughput and 10% lower buffer size requirement compared to state-of-the-art dataflow-based mapping solutions. </details>
<details>	<summary>注释</summary>	To appear in ICCAD 2021 </details>
<details>	<summary>邮件日期</summary>	2021年08月31日</details>

# 203、反向传播算法在Spiking神经形态硬件上的实现
- [ ] The Backpropagation Algorithm Implemented on Spiking Neuromorphic Hardware 
时间：2021年08月26日                         第一作者：Alpha Renner                       [链接](https://arxiv.org/abs/2106.07030).                     
<details>	<summary>注释</summary>	21 pages, 5 figures, Changes v1->v2: minor changes of text and formatting, correction of total power in supplementary Table III Report-no: LA-UR-21-24457 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年08月27日</details>

# 202、脉冲神经网络的Hessian感知量化
- [ ] Hessian Aware Quantization of Spiking Neural Networks 
时间：2021年08月23日                         第一作者：Hin Wai Lui                        [链接](https://arxiv.org/abs/2104.14117).                     
<details>	<summary>邮件日期</summary>	2021年08月25日</details>

# 201、ReSpawn：考虑不可靠记忆的脉冲神经网络的节能容错
- [ ] ReSpawn: Energy-Efficient Fault-Tolerance for Spiking Neural Networks considering Unreliable Memories 
时间：2021年08月23日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2108.10271).                     
## 摘要：由于其受生物启发的计算能力，脉冲神经网络（SNN）具有低能量和无监督学习能力的潜力。然而，如果在存储器中存在硬件引起的故障（可能来自制造缺陷或电压引起的近似误差）的情况下执行它们的处理，则它们可能会受到精度降低的影响。由于最近的工作仍然集中于SNN中的故障建模和随机故障注入，因此SNN硬件架构中的内存故障对准确性的影响以及相应的故障缓解技术没有得到彻底的探讨。为此，我们提出了ReSpawn，这是一个新的框架，用于缓解弹性和节能SNN的片外和片内存储器中故障的负面影响。ReSpawn的关键机制是：（1）分析SNNs的容错性；（2）通过（a）存储器中的故障感知映射（FAM）和（b）故障感知训练和映射（FATM）提高SNN容错性。如果训练数据集不完全可用，则通过有效的位洗牌技术使用FAM，将有效位放在非故障存储单元上，将不重要位放在故障存储单元上，同时最小化内存访问能量。同时，如果训练数据集完全可用，则在数据映射和训练过程中，通过考虑故障存储单元，采用FATM。实验结果表明，与没有故障缓解技术的基线SNN相比，使用故障感知映射方案的ReSpawn在没有重新训练的情况下，对于包含900个神经元的网络，准确率提高了70%。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have shown a potential for having low energy with unsupervised learning capabilities due to their biologically-inspired computation. However, they may suffer from accuracy degradation if their processing is performed under the presence of hardware-induced faults in memories, which can come from manufacturing defects or voltage-induced approximation errors. Since recent works still focus on the fault-modeling and random fault injection in SNNs, the impact of memory faults in SNN hardware architectures on accuracy and the respective fault-mitigation techniques are not thoroughly explored. Toward this, we propose ReSpawn, a novel framework for mitigating the negative impacts of faults in both the off-chip and on-chip memories for resilient and energy-efficient SNNs. The key mechanisms of ReSpawn are: (1) analyzing the fault tolerance of SNNs; and (2) improving the SNN fault tolerance through (a) fault-aware mapping (FAM) in memories, and (b) fault-aware training-and-mapping (FATM). If the training dataset is not fully available, FAM is employed through efficient bit-shuffling techniques that place the significant bits on the non-faulty memory cells and the insignificant bits on the faulty ones, while minimizing the memory access energy. Meanwhile, if the training dataset is fully available, FATM is employed by considering the faulty memory cells in the data mapping and training processes. The experimental results show that, compared to the baseline SNN without fault-mitigation techniques, ReSpawn with a fault-aware mapping scheme improves the accuracy by up to 70% for a network with 900 neurons without retraining. </details>
<details>	<summary>注释</summary>	To appear at the 40th IEEE/ACM International Conference on Computer-Aided Design (ICCAD), November 2021, Virtual Event </details>
<details>	<summary>邮件日期</summary>	2021年08月24日</details>

# 200、结合可学习膜时间常数增强脉冲神经网络的学习
- [ ] Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks 
时间：2021年08月17日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2007.05785).                     
<details>	<summary>注释</summary>	Accepted by International Conference on Computer Vision (ICCV) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年08月18日</details>

# 199、用脉冲神经网络进行类比推理和关系推理
- [ ] Analogical and Relational Reasoning with Spiking Neural Networks 
时间：2021年08月17日                         第一作者：Rollin Omari                       [链接](https://arxiv.org/abs/2010.06746).                     
<details>	<summary>注释</summary>	Problems were discovered with the details of the experiments involving the LSM neural network, the results reported were not correct. A new version of the paper is in progress with corrected results ACM-class: F.2.2; I.2.6; I.2.10; I.5.1; I.5.3 </details>
<details>	<summary>邮件日期</summary>	2021年08月18日</details>

# 198、脉冲神经元的线性约束学习
- [ ] Linear Constraints Learning for Spiking Neurons 
时间：2021年08月11日                         第一作者：Huy Le Nguyen                       [链接](https://arxiv.org/abs/2103.12564).                     
<details>	<summary>注释</summary>	35 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年08月12日</details>

# 197、鲁棒视觉目标跟踪的多域协同特征表示
- [ ] Multi-domain Collaborative Feature Representation for Robust Visual Object Tracking 
时间：2021年08月10日                         第一作者：Jiqing Zhang                        [链接](https://arxiv.org/abs/2108.04521).                     
## 摘要：联合利用多个不同但互补的领域信息已被证明是执行鲁棒目标跟踪的有效方法。本文致力于在挑战场景中有效地表示和利用帧域和事件域的互补特征来提高目标跟踪性能。具体来说，我们提出了公共特征提取器（CFE）来从RGB域和事件域学习潜在的公共表示。为了学习这两个领域的独特特征，我们利用一种基于脉冲神经网络的独特事件提取器（UEE）来提取事件领域中在某些挑战性条件下RGB可能遗漏的边缘线索，基于深度卷积神经网络的RGB（UER）抽取器用于提取RGB域的纹理和语义信息。在标准RGB基准和真实事件跟踪数据集上的大量实验证明了该方法的有效性。我们展示了我们的方法优于所有比较先进的跟踪算法，并验证了基于事件的数据是在具有挑战性的场景中进行跟踪的有力线索。
<details>	<summary>英文摘要</summary>	Jointly exploiting multiple different yet complementary domain information has been proven to be an effective way to perform robust object tracking. This paper focuses on effectively representing and utilizing complementary features from the frame domain and event domain for boosting object tracking performance in challenge scenarios. Specifically, we propose Common Features Extractor (CFE) to learn potential common representations from the RGB domain and event domain. For learning the unique features of the two domains, we utilize a Unique Extractor for Event (UEE) based on Spiking Neural Networks to extract edge cues in the event domain which may be missed in RGB in some challenging conditions, and a Unique Extractor for RGB (UER) based on Deep Convolutional Neural Networks to extract texture and semantic information in RGB domain. Extensive experiments on standard RGB benchmark and real event tracking dataset demonstrate the effectiveness of the proposed approach. We show our approach outperforms all compared state-of-the-art tracking algorithms and verify event-based data is a powerful cue for tracking in challenging scenes. </details>
<details>	<summary>邮件日期</summary>	2021年08月11日</details>

# 196、脉冲时间依赖的可塑性训练脉冲神经网络的泛化特性
- [ ] Characterization of Generalizability of Spike Timing Dependent Plasticity trained Spiking Neural Networks 
时间：2021年08月09日                         第一作者：Biswadeep Chakraborty                       [链接](https://arxiv.org/abs/2105.14677).                     
<details>	<summary>注释</summary>	23 pages, submitted to Frontiers in Neuroscience. arXiv admin note: text overlap with arXiv:2010.08195, arXiv:2006.09313 by other authors </details>
<details>	<summary>邮件日期</summary>	2021年08月10日</details>

# 195、基于神经形态芯片的无人机事件驱动视觉与控制
- [ ] Event-driven Vision and Control for UAVs on a Neuromorphic Chip 
时间：2021年08月08日                         第一作者：Antonio Vitale                       [链接](https://arxiv.org/abs/2108.03694).                     
## 摘要：与传统图像传感器相比，基于事件的视觉传感器在无人机高速控制中实现了高达三个数量级的速度与功耗权衡。基于事件的摄像头产生稀疏的事件流，可以比图像更有效地处理，延迟更低，从而实现超快速的视觉驱动控制。在这里，我们探讨如何将基于事件的视觉算法实现为神经形态芯片上的脉冲神经元网络，并将其用于无人机控制器。我们展示了基于事件的感知在芯片上的无缝集成如何导致更快的控制率和更低的延迟。此外，我们还演示了如何使用片上学习实现SNN控制器的在线自适应。我们的芯片上脉冲神经元网络是基于神经形态视觉的控制器解决高速无人机控制任务的第一个例子。神经形态硬件中出色的处理可扩展性为将来解决更具挑战性的视觉任务和将视觉感知集成到快速控制回路中提供了可能。
<details>	<summary>英文摘要</summary>	Event-based vision sensors achieve up to three orders of magnitude better speed vs. power consumption trade off in high-speed control of UAVs compared to conventional image sensors. Event-based cameras produce a sparse stream of events that can be processed more efficiently and with a lower latency than images, enabling ultra-fast vision-driven control. Here, we explore how an event-based vision algorithm can be implemented as a spiking neuronal network on a neuromorphic chip and used in a drone controller. We show how seamless integration of event-based perception on chip leads to even faster control rates and lower latency. In addition, we demonstrate how online adaptation of the SNN controller can be realised using on-chip learning. Our spiking neuronal network on chip is the first example of a neuromorphic vision-based controller solving a high-speed UAV control task. The excellent scalability of processing in neuromorphic hardware opens the possibility to solve more challenging visual tasks in the future and integrate visual perception in fast control loops. </details>
<details>	<summary>注释</summary>	7 pages, 7 figures, 1 table </details>
<details>	<summary>邮件日期</summary>	2021年08月10日</details>

# 194、强化学习剂中高温提取神经元峰电位
- [ ] Distilling Neuron Spike with High Temperature in Reinforcement Learning Agents 
时间：2021年08月05日                         第一作者：Ling Zhang                       [链接](https://arxiv.org/abs/2108.10078).                     
## 摘要：与深度神经网络（DNN）相比，脉冲神经网络（SNN）具有更快的处理速度、更低的能耗和更高的生物可解释性，有望接近强人工智能。强化学习类似于生物学学习。研究SNN与RL的结合具有重要意义。提出了基于STBP的脉冲蒸馏网络（SDN）强化学习方法。该方法采用蒸馏法，有效地避免了STBP算法的缺点，在分类上可以达到SOTA性能，并且可以得到更小、更快收敛和更低功耗的SNN强化学习模型。实验表明，该方法比传统的SNN强化学习和DNN强化学习方法收敛速度快，约快1000个历元，获得的SNN比DNN小200倍。我们还将SDN部署到PKU nc64c芯片上，证明SDN的功耗比DNN低，在大规模设备上SDN的功耗比DNN低600多倍。SDN提供了一种新的SNN强化学习方式，并能实现SOTA性能，证明了SNN强化学习进一步发展的可能性。
<details>	<summary>英文摘要</summary>	Spiking neural network (SNN), compared with depth neural network (DNN), has faster processing speed, lower energy consumption and more biological interpretability, which is expected to approach Strong AI. Reinforcement learning is similar to learning in biology. It is of great significance to study the combination of SNN and RL. We propose the reinforcement learning method of spike distillation network (SDN) with STBP. This method uses distillation to effectively avoid the weakness of STBP, which can achieve SOTA performance in classification, and can obtain a smaller, faster convergence and lower power consumption SNN reinforcement learning model. Experiments show that our method can converge faster than traditional SNN reinforcement learning and DNN reinforcement learning methods, about 1000 epochs faster, and obtain SNN 200 times smaller than DNN. We also deploy SDN to the PKU nc64c chip, which proves that SDN has lower power consumption than DNN, and the power consumption of SDN is more than 600 times lower than DNN on large-scale devices. SDN provides a new way of SNN reinforcement learning, and can achieve SOTA performance, which proves the possibility of further development of SNN reinforcement learning. </details>
<details>	<summary>注释</summary>	7 pages, 5 figures, conference </details>
<details>	<summary>邮件日期</summary>	2021年08月24日</details>

# 193、使用局部递归模体构建递归脉冲神经网络和降低风险的结构优化
- [ ] Composing Recurrent Spiking Neural Networks using Locally-Recurrent Motifs and Risk-Mitigating Architectural Optimization 
时间：2021年08月04日                         第一作者：Wenrui Zhang                       [链接](https://arxiv.org/abs/2108.01793).                     
## 摘要：在神经电路中，循环连通性对网络的功能和稳定性起着至关重要的作用。然而，现有的递归脉冲神经网络（RSNN）通常是由随机连接构造的，没有经过优化。尽管RSNN可以产生对记忆形成和学习至关重要的丰富动态，但RSNN的系统架构优化仍然是一个开放性挑战。我们的目标是通过新的可扩展RSNN体系结构和自动化体系结构优化，实现大型RSNN的系统设计。我们基于一种称为稀疏连接的递归模体层（SC-ML）的层结构构建RSNN，该层结构由多个通过稀疏横向连接连接在一起的小递归模体组成。基序的小尺寸和稀疏的基序间连接导致RSNN架构可扩展到大网络尺寸。我们进一步提出了一种称为混合风险缓解架构搜索（HRMAS）的方法，以系统地优化所提出的循环模体和SC-ML层架构的拓扑结构。HRMAS是一个交替的两步优化过程，通过引入一种新的生物启发的“自我修复”机制，通过内在可塑性，我们减轻了由体系结构变化引起的网络不稳定和性能下降的风险。内在可塑性被引入每个HRMAS迭代的第二步，并在RSNN架构“进化”过程中，作为对第一步引入的结构和突触重量修改的无监督快速自适应。据作者所知，这是第一个对RSNNs进行系统架构优化的工作。使用一个语音和三个神经形态数据集，我们展示了所提出的自动化架构优化比现有手动设计的RSNN带来的显著性能改进。
<details>	<summary>英文摘要</summary>	In neural circuits, recurrent connectivity plays a crucial role in network function and stability. However, existing recurrent spiking neural networks (RSNNs) are often constructed by random connections without optimization. While RSNNs can produce rich dynamics that are critical for memory formation and learning, systemic architectural optimization of RSNNs is still an opening challenge. We aim to enable systemic design of large RSNNs via a new scalable RSNN architecture and automated architectural optimization. We compose RSNNs based on a layer architecture called Sparsely-Connected Recurrent Motif Layer (SC-ML) that consists of multiple small recurrent motifs wired together by sparse lateral connections. The small size of the motifs and sparse inter-motif connectivity leads to an RSNN architecture scalable to large network sizes. We further propose a method called Hybrid Risk-Mitigating Architectural Search (HRMAS) to systematically optimize the topology of the proposed recurrent motifs and SC-ML layer architecture. HRMAS is an alternating two-step optimization process by which we mitigate the risk of network instability and performance degradation caused by architectural change by introducing a novel biologically-inspired "self-repairing" mechanism through intrinsic plasticity. The intrinsic plasticity is introduced to the second step of each HRMAS iteration and acts as unsupervised fast self-adaption to structural and synaptic weight modifications introduced by the first step during the RSNN architectural "evolution". To the best of the authors' knowledge, this is the first work that performs systematic architectural optimization of RSNNs. Using one speech and three neuromorphic datasets, we demonstrate the significant performance improvement brought by the proposed automated architecture optimization over existing manually-designed RSNNs. </details>
<details>	<summary>注释</summary>	20 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年08月05日</details>

# 192、具有相变记忆突触的脉冲递归神经网络的在线训练
- [ ] Online Training of Spiking Recurrent Neural Networks with Phase-Change Memory Synapses 
时间：2021年08月04日                         第一作者：Yigit Demirag                       [链接](https://arxiv.org/abs/2108.01804).                     
## 摘要：脉冲递归神经网络（RNN）由于其丰富的时间动态性和稀疏的处理能力，在解决各种复杂的认知和运动任务方面是一种很有前途的工具。然而，在专用神经形态硬件上训练脉冲RNN仍然是一个开放的挑战。这主要是由于缺乏本地、硬件友好的学习机制，即使在权重分辨率有限的情况下，也无法解决暂时的信用分配问题并确保稳定的网络动态。如果人们利用记忆器件进行内存计算来解决冯·诺依曼瓶颈问题，而牺牲了脉冲RNN的计算和工作内存的可变性，那么这些挑战将进一步加剧。为了解决这些挑战，并在记忆性神经形态RNN中实现在线学习，我们提出了一个基于精确和全面的相变记忆（PCM）器件模型的差分结构交叉阵列仿真框架。我们使用最近提出的e-prop学习规则训练了一个脉冲RNN，其权值在所提出的仿真框架中进行了仿真。虽然e-prop局部近似于理想的突触更新，但由于大量PCM非理想性，很难在记忆基底上实现更新。我们比较了几种广泛采用的权值更新方案，这些方案主要是为了应对这些设备的非理想性，并证明累积梯度可以在线有效地训练记忆基底上的脉冲RNN。
<details>	<summary>英文摘要</summary>	Spiking recurrent neural networks (RNNs) are a promising tool for solving a wide variety of complex cognitive and motor tasks, due to their rich temporal dynamics and sparse processing. However training spiking RNNs on dedicated neuromorphic hardware is still an open challenge. This is due mainly to the lack of local, hardware-friendly learning mechanisms that can solve the temporal credit assignment problem and ensure stable network dynamics, even when the weight resolution is limited. These challenges are further accentuated, if one resorts to using memristive devices for in-memory computing to resolve the von-Neumann bottleneck problem, at the expense of a substantial increase in variability in both the computation and the working memory of the spiking RNNs. To address these challenges and enable online learning in memristive neuromorphic RNNs, we present a simulation framework of differential-architecture crossbar arrays based on an accurate and comprehensive Phase-Change Memory (PCM) device model. We train a spiking RNN whose weights are emulated in the presented simulation framework, using a recently proposed e-prop learning rule. Although e-prop locally approximates the ideal synaptic updates, it is difficult to implement the updates on the memristive substrate due to substantial PCM non-idealities. We compare several widely adapted weight update schemes that primarily aim to cope with these device non-idealities and demonstrate that accumulating gradients can enable online and efficient training of spiking RNN on memristive substrates. </details>
<details>	<summary>邮件日期</summary>	2021年08月05日</details>

# 191、DFSynthesizer：基于数据流的神经网络到神经形态硬件的合成
- [ ] DFSynthesizer: Dataflow-based Synthesis of Spiking Neural Networks to Neuromorphic Hardware 
时间：2021年08月04日                         第一作者：Shihao Song                       [链接](https://arxiv.org/abs/2108.02023).                     
## 摘要：脉冲神经网络（SNN）是一种新兴的计算模型，它使用事件驱动激活和仿生学习算法。基于SNN的机器学习程序通常在基于tile的神经形态硬件平台上执行，其中每个tile由一个称为crossbar的计算单元组成，该计算单元映射程序的神经元和突触。然而，在现成的神经形态硬件上合成这样的程序是一个挑战。这是因为硬件固有的资源和延迟限制，这会影响模型性能（例如精度）和硬件性能（例如吞吐量）。我们提出了DFSynthesizer，这是一个端到端的框架，用于将基于SNN的机器学习程序合成到神经形态硬件。拟议的框架分为四个步骤。首先，它分析一个机器学习程序，并使用代表性数据生成SNN工作负载。其次，它对SNN工作负载进行分区，并生成适合目标神经形态硬件交叉条的集群。第三，它利用同步数据流图（SDFG）丰富的语义来表示集群SNN程序，允许根据关键硬件约束（如交叉杆的数量、每个交叉杆的尺寸、磁贴上的缓冲区空间和磁贴通信带宽）进行性能分析。最后，它使用一种新的调度算法在硬件的横杆上执行集群，从而保证硬件性能。我们用10个常用的机器学习程序来评估DFSynthesizer。我们的结果表明，与当前的映射方法相比，DFSynthesizer提供了更严格的性能保证。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNN) are an emerging computation model, which uses event-driven activation and bio-inspired learning algorithms. SNN-based machine-learning programs are typically executed on tile- based neuromorphic hardware platforms, where each tile consists of a computation unit called crossbar, which maps neurons and synapses of the program. However, synthesizing such programs on an off-the-shelf neuromorphic hardware is challenging. This is because of the inherent resource and latency limitations of the hardware, which impact both model performance, e.g., accuracy, and hardware performance, e.g., throughput. We propose DFSynthesizer, an end-to-end framework for synthesizing SNN-based machine learning programs to neuromorphic hardware. The proposed framework works in four steps. First, it analyzes a machine-learning program and generates SNN workload using representative data. Second, it partitions the SNN workload and generates clusters that fit on crossbars of the target neuromorphic hardware. Third, it exploits the rich semantics of Synchronous Dataflow Graph (SDFG) to represent a clustered SNN program, allowing for performance analysis in terms of key hardware constraints such as number of crossbars, dimension of each crossbar, buffer space on tiles, and tile communication bandwidth. Finally, it uses a novel scheduling algorithm to execute clusters on crossbars of the hardware, guaranteeing hardware performance. We evaluate DFSynthesizer with 10 commonly used machine-learning programs. Our results demonstrate that DFSynthesizer provides much tighter performance guarantee compared to current mapping approaches. </details>
<details>	<summary>注释</summary>	Accepted for publication at ACM Transactions on Embedded Computing </details>
<details>	<summary>邮件日期</summary>	2021年08月05日</details>

# 190、脉冲神经形态芯片学习纠缠量子态
- [ ] Spiking neuromorphic chip learns entangled quantum states 
时间：2021年08月03日                         第一作者：Stefanie Czischek                       [链接](https://arxiv.org/abs/2008.01039).                     
<details>	<summary>注释</summary>	22 pages, 6 figures Submission to SciPost </details>
<details>	<summary>邮件日期</summary>	2021年08月04日</details>

# 189、形成具有迭代优胜者的单元组件需要所有计算和激发抑制平衡
- [ ] Formation of cell assemblies with iterative winners-take-all computation and excitation-inhibition balance 
时间：2021年08月02日                         第一作者：Viacheslav Osaulenko                        [链接](https://arxiv.org/abs/2108.00706).                     
## 摘要：本文针对将信息编码为二进制单元程序集的问题。脉冲神经网络和k-winners-take-all模型是两种常见的方法，但第一种方法难以用于信息处理，第二种方法过于简单，缺乏第一种方法的重要特征。我们提出了一个中间模型，该模型与kWTA的计算简便性相同，并且具有更灵活和更丰富的动力学特性。它使用显式抑制神经元通过迭代程序平衡和塑造兴奋。这导致抑制性和兴奋性神经元之间的反复交互作用，更好地适应输入分布，并执行习惯化、去相关和聚类等计算。为了证明这些，我们研究了类Hebbian学习规则，并提出了一种新的具有多种稳定机制的二元权重学习规则。我们的源代码是公开的。
<details>	<summary>英文摘要</summary>	This paper targets the problem of encoding information into binary cell assemblies. Spiking neural networks and k-winners-take-all models are two common approaches, but the first is hard to use for information processing and the second is too simple and lacks important features of the first. We present an intermediate model that shares the computational ease of kWTA and has more flexible and richer dynamics. It uses explicit inhibitory neurons to balance and shape excitation through an iterative procedure. This leads to a recurrent interaction between inhibitory and excitatory neurons that better adapts to the input distribution and performs such computations as habituation, decorrelation, and clustering. To show these, we investigate Hebbian-like learning rules and propose a new learning rule for binary weights with multiple stabilization mechanisms. Our source code is publicly available. </details>
<details>	<summary>注释</summary>	14 pages, 8 figures </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 188、通过乘法突触的脉冲神经网络的非线性计算
- [ ] Nonlinear computations in spiking neural networks through multiplicative synapses 
时间：2021年08月02日                         第一作者：Michele Nardin                       [链接](https://arxiv.org/abs/2009.03857).                     
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 187、基于片上学习的深脉冲神经网络连接剪枝
- [ ] Connection Pruning for Deep Spiking Neural Networks with On-Chip Learning 
时间：2021年07月31日                         第一作者：Thao N.N. Nguyen                       [链接](https://arxiv.org/abs/2010.04351).                     
<details>	<summary>注释</summary>	8 pages, 9 figures This paper has been accepted for publication in the International Conference on Neuromorphic Systems (ICONS) 2021 Journal-ref: International Conference on Neuromorphic Systems 2021 DOI: 10.1145/3477145.3477157 </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 186、脉冲神经网络无监督模式识别的权值发散易化原理
- [ ] The principle of weight divergence facilitation for unsupervised pattern recognition in spiking neural networks 
时间：2021年07月31日                         第一作者：Oleg Nikitin                       [链接](https://arxiv.org/abs/2104.09943).                     
<details>	<summary>注释</summary>	9 pages, 5 figures, submitted to the conference ICANN 2021 MSC-class: 68T05 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年08月03日</details>

# 185、HYPER-SNN：面向高光谱图像分类的节能量化深脉冲神经网络
- [ ] HYPER-SNN: Towards Energy-efficient Quantized Deep Spiking Neural Networks for Hyperspectral Image Classification 
时间：2021年07月28日                         第一作者：Gourav Datta                       [链接](https://arxiv.org/abs/2107.11979).                     
<details>	<summary>邮件日期</summary>	2021年07月29日</details>

# 184、节能随机游动计算的神经形态标度优势
- [ ] Neuromorphic scaling advantages for energy-efficient random walk computation 
时间：2021年07月27日                         第一作者：J. Darby Smith                       [链接](https://arxiv.org/abs/2107.13057).                     
## 摘要：受大脑难以置信的效率和能力的启发，神经形态计算（NMC）方法将从根本上改进计算。大多数NMC研究的目标是在人造硬件中复制大脑的计算结构和架构，其重点是人工智能；然而，很少有人探究这种大脑启发的硬件是否能提供超越认知任务的价值。我们证明了脉冲神经形态结构的高度并行性和可配置性使得它们非常适合通过离散时间马尔可夫链实现随机游动。这种随机游动在蒙特卡罗方法中非常有用，蒙特卡罗方法是解决广泛数值计算任务的基本计算工具。此外，我们还展示了涉及一类随机微分方程的概率解的数学基础如何利用这些模拟为一系列广泛适用的计算任务提供解决方案。尽管处于早期开发阶段，我们发现NMC平台在足够大的规模下可以大幅降低高性能计算（HPC）平台的能源需求。
<details>	<summary>英文摘要</summary>	Computing stands to be radically improved by neuromorphic computing (NMC) approaches inspired by the brain's incredible efficiency and capabilities. Most NMC research, which aims to replicate the brain's computational structure and architecture in man-made hardware, has focused on artificial intelligence; however, less explored is whether this brain-inspired hardware can provide value beyond cognitive tasks. We demonstrate that high-degree parallelism and configurability of spiking neuromorphic architectures makes them well-suited to implement random walks via discrete time Markov chains. Such random walks are useful in Monte Carlo methods, which represent a fundamental computational tool for solving a wide range of numerical computing tasks. Additionally, we show how the mathematical basis for a probabilistic solution involving a class of stochastic differential equations can leverage those simulations to provide solutions for a range of broadly applicable computational tasks. Despite being in an early development stage, we find that NMC platforms, at a sufficient scale, can drastically reduce the energy demands of high-performance computing (HPC) platforms. </details>
<details>	<summary>注释</summary>	Paper, figures, supplement Report-no: SAND2021-9085 O </details>
<details>	<summary>邮件日期</summary>	2021年07月29日</details>

# 183、用单脉冲混合输入编码训练能量有效的深脉冲神经网络
- [ ] Training Energy-Efficient Deep Spiking Neural Networks with Single-Spike Hybrid Input Encoding 
时间：2021年07月26日                         第一作者：Gourav Datta                       [链接](https://arxiv.org/abs/2107.12374).                     
## 摘要：脉冲神经网络（SNN）已经成为传统深度学习框架的一个有吸引力的替代方案，因为它们在事件驱动的神经形态硬件中提供了更高的计算效率。然而，由于输入编码和训练技术的效率低下，最先进的（SOTA）SNN存在较高的推理延迟。最广泛使用的输入编码方案，例如基于泊松的速率编码，没有利用snn的时间学习能力。本文提出了一种低延迟节能SNN的训练框架，该框架在输入层使用混合编码方案，在第一个时间步中直接应用图像的模拟像素值，在随后的时间步中使用一种新的脉冲时间编码变体。特别是，每个隐藏层中的神经元被限制在每张图像中最多发射一次，这增加了激活稀疏性。为了训练这些混合编码的SNN，我们提出了一种基于梯度下降的脉冲时间相关反向传播（STDB）机制，该机制使用了一种新的基于输出神经元脉冲时间和膜电位的交叉熵损失函数。由此产生的SNN降低了延迟和高激活稀疏性，显著提高了计算效率。特别是，我们评估了在几种VGG体系结构上针对来自CIFAR-10和CIFAR-100数据集的图像分类任务提出的培训方案。我们在CIFAR-100数据集上以$5$timesteps获得了$66.46$\%的顶级精度，计算能量比同等标准ANN少${\sim}125 \倍。此外，与其他最先进的速率或时间编码的SNN模型相比，我们提出的SNN推理速度快5$-$300\倍。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have emerged as an attractive alternative to traditional deep learning frameworks, since they provide higher computational efficiency in event driven neuromorphic hardware. However, the state-of-the-art (SOTA) SNNs suffer from high inference latency, resulting from inefficient input encoding and training techniques. The most widely used input coding schemes, such as Poisson based rate-coding, do not leverage the temporal learning capabilities of SNNs. This paper presents a training framework for low-latency energy-efficient SNNs that uses a hybrid encoding scheme at the input layer in which the analog pixel values of an image are directly applied during the first timestep and a novel variant of spike temporal coding is used during subsequent timesteps. In particular, neurons in every hidden layer are restricted to fire at most once per image which increases activation sparsity. To train these hybrid-encoded SNNs, we propose a variant of the gradient descent based spike timing dependent back propagation (STDB) mechanism using a novel cross entropy loss function based on both the output neurons' spike time and membrane potential. The resulting SNNs have reduced latency and high activation sparsity, yielding significant improvements in computational efficiency. In particular, we evaluate our proposed training scheme on image classification tasks from CIFAR-10 and CIFAR-100 datasets on several VGG architectures. We achieve top-1 accuracy of $66.46$\% with $5$ timesteps on the CIFAR-100 dataset with ${\sim}125\times$ less compute energy than an equivalent standard ANN. Additionally, our proposed SNN performs $5$-$300\times$ faster inference compared to other state-of-the-art rate or temporally coded SNN models. </details>
<details>	<summary>注释</summary>	arXiv admin note: text overlap with arXiv:2107.11979 </details>
<details>	<summary>邮件日期</summary>	2021年07月28日</details>

# 182、面向高光谱图像分类的能量高效量化深棘波神经网络
- [ ] Towards Energy-efficient Quantized Deep SpikingNeural Networks for Hyperspectral Image Classification 
时间：2021年07月26日                         第一作者：Gourav Datta                       [链接](https://arxiv.org/abs/2107.11979).                     
## 摘要：超光谱图像（HSI）在一系列连续的光谱带中提供丰富的光谱和空间信息。然而，精确处理波段之间的光谱和空间相关性需要使用能量昂贵的三维卷积神经网络（CNN）。为了应对这一挑战，我们建议使用由iso体系结构CNN生成并通过量化感知梯度下降进行训练的脉冲神经网络（SNN），以优化其权重、膜泄漏和触发阈值。在训练和推断期间，HSI的模拟像素值直接应用于SNN的输入层，而无需转换为脉冲序列。我们的训练技术降低了延迟，并结合了高激活稀疏性，从而显著提高了计算效率。我们使用3-D和3-D/2-D混合卷积结构上的三个HSI数据集来评估我们的方案。我们在Indian Pines数据集上通过5个时间步（推断延迟）和6位权重量化分别实现了98.68%、98.34%和98.20%的总体准确度、平均准确度和kappa系数。特别是，我们的模型实现了与最新技术（SOTA）相似的精度，在三个HSI数据集上的计算能量平均分别比iso体系结构全精度和6位量化CNN少560.6倍和44.8倍。
<details>	<summary>英文摘要</summary>	Hyper spectral images (HSI) provide rich spectral and spatial information across a series of contiguous spectral bands. However, the accurate processing of the spectral and spatial correlation between the bands requires the use of energy-expensive 3-D Convolutional Neural Networks (CNNs). To address this challenge, we propose the use of Spiking Neural Networks (SNNs) that are generated from iso-architecture CNNs and trained with quantization-aware gradient descent to optimize their weights, membrane leak, and firing thresholds. During both training and inference, the analog pixel values of a HSI are directly applied to the input layer of the SNN without the need to convert to a spike-train. The reduced latency of our training technique combined with high activation sparsity yields significant improvements in computational efficiency. We evaluate our proposal using three HSI datasets on a 3-D and a 3-D / 2-D hybrid convolutional architecture. We achieve overall accuracy, average accuracy, and kappa coefficient of 98.68%, 98.34%, and 98.20% respectively with 5 time steps (inference latency) and 6-bit weight quantization on the Indian Pines dataset. In particular, our models achieved accuracies similar to state-of-the-art (SOTA) with 560.6 and 44.8 times less compute energy on average over three HSI datasets than an iso-architecture full-precision and 6-bit quantized CNN, respectively. </details>
<details>	<summary>邮件日期</summary>	2021年07月27日</details>

# 181、用于事件流分类的时间型注意脉冲神经网络
- [ ] Temporal-wise Attention Spiking Neural Networks for Event Streams Classification 
时间：2021年07月25日                         第一作者：Man Yao                       [链接](https://arxiv.org/abs/2107.11711).                     
## 摘要：如何有效地处理事件稀疏、不均匀、时间分辨率为微秒的时空事件流具有重要的应用价值。脉冲神经网络（SNN）作为一种基于大脑的事件触发计算模型，具有从事件流中提取有效时空特征的潜力。然而，当将单个事件聚合为具有新的更高时间分辨率的帧时，现有的SNN模型并不重视序列帧具有不同的信噪比，因为事件流是稀疏和非均匀的。这种情况会干扰现有SNN的性能。在这项工作中，我们提出了一个基于时间的注意SNN（TA-SNN）模型来学习基于帧的表示方法来处理事件流。具体来说，我们将注意概念扩展到时间输入，在训练阶段判断框架对最终决策的重要性，在推理阶段丢弃不相关的框架。我们证明了TA-SNN模型提高了事件流分类任务的准确性。我们还研究了多尺度时间分辨率对基于帧表示的影响。我们的方法在三个不同的分类任务上进行了测试：手势识别、图像分类和语音数字识别。我们报告了关于这些任务的最新结果，并在仅60毫秒的时间内就基本提高了手势识别的准确率（几乎19%）。
<details>	<summary>英文摘要</summary>	How to effectively and efficiently deal with spatio-temporal event streams, where the events are generally sparse and non-uniform and have the microsecond temporal resolution, is of great value and has various real-life applications. Spiking neural network (SNN), as one of the brain-inspired event-triggered computing models, has the potential to extract effective spatio-temporal features from the event streams. However, when aggregating individual events into frames with a new higher temporal resolution, existing SNN models do not attach importance to that the serial frames have different signal-to-noise ratios since event streams are sparse and non-uniform. This situation interferes with the performance of existing SNNs. In this work, we propose a temporal-wise attention SNN (TA-SNN) model to learn frame-based representation for processing event streams. Concretely, we extend the attention concept to temporal-wise input to judge the significance of frames for the final decision at the training stage, and discard the irrelevant frames at the inference stage. We demonstrate that TA-SNN models improve the accuracy of event streams classification tasks. We also study the impact of multiple-scale temporal resolutions for frame-based representation. Our approach is tested on three different classification tasks: gesture recognition, image classification, and spoken digit recognition. We report the state-of-the-art results on these tasks, and get the essential improvement of accuracy (almost 19\%) for gesture recognition with only 60 ms. </details>
<details>	<summary>注释</summary>	Accepted by ICCV 2021 </details>
<details>	<summary>邮件日期</summary>	2021年07月27日</details>

# 180、H2Learn：用于高精度脉冲神经网络的高效学习加速器
- [ ] H2Learn: High-Efficiency Learning Accelerator for High-Accuracy Spiking Neural Networks 
时间：2021年07月25日                         第一作者：Ling Liang                       [链接](https://arxiv.org/abs/2107.11746).                     
## 摘要：虽然脉冲神经网络（spiking neural networks，SNNs）得益于生物似然神经模型，但在常见的局部突触可塑性学习规则下，其学习精度较低，限制了其在许多实际任务中的应用。最近，人工神经网络（ANN）领域中一种新兴的基于时间反向传播（BPTT）的SNN监督学习算法成功地提高了SNN的准确性，并有助于提高SNN的实用性。然而，当前的通用处理器在对SNN执行BPTT时，由于ANN定制的优化，效率较低。另一方面，目前的神经形态芯片不能支持BPTT，因为它们主要采用局部突触可塑性规则来简化实现。在这项工作中，我们提出了H2Learn，一种新的体系结构，可以实现基于BPTT的SNN学习的高效率，从而确保SNN的高精度。首先，我们描述了基于BPTT的SNN学习行为。得益于前向传递中基于二进制脉冲的计算和权重更新，我们首先在前向引擎和权重更新引擎中设计基于查找表（LUT）的处理元素，使累加隐式化，并融合多个输入点的计算。其次，得益于后向过程中丰富的稀疏性，我们设计了一个双稀疏感知后向引擎，该引擎利用了输入和输出的稀疏性。最后，我们应用不同引擎之间的管道优化来构建基于BPTT的SNN学习的端到端解决方案。与现代NVIDIA V100 GPU相比，H2Learn在多个基准数据集上实现了7.38倍的面积节约、5.74-10.20倍的加速比和5.25-7.12倍的节能。
<details>	<summary>英文摘要</summary>	Although spiking neural networks (SNNs) take benefits from the bio-plausible neural modeling, the low accuracy under the common local synaptic plasticity learning rules limits their application in many practical tasks. Recently, an emerging SNN supervised learning algorithm inspired by backpropagation through time (BPTT) from the domain of artificial neural networks (ANNs) has successfully boosted the accuracy of SNNs and helped improve the practicability of SNNs. However, current general-purpose processors suffer from low efficiency when performing BPTT for SNNs due to the ANN-tailored optimization. On the other hand, current neuromorphic chips cannot support BPTT because they mainly adopt local synaptic plasticity rules for simplified implementation. In this work, we propose H2Learn, a novel architecture that can achieve high efficiency for BPTT-based SNN learning which ensures high accuracy of SNNs. At the beginning, we characterized the behaviors of BPTT-based SNN learning. Benefited from the binary spike-based computation in the forward pass and the weight update, we first design lookup table (LUT) based processing elements in Forward Engine and Weight Update Engine to make accumulations implicit and to fuse the computations of multiple input points. Second, benefited from the rich sparsity in the backward pass, we design a dual-sparsity-aware Backward Engine which exploits both input and output sparsity. Finally, we apply a pipeline optimization between different engines to build an end-to-end solution for the BPTT-based SNN learning. Compared with the modern NVIDIA V100 GPU, H2Learn achieves 7.38x area saving, 5.74-10.20x speedup, and 5.25-7.12x energy saving on several benchmark datasets. </details>
<details>	<summary>邮件日期</summary>	2021年07月27日</details>

# 179、一种用于节能目标检测的全脉冲混合神经网络
- [ ] A Fully Spiking Hybrid Neural Network for Energy-Efficient Object Detection 
时间：2021年07月24日                         第一作者：Biswadeep Chakraborty                       [链接](https://arxiv.org/abs/2104.10719).                     
<details>	<summary>注释</summary>	10 pages, Submitted Manuscript </details>
<details>	<summary>邮件日期</summary>	2021年07月27日</details>

# 178、通过注意引导压缩实现低延迟节能的深度SNN
- [ ] Towards Low-Latency Energy-Efficient Deep SNNs via Attention-Guided Compression 
时间：2021年07月16日                         第一作者：Souvik Kundu                       [链接](https://arxiv.org/abs/2107.12445).                     
## 摘要：深脉冲神经网络（SNN）已成为传统深度学习框架的潜在替代方案，因为它们有望在事件驱动的神经形态硬件上提供更高的计算效率。然而，为了在复杂的视觉应用中表现良好，大多数SNN训练框架都会产生较大的推理延迟，这会导致脉冲活动增加和能量效率降低。因此，最小化平均脉冲活动同时保持EP SNN的准确性仍然是一个重大的挑战和机遇。本文提出了一种非迭代SNN训练技术，该技术在保持高推理精度的同时实现超高压缩，同时降低脉冲活动。特别是，我们的框架首先使用未压缩元模型的注意图来生成压缩的ANN。这一步可以调整为支持不规则和结构化通道修剪，以利用各种平台上的计算优势。然后，该框架使用直接输入执行基于稀疏学习的监督SNN训练。在训练过程中，它联合优化SNN权重、阈值和泄漏参数，以在保持压缩的同时大幅减少所需的时间步数。为了评估我们的方法的优点，我们在CIFAR-10和CIFAR-100上对VGG和ResNet的变体进行了实验，并在Tiny-ImageNet上对VGG16进行了实验。通过建议的技术生成的SNN模型产生的SOTA压缩比高达33.4x，与基线未运行的对应模型相比，精度没有显著下降。与现有的SNN修剪方法相比，我们实现了高达8.3倍的压缩，并提高了精度。
<details>	<summary>英文摘要</summary>	Deep spiking neural networks (SNNs) have emerged as a potential alternative to traditional deep learning frameworks, due to their promise to provide increased compute efficiency on event-driven neuromorphic hardware. However, to perform well on complex vision applications, most SNN training frameworks yield large inference latency which translates to increased spike activity and reduced energy efficiency. Hence,minimizing average spike activity while preserving accuracy indeep SNNs remains a significant challenge and opportunity.This paper presents a non-iterative SNN training technique thatachieves ultra-high compression with reduced spiking activitywhile maintaining high inference accuracy. In particular, our framework first uses the attention-maps of an un compressed meta-model to yield compressed ANNs. This step can be tuned to support both irregular and structured channel pruning to leverage computational benefits over a broad range of platforms. The framework then performs sparse-learning-based supervised SNN training using direct inputs. During the training, it jointly optimizes the SNN weight, threshold, and leak parameters to drastically minimize the number of time steps required while retaining compression. To evaluate the merits of our approach, we performed experiments with variants of VGG and ResNet, on both CIFAR-10 and CIFAR-100, and VGG16 on Tiny-ImageNet.The SNN models generated through the proposed technique yield SOTA compression ratios of up to 33.4x with no significant drops in accuracy compared to baseline unpruned counterparts. Compared to existing SNN pruning methods, we achieve up to 8.3x higher compression with improved accuracy. </details>
<details>	<summary>注释</summary>	10 Pages, 8 Figures, 5 Tables </details>
<details>	<summary>邮件日期</summary>	2021年07月28日</details>

# 177、用于神经形态信息处理的共振隧道二极管纳米光电脉冲节点
- [ ] Resonant tunnelling diode nano-optoelectronic spiking nodes for neuromorphic information processing 
时间：2021年07月21日                         第一作者：Mat\v{e}j Hejda                       [链接](https://arxiv.org/abs/2107.06721).                     
<details>	<summary>注释</summary>	Changed manuscript title to lower-case </details>
<details>	<summary>邮件日期</summary>	2021年07月22日</details>

# 176、深度神经网络中时间稀疏性的训练，在视频处理中的应用
- [ ] Training for temporal sparsity in deep neural networks, application in video processing 
时间：2021年07月15日                         第一作者：Amirreza Yousefzadeh                       [链接](https://arxiv.org/abs/2107.07305).                     
## 摘要：激活稀疏性提高了稀疏感知神经网络加速器的计算效率和资源利用率。由于DNNs中的主要操作是使用权重计算内积的激活的乘法累加（MAC），跳过两个操作数中至少有一个为零的操作可以使推断在延迟和功率方面更有效。激活的空间稀疏化是DNN文献中的一个热门话题，已经建立了几种方法来偏向DNN。另一方面，时间稀疏性是仿生脉冲神经网络（SNNs）的固有特性，神经形态处理利用SNNs提高硬件效率。引入和利用时空稀疏性，是DNN文献中很少探讨的话题，但与DNN的发展趋势完全一致，从静态信号处理转向更多的流信号处理。为了实现这一目标，本文引入了一种新的DNN层（称为Delta激活层），其唯一目的是提高训练过程中激活的时间稀疏性。Delta激活层将时间稀疏性转化为空间稀疏性，以便在硬件中执行稀疏张量乘法时加以利用。通过在训练过程中使用delta推理和“通常的”空间稀疏启发式，所得到的模型不仅学习利用空间而且还学习利用时间激活稀疏性（对于给定的输入数据分布）。人们可以在普通训练或细化阶段使用δ激活层。我们实现了Delta激活层作为标准Tensoflow Keras库的扩展，并将其应用于人体动作识别（UCF101）数据集的深层神经网络训练。我们报告了激活稀疏度几乎提高了3倍，在长时间的训练后，模型精度的可恢复性损失。
<details>	<summary>英文摘要</summary>	Activation sparsity improves compute efficiency and resource utilization in sparsity-aware neural network accelerators. As the predominant operation in DNNs is multiply-accumulate (MAC) of activations with weights to compute inner products, skipping operations where (at least) one of the two operands is zero can make inference more efficient in terms of latency and power. Spatial sparsification of activations is a popular topic in DNN literature and several methods have already been established to bias a DNN for it. On the other hand, temporal sparsity is an inherent feature of bio-inspired spiking neural networks (SNNs), which neuromorphic processing exploits for hardware efficiency. Introducing and exploiting spatio-temporal sparsity, is a topic much less explored in DNN literature, but in perfect resonance with the trend in DNN, to shift from static signal processing to more streaming signal processing. Towards this goal, in this paper we introduce a new DNN layer (called Delta Activation Layer), whose sole purpose is to promote temporal sparsity of activations during training. A Delta Activation Layer casts temporal sparsity into spatial activation sparsity to be exploited when performing sparse tensor multiplications in hardware. By employing delta inference and ``the usual'' spatial sparsification heuristics during training, the resulting model learns to exploit not only spatial but also temporal activation sparsity (for a given input data distribution). One may use the Delta Activation Layer either during vanilla training or during a refinement phase. We have implemented Delta Activation Layer as an extension of the standard Tensoflow-Keras library, and applied it to train deep neural networks on the Human Action Recognition (UCF101) dataset. We report an almost 3x improvement of activation sparsity, with recoverable loss of model accuracy after longer training. </details>
<details>	<summary>邮件日期</summary>	2021年07月16日</details>

# 175、用于神经形态信息处理的共振隧道二极管纳米光电脉冲节点
- [ ] Resonant Tunnelling Diode Nano-Optoelectronic Spiking Nodes For Neuromorphic Information Processing 
时间：2021年07月14日                         第一作者：Mat\v{e}j Hejda                       [链接](https://arxiv.org/abs/2107.06721).                     
## 摘要：在这项工作中，我们介绍了一种光电脉冲人工神经元，它能够以超快的速率（$\大约$100 ps/光脉冲）和低能耗（$<$pJ/脉冲）工作。所提出的系统结合了一个表现出负微分电导的可激发共振隧道二极管（RTD）元件，该元件耦合到纳米级光源（形成主节点）或光电探测器（形成接收节点）。我们数值研究了一个互连的主-接收RTD节点系统的脉冲动态响应和信息传播功能。利用脉冲阈值化和积分的关键功能，我们利用单个节点对序列脉冲模式进行分类，并对图像特征（边缘）进行卷积识别。我们还展示了一个光学互连的脉冲神经网络模型，用于处理超过10gbps的时空数据，具有很高的推理精度。最后，我们展示了一种利用脉冲时间相关可塑性的片外监督学习方法，用于支持RTD的光子脉冲神经网络。这些结果证明了RTD脉冲节点在低占地面积、低能耗、高速光电实现神经形态硬件方面的潜力和可行性。
<details>	<summary>英文摘要</summary>	In this work, we introduce an optoelectronic spiking artificial neuron capable of operating at ultrafast rates ($\approx$ 100 ps/optical spike) and with low energy consumption ($<$ pJ/spike). The proposed system combines an excitable resonant tunnelling diode (RTD) element exhibiting negative differential conductance, coupled to a nanoscale light source (forming a master node) or a photodetector (forming a receiver node). We study numerically the spiking dynamical responses and information propagation functionality of an interconnected master-receiver RTD node system. Using the key functionality of pulse thresholding and integration, we utilize a single node to classify sequential pulse patterns and perform convolutional functionality for image feature (edge) recognition. We also demonstrate an optically-interconnected spiking neural network model for processing of spatiotemporal data at over 10 Gbps with high inference accuracy. Finally, we demonstrate an off-chip supervised learning approach utilizing spike-timing dependent plasticity for the RTD-enabled photonic spiking neural network. These results demonstrate the potential and viability of RTD spiking nodes for low footprint, low energy, high-speed optoelectronic realization of neuromorphic hardware. </details>
<details>	<summary>邮件日期</summary>	2021年07月15日</details>

# 174、利用图形学习中时空特征归一化的脉冲动态特性
- [ ] Exploiting Spiking Dynamics with Spatial-temporal Feature Normalization in Graph Learning 
时间：2021年06月30日                         第一作者：Mingkun Xu                       [链接](https://arxiv.org/abs/2107.06865).                     
## 摘要：具有内在动力学的生物脉冲神经元是大脑在复杂环境中处理多模态信息的强大表征和学习能力的基础。尽管最近在处理欧几里德空间任务的脉冲神经网络（SNNs）方面取得了巨大的进展，但利用SNNs处理以图形数据为代表的非欧几里德空间数据仍然是一个挑战，这主要是由于缺乏有效的建模框架和有用的训练技术。在这里，我们提出了一个通用的基于脉冲的建模框架，可以直接训练snn进行图形学习。通过对节点特征的脉冲数据流进行时空展开，将图卷积滤波器引入到脉冲动力学中，形成了一种协同学习范式。针对SNN特有的脉冲表示和脉冲动态特性，提出了一种适用于SNN的时空特征归一化（STFN）技术，以加速SNN的收敛。我们将我们的方法实例化为两个脉冲图模型，包括图卷积SNNs和图注意SNNs，并在Cora、citeser和Pubmed三个节点分类基准上验证了它们的性能。我们的模型可以以较低的计算成本达到与最先进的图形神经网络（GNN）模型相当的性能，显示了在神经形态硬件上执行的巨大优势，并促进了神经形态在图形场景中的应用。
<details>	<summary>英文摘要</summary>	Biological spiking neurons with intrinsic dynamics underlie the powerful representation and learning capabilities of the brain for processing multimodal information in complex environments. Despite recent tremendous progress in spiking neural networks (SNNs) for handling Euclidean-space tasks, it still remains challenging to exploit SNNs in processing non-Euclidean-space data represented by graph data, mainly due to the lack of effective modeling framework and useful training techniques. Here we present a general spike-based modeling framework that enables the direct training of SNNs for graph learning. Through spatial-temporal unfolding for spiking data flows of node features, we incorporate graph convolution filters into spiking dynamics and formalize a synergistic learning paradigm. Considering the unique features of spike representation and spiking dynamics, we propose a spatial-temporal feature normalization (STFN) technique suitable for SNN to accelerate convergence. We instantiate our methods into two spiking graph models, including graph convolution SNNs and graph attention SNNs, and validate their performance on three node-classification benchmarks, including Cora, Citeseer, and Pubmed. Our model can achieve comparable performance with the state-of-the-art graph neural network (GNN) models with much lower computation costs, demonstrating great benefits for the execution on neuromorphic hardware and prompting neuromorphic applications in graphical scenarios. </details>
<details>	<summary>注释</summary>	Accepted by IJCAI-21 </details>
<details>	<summary>邮件日期</summary>	2021年07月15日</details>

# 173、反向传播邻域聚合用于脉冲神经网络的精确训练
- [ ] Backpropagated Neighborhood Aggregation for Accurate Training of Spiking Neural Networks 
时间：2021年06月22日                         第一作者：Yukun Yang                       [链接](https://arxiv.org/abs/2107.06861).                     
## 摘要：虽然反向传播（BP）已被应用于脉冲神经网络（SNNs）取得了令人鼓舞的结果，但一个关键的挑战是将连续值损失反向传播到具有不连续全部或无放电活动的脉冲神经元层上。现有的方法通过引入具有自身局限性的折衷方案来解决这一难题，从而导致潜在的性能下降。我们提出了一种新的类似BP的方法，称为邻域聚合（NA），它计算精确的误差梯度来指导权重更新，这可能导致不连续的修改射击活动。NA利用一个新的膜电位距离函数，通过在每个神经元的当前膜电位附近聚集多个扰动膜电位波形损失的有限差分来实现这一目标。实验表明，该算法在多个数据集的SNN训练中具有良好的性能。
<details>	<summary>英文摘要</summary>	While backpropagation (BP) has been applied to spiking neural networks (SNNs) achieving encouraging results, a key challenge involved is to backpropagate a continuous-valued loss over layers of spiking neurons exhibiting discontinuous all-or-none firing activities. Existing methods deal with this difficulty by introducing compromises that come with their own limitations, leading to potential performance degradation. We propose a novel BP-like method, called neighborhood aggregation (NA), which computes accurate error gradients guiding weight updates that may lead to discontinuous modifications of firing activities. NA achieves this goal by aggregating finite differences of the loss over multiple perturbed membrane potential waveforms in the neighborhood of the present membrane potential of each neuron while utilizing a new membrane potential distance function. Our experiments show that the proposed NA algorithm delivers the state-of-the-art performance for SNN training on several datasets. </details>
<details>	<summary>邮件日期</summary>	2021年07月15日</details>

# 172、活性树突树可以减轻超导神经元扇入的限制
- [ ] An active dendritic tree can mitigate fan-in limitations in superconducting neurons 
时间：2021年07月12日                         第一作者：Bryce A. Primavera                        [链接](https://arxiv.org/abs/2107.05777).                     
## 摘要：超导电子电路在神经形态硬件方面有很多可提供的。超导量子干涉器件（SQUIDs）可以作为神经元胞体阈值操作的有源元件。然而，SQUID在应用信号中具有周期性的响应函数。我们从理论上证明，如果一个人限制对鱿鱼的总输入以维持一个单调递增的反应，那么一大部分突触必须是活跃的，以驱动神经元达到阈值。然后我们证明了一个活跃的树突树（也基于SQUIDs）可以显著减少突触的比例，这些突触必须活跃才能驱动神经元达到阈值。在这种情况下，树突树的加入提供了增强每个神经元的计算能力和允许神经元以稀疏的输入活动脉冲的双重好处。
<details>	<summary>英文摘要</summary>	Superconducting electronic circuits have much to offer with regard to neuromorphic hardware. Superconducting quantum interference devices (SQUIDs) can serve as an active element to perform the thresholding operation of a neuron's soma. However, a SQUID has a response function that is periodic in the applied signal. We show theoretically that if one restricts the total input to a SQUID to maintain a monotonically increasing response, a large fraction of synapses must be active to drive a neuron to threshold. We then demonstrate that an active dendritic tree (also based on SQUIDs) can significantly reduce the fraction of synapses that must be active to drive the neuron to threshold. In this context, the inclusion of a dendritic tree provides the dual benefits of enhancing the computational abilities of each neuron and allowing the neuron to spike with sparse input activity. </details>
<details>	<summary>注释</summary>	8 pages, 5 figures </details>
<details>	<summary>邮件日期</summary>	2021年07月14日</details>

# 171、更快速的SNN模拟，具有惰性+事件驱动的可塑性和共享原子
- [ ] Even Faster SNN Simulation with Lazy+Event-driven Plasticity and Shared Atomics 
时间：2021年07月08日                         第一作者：Dennis Bautembach                       [链接](https://arxiv.org/abs/2107.04092).                     
## 摘要：我们提出了两种新的优化方法来加速基于时钟的脉冲神经网络（SNN）模拟器。第一个目标是脉冲时间依赖性可塑性（STDP）。它结合了懒惰和事件驱动的可塑性，并有效地促进了使用位场和整数内部函数计算突触前和突触后峰值。它提供了更高的带宽比事件驱动塑性单独实现了1.5倍-2倍的加速比我们最接近的竞争对手。第二个优化目标是脉冲交货。我们以一种限制在任何给定时间需要更新的神经元数量的方式来划分我们的图表示，这允许我们在共享内存而不是全局内存中执行所述更新。这比我们最接近的竞争对手快2-2.5倍。这两种优化都代表了STDP和Spice（我们最先进的SNN模拟器）内部峰值交付多年迭代的最终进化阶段。所提出的优化不仅限于我们的图形表示或管道，而且适用于多种模拟器设计。我们在三个成熟的模型上评估我们的性能，并将我们自己与其他三个最先进的模拟器进行比较。
<details>	<summary>英文摘要</summary>	We present two novel optimizations that accelerate clock-based spiking neural network (SNN) simulators. The first one targets spike timing dependent plasticity (STDP). It combines lazy- with event-driven plasticity and efficiently facilitates the computation of pre- and post-synaptic spikes using bitfields and integer intrinsics. It offers higher bandwidth than event-driven plasticity alone and achieves a 1.5x-2x speedup over our closest competitor. The second optimization targets spike delivery. We partition our graph representation in a way that bounds the number of neurons that need be updated at any given time which allows us to perform said update in shared memory instead of global memory. This is 2x-2.5x faster than our closest competitor. Both optimizations represent the final evolutionary stages of years of iteration on STDP and spike delivery inside "Spice" (/spaIk/), our state of the art SNN simulator. The proposed optimizations are not exclusive to our graph representation or pipeline but are applicable to a multitude of simulator designs. We evaluate our performance on three well-established models and compare ourselves against three other state of the art simulators. </details>
<details>	<summary>注释</summary>	Submitted to IEEE-HPEC 2021 </details>
<details>	<summary>邮件日期</summary>	2021年07月12日</details>

# 170、一种用于人工智能的长-短期记忆算法在脉冲神经元硬件中的应用
- [ ] A Long Short-Term Memory for AI Applications in Spike-based Neuromorphic Hardware 
时间：2021年07月08日                         第一作者：Philipp Plank                       [链接](https://arxiv.org/abs/2107.03992).                     
## 摘要：尽管付出了大量的努力，但目前采用深度神经网络（DNNs）的人工智能（AI）方法在多大程度上可以在基于spike的神经形态硬件上更有效地实现仍然是一个悬而未决的问题。这尤其适用于解决序列处理任务的人工智能方法，序列处理任务是基于峰值的神经形态硬件的主要应用目标。一个困难是，用于此类任务的dnn通常采用长-短期记忆（LSTM）单元。然而，在基于峰值的硬件中对这些单元的有效仿真却一直缺失。我们提出了一个生物启发的解决方案，解决了这个问题。该解决方案使我们能够实现一类主要的dnn，用于序列处理任务，如时间序列分类和问答，并在神经形态硬件上节省大量的能量。事实上，我们用来回答问题的用于推理对象之间关系的关系网络是大型DNN的第一个例子，该DNN在神经形态硬件上执行序列处理任务，具有显著的节能效果。
<details>	<summary>英文摘要</summary>	In spite of intensive efforts it has remained an open problem to what extent current Artificial Intelligence (AI) methods that employ Deep Neural Networks (DNNs) can be implemented more energy-efficiently on spike-based neuromorphic hardware. This holds in particular for AI methods that solve sequence processing tasks, a primary application target for spike-based neuromorphic hardware. One difficulty is that DNNs for such tasks typically employ Long Short-Term Memory (LSTM) units. Yet an efficient emulation of these units in spike-based hardware has been missing. We present a biologically inspired solution that solves this problem. This solution enables us to implement a major class of DNNs for sequence processing tasks such as time series classification and question answering with substantial energy savings on neuromorphic hardware. In fact, the Relational Network for reasoning about relations between objects that we use for question answering is the first example of a large DNN that carries out a sequence processing task with substantial energy-saving on neuromorphic hardware. </details>
<details>	<summary>注释</summary>	Philipp Plank and Arjun Rao have contributed equally to this work as first authors </details>
<details>	<summary>邮件日期</summary>	2021年07月09日</details>

# 169、Q-SpiNN：一种量化脉冲神经网络的框架
- [ ] Q-SpiNN: A Framework for Quantizing Spiking Neural Networks 
时间：2021年07月05日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2107.01807).                     
## 摘要：量化是在不显著降低准确度的情况下减少脉冲神经网络（SNNs）内存占用的一项重要技术。然而，最新的技术仅集中于从特定量化方案直接采用权重量化，即训练后量化（PTQ）或训练量化（ITQ），并且不考虑（1）量化其他SNN参数（例如，神经元膜电位），（2）探索量化方法的不同组合（即量化方案、精度水平和舍入方案），以及（3）在最后选择具有良好记忆精度权衡的SNN模型。因此，由这些最新技术提供的用于满足目标精度的存储器节省是有限的，从而妨碍在资源受限的系统（例如，物联网边缘设备）上处理snn。针对这一点，我们提出了Q-SpiNN，一个新的量化框架的内存效率SNNs。Q-SpiNN的关键机制是：（1）根据不同SNN参数对精度的重要性对其进行量化；（2）探索量化方案、精度水平和舍入方案的不同组合，以找到有效的SNN模型候选，（3）开发一种算法，该算法量化了候选人获得的记忆精度折衷的好处，并选择帕累托最优的。实验结果表明，对于无监督网络，Q-SpiNN减少了约4倍的内存占用，同时在MNIST数据集上保持了1%的准确率。对于有监督的网络，Q-SpiNN减少了大约2倍的内存，同时将精度保持在DVS手势数据集基线的2%以内。
<details>	<summary>英文摘要</summary>	A prominent technique for reducing the memory footprint of Spiking Neural Networks (SNNs) without decreasing the accuracy significantly is quantization. However, the state-of-the-art only focus on employing the weight quantization directly from a specific quantization scheme, i.e., either the post-training quantization (PTQ) or the in-training quantization (ITQ), and do not consider (1) quantizing other SNN parameters (e.g., neuron membrane potential), (2) exploring different combinations of quantization approaches (i.e., quantization schemes, precision levels, and rounding schemes), and (3) selecting the SNN model with a good memory-accuracy trade-off at the end. Therefore, the memory saving offered by these state-of-the-art to meet the targeted accuracy is limited, thereby hindering processing SNNs on the resource-constrained systems (e.g., the IoT-Edge devices). Towards this, we propose Q-SpiNN, a novel quantization framework for memory-efficient SNNs. The key mechanisms of the Q-SpiNN are: (1) employing quantization for different SNN parameters based on their significance to the accuracy, (2) exploring different combinations of quantization schemes, precision levels, and rounding schemes to find efficient SNN model candidates, and (3) developing an algorithm that quantifies the benefit of the memory-accuracy trade-off obtained by the candidates, and selects the Pareto-optimal one. The experimental results show that, for the unsupervised network, the Q-SpiNN reduces the memory footprint by ca. 4x, while maintaining the accuracy within 1% from the baseline on the MNIST dataset. For the supervised network, the Q-SpiNN reduces the memory by ca. 2x, while keeping the accuracy within 2% from the baseline on the DVS-Gesture dataset. </details>
<details>	<summary>注释</summary>	Accepted for publication at the 2021 International Joint Conference on Neural Networks (IJCNN), July 2021, Virtual Event </details>
<details>	<summary>邮件日期</summary>	2021年07月06日</details>

# 168、DVS攻击：对动态视觉传感器的攻击
- [ ] DVS-Attacks: Adversarial Attacks on Dynamic Vision Sensors for Spiking Neural Networks 
时间：2021年07月01日                         第一作者：Alberto Marchisio                        [链接](https://arxiv.org/abs/2107.00415).                     
## 摘要：脉冲神经网络（SNN）尽管在神经形态硬件上实现时具有能量效率，并且与基于事件的动态视觉传感器（DV）相结合，但是容易受到安全威胁，例如对抗性攻击，即，为诱导误分类而添加到输入中的小扰动。为此，我们提出了DVS攻击，这是一套隐蔽而有效的对抗性攻击方法，旨在干扰构成snn输入的事件序列。首先，我们证明了DVS的噪声滤波器可以作为对抗攻击的防御机制。然后，我们实现了几种攻击，并在DVS摄像机的两种噪声滤波器的情况下进行了测试。实验结果表明，所设计的滤波器只能部分地抵抗我们提出的DVS攻击。使用最佳的噪声滤波器设置，我们提出的基于掩码滤波器的破折号攻击在DVS手势数据集和MNIST数据集上的准确率分别比原始干净帧降低了20%和65%以上。所有建议的DVS攻击和噪声滤波器的源代码发布于https://github.com/albertomarchisio/DVS-Attacks.
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs), despite being energy-efficient when implemented on neuromorphic hardware and coupled with event-based Dynamic Vision Sensors (DVS), are vulnerable to security threats, such as adversarial attacks, i.e., small perturbations added to the input for inducing a misclassification. Toward this, we propose DVS-Attacks, a set of stealthy yet efficient adversarial attack methodologies targeted to perturb the event sequences that compose the input of the SNNs. First, we show that noise filters for DVS can be used as defense mechanisms against adversarial attacks. Afterwards, we implement several attacks and test them in the presence of two types of noise filters for DVS cameras. The experimental results show that the filters can only partially defend the SNNs against our proposed DVS-Attacks. Using the best settings for the noise filters, our proposed Mask Filter-Aware Dash Attack reduces the accuracy by more than 20% on the DVS-Gesture dataset and by more than 65% on the MNIST dataset, compared to the original clean frames. The source code of all the proposed DVS-Attacks and noise filters is released at https://github.com/albertomarchisio/DVS-Attacks. </details>
<details>	<summary>注释</summary>	Accepted for publication at IJCNN 2021 </details>
<details>	<summary>邮件日期</summary>	2021年07月02日</details>

# 167、CarSNN：一种基于Loihi神经形态研究处理器的基于事件的自主汽车的高效脉冲神经网络
- [ ] CarSNN: An Efficient Spiking Neural Network for Event-Based Autonomous Cars on the Loihi Neuromorphic Research Processor 
时间：2021年07月01日                         第一作者：Alberto Viale                        [链接](https://arxiv.org/abs/2107.00401).                     
## 摘要：与自动驾驶（AD）相关的功能提供了新的移动性形式，这也有利于其他类型的智能和自动系统，如机器人、智能交通和智能工业。对于这些应用程序，需要快速、实时地做出决策。此外，在寻求电动移动性的过程中，这项任务必须遵循低功耗策略，而不影响交通工具或机器人的自主性。这两个挑战可以利用新兴的脉冲神经网络（SNNs）来解决。当部署在专门的神经形态硬件上时，SNNs可以以低延迟和低功耗实现高性能。在本文中，我们使用一个连接到基于事件的摄像机的SNN来解决AD的一个关键问题，即汽车和其他物体之间的分类。为了比传统的基于帧的相机消耗更少的能量，我们使用了动态视觉传感器（DVS）。实验遵循离线监督学习规则进行，然后将学习到的SNN模型映射到Intel-Loihi神经形态研究芯片上。我们的最佳实验实现了86%的离线实现准确率，当它被移植到Loihi芯片上时，准确率下降到83%。神经形态硬件实现每个样本的最大延迟为0.72ms，仅消耗310mw。据我们所知，这项工作是第一个在神经形态芯片上实现基于事件的汽车分类器。
<details>	<summary>英文摘要</summary>	Autonomous Driving (AD) related features provide new forms of mobility that are also beneficial for other kind of intelligent and autonomous systems like robots, smart transportation, and smart industries. For these applications, the decisions need to be made fast and in real-time. Moreover, in the quest for electric mobility, this task must follow low power policy, without affecting much the autonomy of the mean of transport or the robot. These two challenges can be tackled using the emerging Spiking Neural Networks (SNNs). When deployed on a specialized neuromorphic hardware, SNNs can achieve high performance with low latency and low power consumption. In this paper, we use an SNN connected to an event-based camera for facing one of the key problems for AD, i.e., the classification between cars and other objects. To consume less power than traditional frame-based cameras, we use a Dynamic Vision Sensor (DVS). The experiments are made following an offline supervised learning rule, followed by mapping the learnt SNN model on the Intel Loihi Neuromorphic Research Chip. Our best experiment achieves an accuracy on offline implementation of 86%, that drops to 83% when it is ported onto the Loihi Chip. The Neuromorphic Hardware implementation has maximum 0.72 ms of latency for every sample, and consumes only 310 mW. To the best of our knowledge, this work is the first implementation of an event-based car classifier on a Neuromorphic Chip. </details>
<details>	<summary>注释</summary>	Accepted for publication at IJCNN 2021 </details>
<details>	<summary>邮件日期</summary>	2021年07月02日</details>

# 166、基于脉冲神经网络的三维趋化算法
- [ ] Algorithm For 3D-Chemotaxis Using Spiking Neural Network 
时间：2021年06月30日                         第一作者：Jayesh Choudhary                       [链接](https://arxiv.org/abs/2106.16215).                     
## 摘要：在这项工作中，我们的目标是设计一个端到端的脉冲实现在三维媒体的轮廓跟踪启发趋化，其中蠕虫到达的地区，有给定的集合浓度。对于刨床介质，有效的轮廓跟踪算法已经被设计出来，但是新的自由度有相当多的挑战。在这里，我们设计了一个基于klinokinesis的算法——蠕虫的运动是对刺激的响应，但与刺激不成正比。因此，所遵循的路径不是最短的，但我们可以成功地跟踪设定浓度。考虑到在神经形态计算硬件上实现的可行性，我们使用简单的LIF神经元来实现神经网络。
<details>	<summary>英文摘要</summary>	In this work, we aim to devise an end-to-end spiking implementation for contour tracking in 3D media inspired by chemotaxis, where the worm reaches the region which has the given set concentration. For a planer medium, efficient contour tracking algorithms have already been devised, but a new degree of freedom has quite a few challenges. Here we devise an algorithm based on klinokinesis - where the motion of the worm is in response to the stimuli but not proportional to it. Thus the path followed is not the shortest, but we can track the set concentration successfully. We are using simple LIF neurons for the neural network implementation, considering the feasibility of its implementation in the neuromorphic computing hardware. </details>
<details>	<summary>注释</summary>	12 pages, 8 figures, accepted for the '30th International Conference on Artificial Neural Networks, ICANN2021' </details>
<details>	<summary>邮件日期</summary>	2021年07月01日</details>

# 165、Spiking-GAN：一种使用时间到第一个Spike编码的生成性攻击网络
- [ ] Spiking-GAN: A Spiking Generative Adversarial Network Using Time-To-First-Spike Coding 
时间：2021年06月29日                         第一作者：Vineet Kotariya                       [链接](https://arxiv.org/abs/2106.15420).                     
## 摘要：脉冲神经网络（SNNs）在解决深度学习问题中显示出巨大的潜力。然而，它们仍然局限于简单的分类任务。在本文中，我们提出了第一个基于脉冲的生成性对抗网络（GAN）。它采用了一种称为时间到第一峰值编码的时态编码方案。我们使用时域近似反向传播来训练它。我们使用简单的集成和火灾（如果）神经元非常高的不应期为我们的网络，这确保了一个神经元的最大峰值。这使得该模型比基于峰值速率的系统要稀疏得多。我们改进的时间损失函数称为“攻击性TTFS”，与以前的工作相比，网络的推理时间提高了33%以上，网络中的脉冲数减少了11%以上。实验表明，利用该方法在MNIST数据集上训练网络，可以生成高质量的样本。从而证明了该框架在解决脉冲域中的此类问题方面的潜力。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have shown great potential in solving deep learning problems in an energy-efficient manner. However, they are still limited to simple classification tasks. In this paper, we propose Spiking-GAN, the first spike-based Generative Adversarial Network (GAN). It employs a kind of temporal coding scheme called time-to-first-spike coding. We train it using approximate backpropagation in the temporal domain. We use simple integrate-and-fire (IF) neurons with very high refractory period for our network which ensures a maximum of one spike per neuron. This makes the model much sparser than a spike rate-based system. Our modified temporal loss function called 'Aggressive TTFS' improves the inference time of the network by over 33% and reduces the number of spikes in the network by more than 11% compared to previous works. Our experiments show that on training the network on the MNIST dataset using this approach, we can generate high quality samples. Thereby demonstrating the potential of this framework for solving such problems in the spiking domain. </details>
<details>	<summary>邮件日期</summary>	2021年06月30日</details>

# 164、分叉脉冲神经网络
- [ ] Bifurcation Spiking Neural Network 
时间：2021年06月25日                         第一作者：Shao-Qun Zhang                        [链接](https://arxiv.org/abs/1909.08341).                     
<details>	<summary>注释</summary>	18 pages </details>
<details>	<summary>邮件日期</summary>	2021年06月28日</details>

# 163、群体编码和动态神经元改进的脉冲参与者网络用于强化学习
- [ ] Population-coding and Dynamic-neurons improved Spiking Actor Network for Reinforcement Learning 
时间：2021年06月23日                         第一作者：Duzhen Zhang                       [链接](https://arxiv.org/abs/2106.07854).                     
<details>	<summary>注释</summary>	27 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月24日</details>

# 162、深度脉冲神经网络的梯度重排剪枝
- [ ] Pruning of Deep Spiking Neural Networks through Gradient Rewiring 
时间：2021年06月22日                         第一作者：Yanqi Chen                       [链接](https://arxiv.org/abs/2105.04916).                     
<details>	<summary>注释</summary>	9 pages, 7 figures, 4 tables. To appear in the 30th International Joint Conference on Artificial Intelligence (IJCAI 2021) </details>
<details>	<summary>邮件日期</summary>	2021年06月23日</details>

# 161、约束塑性储备作为神经网络频率和权值控制的一种自然方法
- [ ] Constrained plasticity reserve as a natural way to control frequency and weights in spiking neural networks 
时间：2021年06月20日                         第一作者：Oleg Nikitin                        [链接](https://arxiv.org/abs/2103.08143).                     
<details>	<summary>注释</summary>	24 pages, 10 figures MSC-class: 68T05 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 160、SiamSNN：用于节能目标跟踪的siames脉冲神经网络
- [ ] SiamSNN: Siamese Spiking Neural Networks for Energy-Efficient Object Tracking 
时间：2021年06月19日                         第一作者：Yihao Luo                       [链接](https://arxiv.org/abs/2003.07584).                     
<details>	<summary>注释</summary>	Accepted by ICANN2021, 12 pages, 5figures </details>
<details>	<summary>邮件日期</summary>	2021年06月22日</details>

# 159、VLSI电路约束对时序编码多层脉冲神经网络的影响
- [ ] Effects of VLSI Circuit Constraints on Temporal-Coding Multilayer Spiking Neural Networks 
时间：2021年06月18日                         第一作者：Yusuke Sakemi                       [链接](https://arxiv.org/abs/2106.10382).                     
## 摘要：脉冲神经网络（spiking neural network，SNN）不仅作为大脑的一种数学模型，而且作为一种能量有效的信息处理模型，在现实世界中得到了广泛的应用。特别地，基于时态编码的snn被期望比基于速率编码的snn更有效，因为前者需要更少的峰值来执行任务。由于snn是连续状态和连续时间模型，用模拟VLSI电路实现snn是非常有利的。然而，当系统规模很大时，用连续时间模拟电路构建整个系统是不可行的。因此，必须采用混合信号电路，对突触权值进行时间离散化和量化。此外，SNNs的模拟VLSI实现具有非理想性，例如噪声和器件失配的影响，以及模拟电路操作引起的其他限制。在这项研究中，我们研究了时间离散化和/或权重量化对SNNs性能的影响。此外，我们还阐明了膜电位下限和放电阈值的时间波动的影响。最后，我们提出了一种将数学SNN模型映射到离散时间模拟电路的优化方法。
<details>	<summary>英文摘要</summary>	The spiking neural network (SNN) has been attracting considerable attention not only as a mathematical model for the brain, but also as an energy-efficient information processing model for real-world applications. In particular, SNNs based on temporal coding are expected to be much more efficient than those based on rate coding, because the former requires substantially fewer spikes to carry out tasks. As SNNs are continuous-state and continuous-time models, it is favorable to implement them with analog VLSI circuits. However, the construction of the entire system with continuous-time analog circuits would be infeasible when the system size is very large. Therefore, mixed-signal circuits must be employed, and the time discretization and quantization of the synaptic weights are necessary. Moreover, the analog VLSI implementation of SNNs exhibits non-idealities, such as the effects of noise and device mismatches, as well as other constraints arising from the analog circuit operation. In this study, we investigated the effects of the time discretization and/or weight quantization on the performance of SNNs. Furthermore, we elucidated the effects the lower bound of the membrane potentials and the temporal fluctuation of the firing threshold. Finally, we propose an optimal approach for the mapping of mathematical SNN models to analog circuits with discretized time. </details>
<details>	<summary>注释</summary>	corrected typos </details>
<details>	<summary>邮件日期</summary>	2021年06月28日</details>

# 158、基于模糊神经遗传算法的水电站恢复力优化规划
- [ ] HydroPower Plant Planning for Resilience Improvement of Power Systems using Fuzzy-Neural based Genetic Algorithm 
时间：2021年06月16日                         第一作者：Akbal Rain                       [链接](https://arxiv.org/abs/2106.12042).                     
## 摘要：本文提出了一种基于负荷频率控制（LFC）的小水电站优化设计新方法，该方法采用自校正模糊比例微分（PD）方法对规划进行估计和预测。由于频率不受任何甩负荷或其它因素的控制，因此该电站处于动态频率变化下，采用PD控制器进行模糊规则优化，再结合神经深度学习技术和遗传算法进行优化。这项工作的主要目的是使小水电站的频率保持在额定值。因此，本文提出的模糊PD遗传优化控制器将应用于小规模水电系统的线性调频控制。该方案可用于小水电和柴油发电机组的不同设计。也可以在水电系统中使用柴油发电机，当用户需求高于发电量时，柴油发电机可以关闭。仿真将在MATLAB/Simulink中进行，以表示和评估这种控制方案在动态频率变化下的性能。以脉冲神经网络（SNN）为主要的深度学习技术，对这种负载频率控制进行优化，形成深度脉冲神经网络（DSNN）。结果表明，与其他方法相比，该方案具有鲁棒性强、性能高的频率控制性能。
<details>	<summary>英文摘要</summary>	This paper will propose a novel technique for optimize hydropower plant in small scale based on load frequency control (LFC) which use self-tuning fuzzy Proportional- Derivative (PD) method for estimation and prediction of planning. Due to frequency is not controlled by any dump load or something else, so this power plant is under dynamic frequency variations that will use PD controller which optimize by fuzzy rules and then with neural deep learning techniques and Genetic Algorithm optimization. The main purpose of this work is because to maintain frequency in small-hydropower plant at nominal value. So, proposed controller means Fuzzy PD optimization with Genetic Algorithm will be used for LFC in small scale of hydropower system. The proposed schema can be used in different designation of both diesel generator and mini-hydropower system at low stream flow. It is also possible to use diesel generator at the hydropower system which can be turn off when Consumer demand is higher than electricity generation. The simulation will be done in MATLAB/Simulink to represent and evaluate the performance of this control schema under dynamic frequency variations. Spiking Neural Network (SNN) used as the main deep learning techniques to optimizing this load frequency control which turns into Deep Spiking Neural Network (DSNN). Obtained results represented that the proposed schema has robust and high-performance frequency control in comparison to other methods. </details>
<details>	<summary>注释</summary>	9 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月24日</details>

# 157、深相量网络：连接传统和脉冲神经网络
- [ ] Deep Phasor Networks: Connecting Conventional and Spiking Neural Networks 
时间：2021年06月15日                         第一作者：Wilkie Olin-Ammentorp                       [链接](https://arxiv.org/abs/2106.11908).                     
## 摘要：在这项工作中，我们扩展了标准的神经网络，假设神经元的激活与单位圆上的复数的角度相对应，即“相量”。这样的网络中的每一层通过对前一层的相位进行加权叠加并计算新的相位值来产生新的激活。这种广义体系结构允许模型达到高精度，并具有独特的优点，即可以在考虑或不考虑时间变量的情况下执行网络的数学等效版本。重要的是，时域中相位角的值可以稀疏地由一系列周期性重复的δ函数或“脉冲”表示。我们展示了在标准深度学习任务上相量网络的非时域训练，并且证明了这些网络可以在传统的非时域或脉冲时域中执行，而不需要转换步骤。这为构建深度网络提供了一个新的基础，该网络通过适合于神经形态计算硬件的基于时间脉冲的计算进行操作。
<details>	<summary>英文摘要</summary>	In this work, we extend standard neural networks by building upon an assumption that neuronal activations correspond to the angle of a complex number lying on the unit circle, or 'phasor.' Each layer in such a network produces new activations by taking a weighted superposition of the previous layer's phases and calculating the new phase value. This generalized architecture allows models to reach high accuracy and carries the singular advantage that mathematically equivalent versions of the network can be executed with or without regard to a temporal variable. Importantly, the value of a phase angle in the temporal domain can be sparsely represented by a periodically repeating series of delta functions or 'spikes'. We demonstrate the atemporal training of a phasor network on standard deep learning tasks and show that these networks can then be executed in either the traditional atemporal domain or spiking temporal domain with no conversion step needed. This provides a novel basis for constructing deep networkswhich operate via temporal, spike-based calculations suitable for neuromorphic computing hardware. </details>
<details>	<summary>注释</summary>	18 pages, 6 figures </details>
<details>	<summary>邮件日期</summary>	2021年06月23日</details>

# 156、群体编码和动态神经元改进的脉冲参与者网络用于强化学习
- [ ] Population-coding and Dynamic-neurons improved Spiking Actor Network for Reinforcement Learning 
时间：2021年06月15日                         第一作者：Duzhen Zhang                       [链接](https://arxiv.org/abs/2106.07854).                     
## 摘要：由于深度神经网络（DNNs）是一种功能强大的函数逼近器，深度强化学习（DRL）在机器人控制任务中得到了很好的应用。与普通人工神经元的dnn相比，具有生物合理性的脉冲神经元网络（SNN）包含了不同数量的脉冲神经元，使得它在时空信息的状态表示上具有很强的自然能力。基于一个混合学习框架，即脉冲-参与者网络从状态中推断行为，而深度批评网络评估参与者，我们提出了一个群体编码和动态神经元改进的脉冲-参与者网络（PDSAN），用于从两个不同的标度（输入编码和神经元编码）进行有效的状态表示。对于输入编码，我们采用具有动态感受野的总体编码来直接编码每个输入状态分量。对于神经元编码，我们提出了不同类型的动态神经元（包括一阶和二阶神经元动力学）来描述更复杂的神经元动力学。最后，使用双延迟深度确定性策略梯度算法（TD3-PDSAN）结合深度批评网络对PDSAN进行训练。大量的实验结果表明，我们的TD3-PDSAN模型在四个OpenAI健身房基准任务上取得了比现有模型更好的性能。利用SNN改进RL，使其向满足生物合理性的有效计算方向发展，是一个重要的尝试。
<details>	<summary>英文摘要</summary>	With the Deep Neural Networks (DNNs) as a powerful function approximator, Deep Reinforcement Learning (DRL) has been excellently demonstrated on robotic control tasks. Compared to DNNs with vanilla artificial neurons, the biologically plausible Spiking Neural Network (SNN) contains a diverse population of spiking neurons, making it naturally powerful on state representation with spatial and temporal information. Based on a hybrid learning framework, where a spike actor-network infers actions from states and a deep critic network evaluates the actor, we propose a Population-coding and Dynamic-neurons improved Spiking Actor Network (PDSAN) for efficient state representation from two different scales: input coding and neuronal coding. For input coding, we apply population coding with dynamically receptive fields to directly encode each input state component. For neuronal coding, we propose different types of dynamic-neurons (containing 1st-order and 2nd-order neuronal dynamics) to describe much more complex neuronal dynamics. Finally, the PDSAN is trained in conjunction with deep critic networks using the Twin Delayed Deep Deterministic policy gradient algorithm (TD3-PDSAN). Extensive experimental results show that our TD3-PDSAN model achieves better performance than state-of-the-art models on four OpenAI gym benchmark tasks. It is an important attempt to improve RL with SNN towards the effective computation satisfying biological plausibility. </details>
<details>	<summary>注释</summary>	27 pages, 11 figures, accepted by Journal of Neural Networks </details>
<details>	<summary>邮件日期</summary>	2021年06月16日</details>

# 155、基于脉冲时变塑性和梯度下降的脉冲神经网络SAR图像分类
- [ ] SAR Image Classification Based on Spiking Neural Network through Spike-Time Dependent Plasticity and Gradient Descent 
时间：2021年06月15日                         第一作者：Jiankun Chen                       [链接](https://arxiv.org/abs/2106.08005).                     
## 摘要：目前，基于卷积神经网络（CNN）的合成孔径雷达（SAR）图像分类方法存在抗噪性差、泛化能力差等问题。脉冲神经网络是类脑智能的核心组成部分之一，具有良好的应用前景。本文利用具有复杂时空信息的脉冲序列，基于SNN的无监督和有监督学习，构造了一个完整的SAR图像分类器。本文首先阐述了脉冲神经元模型、SNN的感受野以及脉冲序列的构建。提出了一种基于STDP的无监督学习算法和一种基于梯度下降的有监督学习算法。在MSTAR数据集的三类图像中，单层和双层无监督学习SNN的平均分类准确率分别为80.8%和85.1%。此外，无监督学习的收敛输出脉冲序列可以作为教学信号。基于TensorFlow框架，自下而上构建了单层监督学习SNN，分类准确率达到90.05%。通过比较SNN和cnn的抗噪性能和模型参数，验证了SNN的有效性和突出的优点。复制我们实验的代码可从\url获得{https://github.com/Jiankun-chen/Supervised-SNN-with-GD}.
<details>	<summary>英文摘要</summary>	At present, the Synthetic Aperture Radar (SAR) image classification method based on convolution neural network (CNN) has faced some problems such as poor noise resistance and generalization ability. Spiking neural network (SNN) is one of the core components of brain-like intelligence and has good application prospects. This article constructs a complete SAR image classifier based on unsupervised and supervised learning of SNN by using spike sequences with complex spatio-temporal information. We firstly expound the spiking neuron model, the receptive field of SNN, and the construction of spike sequence. Then we put forward an unsupervised learning algorithm based on STDP and a supervised learning algorithm based on gradient descent. The average classification accuracy of single layer and bilayer unsupervised learning SNN in three categories images on MSTAR dataset is 80.8\% and 85.1\%, respectively. Furthermore, the convergent output spike sequences of unsupervised learning can be used as teaching signals. Based on the TensorFlow framework, a single layer supervised learning SNN is built from the bottom, and the classification accuracy reaches 90.05\%. By comparing noise resistance and model parameters between SNNs and CNNs, the effectiveness and outstanding advantages of SNN are verified. Code to reproduce our experiments is available at \url{https://github.com/Jiankun-chen/Supervised-SNN-with-GD}. </details>
<details>	<summary>邮件日期</summary>	2021年06月16日</details>

# 154、脉冲神经网络的能量有效知识提取
- [ ] Energy-efficient Knowledge Distillation for Spiking Neural Networks 
时间：2021年06月14日                         第一作者：Dongjin Lee                       [链接](https://arxiv.org/abs/2106.07172).                     
## 摘要：脉冲神经网络（SNNs）作为传统人工神经网络（ANNs）的一种高效节能的替代方法，由于其事件驱动的计算能力而受到广泛关注。考虑到SNN模型在约束神经形态设备中的应用前景，许多研究将最初用于ANN模型压缩的技术，如网络量化、剪枝和知识提取等，应用到SNN中。其中，已有的关于知识提炼的研究报道了student-SNN模型的精度改进。然而，对SNN的一个重要特征&能量效率的分析却很少。在本文中，我们从准确性和能源效率两个方面深入分析了提取的SNN模型的性能。在这个过程中，我们观察到，当使用传统的知识蒸馏方法时，脉冲数量大幅增加，导致能源效率低下。在此基础上，提出了一种基于非均匀温度参数的知识提取方法。我们在两个不同的数据集上对我们的方法进行了评估，结果表明SNN学生既满足准确性的提高，也满足脉冲数目的减少。在MNIST数据集上，我们提出的学生SNN与传统知识提取方法训练的学生SNN相比，准确率提高了0.09%，峰值减少了65%。我们还将结果与其他SNN压缩技术和训练方法进行了比较。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have been gaining interest as energy-efficient alternatives of conventional artificial neural networks (ANNs) due to their event-driven computation. Considering the future deployment of SNN models to constrained neuromorphic devices, many studies have applied techniques originally used for ANN model compression, such as network quantization, pruning, and knowledge distillation, to SNNs. Among them, existing works on knowledge distillation reported accuracy improvements of student SNN model. However, analysis on energy efficiency, which is also an important feature of SNN, was absent. In this paper, we thoroughly analyze the performance of the distilled SNN model in terms of accuracy and energy efficiency. In the process, we observe a substantial increase in the number of spikes, leading to energy inefficiency, when using the conventional knowledge distillation methods. Based on this analysis, to achieve energy efficiency, we propose a novel knowledge distillation method with heterogeneous temperature parameters. We evaluate our method on two different datasets and show that the resulting SNN student satisfies both accuracy improvement and reduction of the number of spikes. On MNIST dataset, our proposed student SNN achieves up to 0.09% higher accuracy and produces 65% less spikes compared to the student SNN trained with conventional knowledge distillation method. We also compare the results with other SNN compression techniques and training methods. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 153、反向传播算法在脉冲神经元硬件上的实现
- [ ] The Backpropagation Algorithm Implemented on Spiking Neuromorphic Hardware 
时间：2021年06月13日                         第一作者：Alpha Renner                       [链接](https://arxiv.org/abs/2106.07030).                     
## 摘要：自然神经系统的能力激发了新一代机器学习算法以及能够快速、低功耗信息处理的神经形态超大规模集成（VLSI）电路。然而，大多数现代机器学习算法在神经生理学上并不合理，因此不能直接在神经形态硬件中实现。特别是，现代深度学习的主力，反向传播算法，已经证明很难转化为神经形态的硬件。在这项研究中，我们提出了一个基于脉冲门控动态信息协调和处理的神经形态，脉冲反向传播算法，实现在英特尔的Loihi神经形态研究处理器。我们展示了一个原理证明三层电路，学习从MNIST数据集分类数字。这个实现展示了在现代深度学习应用程序中使用大规模并行、低功耗、低延迟的神经形态处理器的途径。
<details>	<summary>英文摘要</summary>	The capabilities of natural neural systems have inspired new generations of machine learning algorithms as well as neuromorphic very large-scale integrated (VLSI) circuits capable of fast, low-power information processing. However, most modern machine learning algorithms are not neurophysiologically plausible and thus are not directly implementable in neuromorphic hardware. In particular, the workhorse of modern deep learning, the backpropagation algorithm, has proven difficult to translate to neuromorphic hardware. In this study, we present a neuromorphic, spiking backpropagation algorithm based on pulse-gated dynamical information coordination and processing, implemented on Intel's Loihi neuromorphic research processor. We demonstrate a proof-of-principle three-layer circuit that learns to classify digits from the MNIST dataset. This implementation shows a path for using massively parallel, low-power, low-latency neuromorphic processors in modern deep learning applications. </details>
<details>	<summary>注释</summary>	20 pages, 5 figures Report-no: LA-UR-21-24457 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 152、基于脉冲神经网络的联合学习
- [ ] Federated Learning with Spiking Neural Networks 
时间：2021年06月11日                         第一作者：Yeshwanth Venkatesha                       [链接](https://arxiv.org/abs/2106.06579).                     
## 摘要：随着神经网络在资源受限的嵌入式设备中的广泛应用，对低功耗神经系统的需求也越来越大。脉冲神经网络（SNNs）是一种新兴的能源效率的替代传统的人工神经网络（ANNs）是众所周知的计算密集型。从应用的角度来看，由于联合学习涉及多个能量受限的设备，因此利用SNNs提供的能量效率有很大的空间。尽管snn非常重要，但是在像联邦学习这样的大规模分布式系统上训练snn的研究却很少。在本文中，我们将SNNs引入到一个更现实的联邦学习场景中。具体来说，我们提出了一个联邦学习框架，分散和隐私保护培训的SNN。为了验证所提出的联邦学习框架，我们使用CIFAR10和CIFAR100基准测试评估了SNNs在联邦学习各个方面的优势。我们观察到，当数据分布在联邦中的大量客户机上时，SNNs在总体准确率方面优于ANNs，超过15%，同时提供高达5.3倍的能效。除了效率之外，我们还分析了所提出的联邦SNN框架对客户端数据分布、散乱和梯度噪声的敏感性，并与人工神经网络进行了综合比较。
<details>	<summary>英文摘要</summary>	As neural networks get widespread adoption in resource-constrained embedded devices, there is a growing need for low-power neural systems. Spiking Neural Networks (SNNs)are emerging to be an energy-efficient alternative to the traditional Artificial Neural Networks (ANNs) which are known to be computationally intensive. From an application perspective, as federated learning involves multiple energy-constrained devices, there is a huge scope to leverage energy efficiency provided by SNNs. Despite its importance, there has been little attention on training SNNs on a large-scale distributed system like federated learning. In this paper, we bring SNNs to a more realistic federated learning scenario. Specifically, we propose a federated learning framework for decentralized and privacy-preserving training of SNNs. To validate the proposed federated learning framework, we experimentally evaluate the advantages of SNNs on various aspects of federated learning with CIFAR10 and CIFAR100 benchmarks. We observe that SNNs outperform ANNs in terms of overall accuracy by over 15% when the data is distributed across a large number of clients in the federation while providing up to5.3x energy efficiency. In addition to efficiency, we also analyze the sensitivity of the proposed federated SNN framework to data distribution among the clients, stragglers, and gradient noise and perform a comprehensive comparison with ANNs. </details>
<details>	<summary>邮件日期</summary>	2021年06月15日</details>

# 151、具有平衡突触的单混合信号神经元的时空棘波模式选择性
- [ ] Spatiotemporal Spike-Pattern Selectivity in Single Mixed-Signal Neurons with Balanced Synapses 
时间：2021年06月10日                         第一作者：Mattias Nilsson                       [链接](https://arxiv.org/abs/2106.05686).                     
## 摘要：实现混合信号神经形态处理器超低功耗推理和学习的潜力，需要有效地利用其非均匀模拟电路以及稀疏的、基于时间的信息编码和处理。在这里，我们研究了时空相关器（STC）网络中输出神经元的基于脉冲时间的时空感受野，我们使用兴奋-抑制平衡的突触前输入代替专门的轴突或神经元延迟。我们提出了一个混合信号DYNAP-SE神经形态处理器的半实物实验，其中硬件神经元的五维感受野通过随机抽样均匀分布的输入脉冲模式来映射。我们发现，当平衡的突触前成分被随机编程时，一些神经元显示出不同的感受野。此外，我们还演示了如何通过激活不同的非均匀模拟突触电路子集，调整神经元以检测特定的时空特征，而神经元最初是非选择性的。与以前基于延迟的神经形态硬件实现相比，平衡的突触元件的能量耗散比每侧连接低一个数量级（0.65nj对9.3nj）。因此，我们展示了如何利用不均匀的突触电路来实现STC网络层的资源有效性，使突触地址重编程成为一种离散的特征调整机制。
<details>	<summary>英文摘要</summary>	Realizing the potential of mixed-signal neuromorphic processors for ultra-low-power inference and learning requires efficient use of their inhomogeneous analog circuitry as well as sparse, time-based information encoding and processing. Here, we investigate spike-timing-based spatiotemporal receptive fields of output-neurons in the Spatiotemporal Correlator (STC) network, for which we used excitatory-inhibitory balanced disynaptic inputs instead of dedicated axonal or neuronal delays. We present hardware-in-the-loop experiments with a mixed-signal DYNAP-SE neuromorphic processor, in which five-dimensional receptive fields of hardware neurons were mapped by randomly sampling input spike-patterns from a uniform distribution. We find that, when the balanced disynaptic elements are randomly programmed, some of the neurons display distinct receptive fields. Furthermore, we demonstrate how a neuron was tuned to detect a particular spatiotemporal feature, to which it initially was non-selective, by activating a different subset of the inhomogeneous analog synaptic circuits. The energy dissipation of the balanced synaptic elements is one order of magnitude lower per lateral connection (0.65 nJ vs 9.3 nJ per spike) than former delay-based neuromorphic hardware implementations. Thus, we show how the inhomogeneous synaptic circuits could be utilized for resource-efficient implementation of STC network layers, in a way that enables synapse-address reprogramming as a discrete mechanism for feature tuning. </details>
<details>	<summary>注释</summary>	This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible </details>
<details>	<summary>邮件日期</summary>	2021年06月11日</details>

# 150、基于反向传播的深脉冲神经网络时间脉冲序列学习
- [ ] Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks 
时间：2021年06月07日                         第一作者：Wenrui Zhang                       [链接](https://arxiv.org/abs/2002.10085).                     
<details>	<summary>注释</summary>	Accepted for spotlight presentation of NeurIPS (Neural Information Processing System) 2020: https://proceedings.neurips.cc/paper/2020/hash/8bdb5058376143fa358981954e7626b8-Abstract.html </details>
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 149、脉冲神经网络中的深度剩余学习
- [ ] Deep Residual Learning in Spiking Neural Networks 
时间：2021年06月05日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2102.04159).                     
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 148、SpikePropamine：脉冲神经网络的可微可塑性
- [ ] SpikePropamine: Differentiable Plasticity in Spiking Neural Networks 
时间：2021年06月04日                         第一作者：Samuel Schmidgall                       [链接](https://arxiv.org/abs/2106.02681).                     
## 摘要：在生物神经网络的学习过程中，突触效能的适应性变化已经被证明起着关键的作用。尽管有这样的灵感来源，许多使用脉冲神经网络（SNNs）的以学习为中心的应用程序仍然保持静态的突触连接，阻止了初始训练期后的额外学习。在这里，我们介绍了一个框架，同时学习潜在的固定权重和规则的动态突触可塑性和神经调节突触可塑性在SNNs通过梯度下降。我们进一步展示了这个框架在一系列具有挑战性的基准上的能力，学习了几个可塑性规则的参数，包括BCM、Oja以及它们各自的一组神经调节变体。实验结果表明，具有可微可塑性的SNN足以解决一组传统SNN无法解决的具有挑战性的时间学习任务，即使在存在显著噪声的情况下也是如此。这些网络也被证明能够在高维机器人学习任务中产生运动，在初始训练期间没有出现的新情况下，可以观察到几乎最小的性能退化。
<details>	<summary>英文摘要</summary>	The adaptive changes in synaptic efficacy that occur between spiking neurons have been demonstrated to play a critical role in learning for biological neural networks. Despite this source of inspiration, many learning focused applications using Spiking Neural Networks (SNNs) retain static synaptic connections, preventing additional learning after the initial training period. Here, we introduce a framework for simultaneously learning the underlying fixed-weights and the rules governing the dynamics of synaptic plasticity and neuromodulated synaptic plasticity in SNNs through gradient descent. We further demonstrate the capabilities of this framework on a series of challenging benchmarks, learning the parameters of several plasticity rules including BCM, Oja's, and their respective set of neuromodulatory variants. The experimental results display that SNNs augmented with differentiable plasticity are sufficient for solving a set of challenging temporal learning tasks that a traditional SNN fails to solve, even in the presence of significant noise. These networks are also shown to be capable of producing locomotion on a high-dimensional robotic learning task, where near-minimal degradation in performance is observed in the presence of novel conditions not seen during the initial training period. </details>
<details>	<summary>邮件日期</summary>	2021年06月08日</details>

# 147、基于时间到第一脉冲编码的能量有效的深脉冲神经网络训练
- [ ] Training Energy-Efficient Deep Spiking Neural Networks with Time-to-First-Spike Coding 
时间：2021年06月04日                         第一作者：Seongsik Park                       [链接](https://arxiv.org/abs/2106.02568).                     
## 摘要：深度神经网络（DNNs）的巨大能量消耗已经成为深度学习中的一个严重问题。脉冲神经网络（Spiking neural networks，SNNs）是一种模拟人脑操作的高效节能神经网络。由于snn的事件驱动和时空稀疏操作，使得snn具有高效节能处理的可能性。为了释放它们的潜能，深部snn采用了时间编码，如TTFS（time-To-first spike）编码，它代表了神经元之间在第一个峰值时间的信息。通过TTFS编码，每个神经元最多产生一个脉冲，从而显著提高了能量效率。已有一些研究成功地将TTFS编码引入深度snn，但由于缺乏对训练效率的考虑，其效率的提高受到限制。针对上述问题，本文提出了基于TTFS编码的能量有效的深度snn训练方法。我们引入了一个代理DNN模型来在可行的时间内训练深度SNN，并分析了时态核对训练性能和效率的影响。在此基础上，我们提出了随机松弛激活和基于初值的时间核参数正则化方法。此外，为了进一步减少峰值的数量，我们提出了时态核感知的批处理规范化。利用所提出的方法，我们可以在显著减少脉冲的情况下获得可比的训练结果，从而产生节能的深SNN。
<details>	<summary>英文摘要</summary>	The tremendous energy consumption of deep neural networks (DNNs) has become a serious problem in deep learning. Spiking neural networks (SNNs), which mimic the operations in the human brain, have been studied as prominent energy-efficient neural networks. Due to their event-driven and spatiotemporally sparse operations, SNNs show possibilities for energy-efficient processing. To unlock their potential, deep SNNs have adopted temporal coding such as time-to-first-spike (TTFS)coding, which represents the information between neurons by the first spike time. With TTFS coding, each neuron generates one spike at most, which leads to a significant improvement in energy efficiency. Several studies have successfully introduced TTFS coding in deep SNNs, but they showed restricted efficiency improvement owing to the lack of consideration for efficiency during training. To address the aforementioned issue, this paper presents training methods for energy-efficient deep SNNs with TTFS coding. We introduce a surrogate DNN model to train the deep SNN in a feasible time and analyze the effect of the temporal kernel on training performance and efficiency. Based on the investigation, we propose stochastically relaxed activation and initial value-based regularization for the temporal kernel parameters. In addition, to reduce the number of spikes even further, we present temporal kernel-aware batch normalization. With the proposed methods, we could achieve comparable training results with significantly reduced spikes, which could lead to energy-efficient deep SNNs. </details>
<details>	<summary>邮件日期</summary>	2021年06月07日</details>

# 146、基于事件的脉冲神经网络光流自监督学习
- [ ] Self-Supervised Learning of Event-Based Optical Flow with Spiking Neural Networks 
时间：2021年06月03日                         第一作者：Federico Paredes-Vall\'es                       [链接](https://arxiv.org/abs/2106.01862).                     
## 摘要：神经形态传感和计算为高能量效率和高带宽的传感器处理带来了希望。神经形态计算面临的一个主要挑战是，传统人工神经网络（ANNs）的学习算法由于离散脉冲和更复杂的神经元动力学特性而不能直接转换为脉冲神经网络（SNNs）。因此，snn尚未成功地应用于复杂的大规模任务。本文主要研究基于事件的摄像机输入光流估计的自监督学习问题，并研究了最新的人工神经网络训练流水线所需的变化，以便成功地用SNNs解决这一问题。更具体地说，我们首先修改输入事件表示，用最少的显式时间信息编码更小的时间片。因此，我们使网络的神经元动力学和循环连接负责整合信息随着时间的推移。此外，我们重新构造了基于事件的光流的自监督损失函数，以改善其凸性。我们使用所提出的管道对各种类型的递归神经网络和snn进行了实验。关于SNNs，我们研究了参数初始化和优化、替代梯度形状和自适应神经元机制等因素的影响。我们发现初始化和替代梯度宽度在稀疏输入的学习中起着关键作用，而自适应性和可学习神经元参数的加入可以提高学习性能。结果表明，所提出的人工神经网络和神经网络的性能与目前最先进的自监督训练人工神经网络相当。
<details>	<summary>英文摘要</summary>	Neuromorphic sensing and computing hold a promise for highly energy-efficient and high-bandwidth-sensor processing. A major challenge for neuromorphic computing is that learning algorithms for traditional artificial neural networks (ANNs) do not transfer directly to spiking neural networks (SNNs) due to the discrete spikes and more complex neuronal dynamics. As a consequence, SNNs have not yet been successfully applied to complex, large-scale tasks. In this article, we focus on the self-supervised learning problem of optical flow estimation from event-based camera inputs, and investigate the changes that are necessary to the state-of-the-art ANN training pipeline in order to successfully tackle it with SNNs. More specifically, we first modify the input event representation to encode a much smaller time slice with minimal explicit temporal information. Consequently, we make the network's neuronal dynamics and recurrent connections responsible for integrating information over time. Moreover, we reformulate the self-supervised loss function for event-based optical flow to improve its convexity. We perform experiments with various types of recurrent ANNs and SNNs using the proposed pipeline. Concerning SNNs, we investigate the effects of elements such as parameter initialization and optimization, surrogate gradient shape, and adaptive neuronal mechanisms. We find that initialization and surrogate gradient width play a crucial part in enabling learning with sparse inputs, while the inclusion of adaptivity and learnable neuronal parameters can improve performance. We show that the performance of the proposed ANNs and SNNs are on par with that of the current state-of-the-art ANNs trained in a self-supervised manner. </details>
<details>	<summary>邮件日期</summary>	2021年06月04日</details>

# 145、可微点过程及其在脉冲神经网络中的应用
- [ ] A Differentiable Point Process with Its Application to Spiking Neural Networks 
时间：2021年06月03日                         第一作者：Hiroshi Kajino                       [链接](https://arxiv.org/abs/2106.00901).                     
<details>	<summary>注释</summary>	Accepted to ICML 2021 </details>
<details>	<summary>邮件日期</summary>	2021年06月04日</details>

# 144、可微点过程及其在脉冲神经网络中的应用
- [ ] A Differentiable Point Process with Its Application to Spiking Neural Networks 
时间：2021年06月02日                         第一作者：Hiroshi Kajino                       [链接](https://arxiv.org/abs/2106.00901).                     
## 摘要：本文研究了一种脉冲神经网络概率模型的学习算法。Jimenez-Rezende和Gerstner（2014）提出了一种随机变分推理算法来训练具有隐藏神经元的snn。该算法利用分数函数梯度估计更新变分分布，其高方差往往阻碍整个学习算法。提出了一种基于路径梯度估计的snn梯度估计方法。主要的技术难点是缺乏一种通用的方法来区分任意点过程的实现，这是导出路径梯度估计器所必需的。我们发展了一个可微点过程，这是本文的技术亮点，并将其应用于推导SNNs的路径梯度估计。通过数值模拟研究了梯度估计的有效性。
<details>	<summary>英文摘要</summary>	This paper is concerned about a learning algorithm for a probabilistic model of spiking neural networks (SNNs). Jimenez Rezende & Gerstner (2014) proposed a stochastic variational inference algorithm to train SNNs with hidden neurons. The algorithm updates the variational distribution using the score function gradient estimator, whose high variance often impedes the whole learning algorithm. This paper presents an alternative gradient estimator for SNNs based on the path-wise gradient estimator. The main technical difficulty is a lack of a general method to differentiate a realization of an arbitrary point process, which is necessary to derive the path-wise gradient estimator. We develop a differentiable point process, which is the technical highlight of this paper, and apply it to derive the path-wise gradient estimator for SNNs. We investigate the effectiveness of our gradient estimator through numerical simulation. </details>
<details>	<summary>注释</summary>	Accepted to ICML 2021 </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 143、通过信息瓶颈学习脉冲神经网络的时间译码
- [ ] Learning to Time-Decode in Spiking Neural Networks Through the Information Bottleneck 
时间：2021年06月02日                         第一作者：Nicolas Skatchkovsky                       [链接](https://arxiv.org/abs/2106.01177).                     
## 摘要：训练脉冲神经网络（SNNs）的一个关键挑战是，目标输出通常以自然信号的形式出现，例如用于分类的标签或用于生成模型的图像，并且需要编码成脉冲。这是通过手工制作目标脉冲信号来实现的，这又隐含地修复了用于将脉冲解码为自然信号的机制，例如速率解码。目标信号和解码规则的任意选择通常会削弱SNN在脉冲定时中编码和处理信息的能力。为了解决这一问题，本文提出了一种混合变分自动编码器结构，由编码SNN和译码人工神经网络（ANN）组成。解码神经网络的作用是学习如何将SNN输出的脉冲信号最佳地转换为目标自然信号。提出了一种新的端到端学习规则，通过代理梯度优化有向信息瓶颈训练准则。我们证明了该技术在各种任务的实验环境中的适用性，包括真实的数据集。
<details>	<summary>英文摘要</summary>	One of the key challenges in training Spiking Neural Networks (SNNs) is that target outputs typically come in the form of natural signals, such as labels for classification or images for generative models, and need to be encoded into spikes. This is done by handcrafting target spiking signals, which in turn implicitly fixes the mechanisms used to decode spikes into natural signals, e.g., rate decoding. The arbitrary choice of target signals and decoding rule generally impairs the capacity of the SNN to encode and process information in the timing of spikes. To address this problem, this work introduces a hybrid variational autoencoder architecture, consisting of an encoding SNN and a decoding Artificial Neural Network (ANN). The role of the decoding ANN is to learn how to best convert the spiking signals output by the SNN into the target natural signal. A novel end-to-end learning rule is introduced that optimizes a directed information bottleneck training criterion via surrogate gradients. We demonstrate the applicability of the technique in an experimental settings on various tasks, including real-life datasets. </details>
<details>	<summary>注释</summary>	Under review for conference publication </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 142、自下而上和自上而下的神经处理系统设计：自然智能和人工智能融合的神经形态智能
- [ ] Bottom-Up and Top-Down Neural Processing Systems Design: Neuromorphic Intelligence as the Convergence of Natural and Artificial Intelligence 
时间：2021年06月02日                         第一作者：Charlotte Frenkel                       [链接](https://arxiv.org/abs/2106.01288).                     
## 摘要：虽然摩尔定律推动了人们对计算能力的指数预期，但它的接近尾声呼唤着改善系统整体性能的新途径。这些途径之一是探索新的替代大脑启发的计算架构，承诺实现生物神经处理系统的灵活性和计算效率。在这一背景下，神经形态智能代表了一个范式的转变，在计算的基础上实现了脉冲神经网络结构紧密地协同定位处理和记忆。在本文中，我们提供了该领域的全面概述，强调了现有硅实现中存在的不同粒度级别，比较了旨在复制自然智能（自下而上）和旨在解决实际人工智能应用（自上而下）的方法，并评估用于实现这些目标的不同电路设计风格的好处。首先，我们介绍了模拟、混合信号和数字电路的设计风格，通过时间复用、内存计算和新型器件来确定处理和内存之间的边界。接下来，我们将重点介绍自底向上和自顶向下方法的关键权衡，调查它们的硅实现，并进行详细的比较分析以提取设计准则。最后，我们确定了实现神经形态边缘计算相对于传统机器学习加速器的竞争优势所需的必要协同作用和缺失元素，并概述了神经形态智能框架的关键元素。
<details>	<summary>英文摘要</summary>	While Moore's law has driven exponential computing power expectations, its nearing end calls for new avenues for improving the overall system performance. One of these avenues is the exploration of new alternative brain-inspired computing architectures that promise to achieve the flexibility and computational efficiency of biological neural processing systems. Within this context, neuromorphic intelligence represents a paradigm shift in computing based on the implementation of spiking neural network architectures tightly co-locating processing and memory. In this paper, we provide a comprehensive overview of the field, highlighting the different levels of granularity present in existing silicon implementations, comparing approaches that aim at replicating natural intelligence (bottom-up) versus those that aim at solving practical artificial intelligence applications (top-down), and assessing the benefits of the different circuit design styles used to achieve these goals. First, we present the analog, mixed-signal and digital circuit design styles, identifying the boundary between processing and memory through time multiplexing, in-memory computation and novel devices. Next, we highlight the key tradeoffs for each of the bottom-up and top-down approaches, survey their silicon implementations, and carry out detailed comparative analyses to extract design guidelines. Finally, we identify both necessary synergies and missing elements required to achieve a competitive advantage for neuromorphic edge computing over conventional machine-learning accelerators, and outline the key elements for a framework toward neuromorphic intelligence. </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 141、基于平衡脉冲神经网络的振动异常在线检测
- [ ] Online Detection of Vibration Anomalies Using Balanced Spiking Neural Networks 
时间：2021年06月01日                         第一作者：Nik Dennler                       [链接](https://arxiv.org/abs/2106.00687).                     
## 摘要：振动模式产生了关于运行机器健康状态的有价值的信息，这通常用于大型工业系统的预测性维护任务。然而，在尺寸、复杂性和功率预算方面，利用这些信息的经典方法所需的开销对于较小规模的应用（如自动汽车、无人机或机器人）通常是禁止的。在这里，我们提出了一种神经形态的方法来执行振动分析使用脉冲神经网络，可以应用于广泛的场景。我们提出了一种基于脉冲的端到端管道，能够使用与模拟-数字神经形态电路兼容的构建块从振动数据中检测系统异常。该管道以在线无监督方式运行，依赖于耳蜗模型、反馈自适应和平衡脉冲神经网络。我们证明了所提出的方法在两个公开的数据集上达到了最先进的性能或更好的性能。此外，我们还演示了在异步神经形态处理器设备上实现的工作概念证明。这项工作是朝着设计和实现用于在线振动监测的自主低功耗边缘计算设备迈出的重要一步。
<details>	<summary>英文摘要</summary>	Vibration patterns yield valuable information about the health state of a running machine, which is commonly exploited in predictive maintenance tasks for large industrial systems. However, the overhead, in terms of size, complexity and power budget, required by classical methods to exploit this information is often prohibitive for smaller-scale applications such as autonomous cars, drones or robotics. Here we propose a neuromorphic approach to perform vibration analysis using spiking neural networks that can be applied to a wide range of scenarios. We present a spike-based end-to-end pipeline able to detect system anomalies from vibration data, using building blocks that are compatible with analog-digital neuromorphic circuits. This pipeline operates in an online unsupervised fashion, and relies on a cochlea model, on feedback adaptation and on a balanced spiking neural network. We show that the proposed method achieves state-of-the-art performance or better against two publicly available data sets. Further, we demonstrate a working proof-of-concept implemented on an asynchronous neuromorphic processor device. This work represents a significant step towards the design and implementation of autonomous low-power edge-computing devices for online vibration monitoring. </details>
<details>	<summary>注释</summary>	This work is presented at the 2021 IEEE AICAS </details>
<details>	<summary>邮件日期</summary>	2021年06月03日</details>

# 140、基于事件的反向传播可以精确计算脉冲神经网络的梯度
- [ ] Event-Based Backpropagation can compute Exact Gradients for Spiking Neural Networks 
时间：2021年05月31日                         第一作者：Timo C. Wunderlich                       [链接](https://arxiv.org/abs/2009.08378).                     
<details>	<summary>邮件日期</summary>	2021年06月02日</details>

# 139、STDP训练的脉冲神经网络预处理对时空动作识别的影响研究
- [ ] A Study On the Effects of Pre-processing On Spatio-temporal Action Recognition Using Spiking Neural Networks Trained with STDP 
时间：2021年05月31日                         第一作者：El-Assal Mireille                        [链接](https://arxiv.org/abs/2105.14740).                     
## 摘要：近年来，脉冲神经网络受到越来越多的关注。snn被视为解决ann在模式识别中的瓶颈问题（如能源效率）的假设性解决方案。但是目前的方法，如ANN-to-SNN转换和反向传播等，并没有充分利用这些网络，无监督方法还没有达到与先进的人工神经网络相媲美的成功。研究非监督学习方法训练的snn在视频分类任务中的行为非常重要，例如脉冲时间依赖可塑性（STDP），包括利用脉冲模拟运动信息的机制，因为这些信息对于视频理解至关重要。本文提出了多种方法将时间信息转换成静态格式，然后使用延迟编码将视觉信息转换成脉冲。这些方法与早期和晚期两种时间融合方法相结合，用于帮助脉冲神经网络捕获视频中的时空特征。本文利用STDP训练的卷积脉冲神经网络的网络结构，测试了该网络在动作识别任务中的性能。了解脉冲神经网络如何响应不同的运动提取和表示方法，有助于缩小snn和ann之间的性能差距。本文利用脉冲神经网络研究了动作形状和速度的相似性对动作识别的影响，并与其它方法进行了比较。
<details>	<summary>英文摘要</summary>	There has been an increasing interest in spiking neural networks in recent years. SNNs are seen as hypothetical solutions for the bottlenecks of ANNs in pattern recognition, such as energy efficiency. But current methods such as ANN-to-SNN conversion and back-propagation do not take full advantage of these networks, and unsupervised methods have not yet reached a success comparable to advanced artificial neural networks. It is important to study the behavior of SNNs trained with unsupervised learning methods such as spike-timing dependent plasticity (STDP) on video classification tasks, including mechanisms to model motion information using spikes, as this information is critical for video understanding. This paper presents multiple methods of transposing temporal information into a static format, and then transforming the visual information into spikes using latency coding. These methods are paired with two types of temporal fusion known as early and late fusion, and are used to help the spiking neural network in capturing the spatio-temporal features from videos. In this paper, we rely on the network architecture of a convolutional spiking neural network trained with STDP, and we test the performance of this network when challenged with action recognition tasks. Understanding how a spiking neural network responds to different methods of movement extraction and representation can help reduce the performance gap between SNNs and ANNs. In this paper we show the effect of the similarity in the shape and speed of certain actions on action recognition with spiking neural networks, we also highlight the effectiveness of some methods compared to others. </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 138、基于脉冲神经网络的硅视网膜生物视觉注意模式分类
- [ ] Bio-inspired visual attention for silicon retinas based on spiking neural networks applied to pattern classification 
时间：2021年05月31日                         第一作者：Am\'elie Gruel                        [链接](https://arxiv.org/abs/2105.14753).                     
## 摘要：视觉注意可以定义为一种行为和认知过程，即有选择地集中在感官线索的一个离散方面，而忽略其他可感知的信息。这种生物机制，特别是显著性检测，长期以来被用于多媒体索引中，只对图像或视频的相关部分进行分析，以便进一步处理。最近出现的硅视网膜（或称事件摄像机——测量亮度像素级变化并相应输出异步事件的传感器）提出了一个问题，即如何使注意力和显著性适应这种传感器的非常规输出类型。硅视网膜旨在重现视网膜的生物学行为。在这方面，它们在时间上产生准时的事件，这些事件可以被解释为神经脉冲，并被神经网络解释为神经脉冲。特别是，脉冲神经网络（Spiking Neural Networks，SNNs）代表了一种比传统人工网络更接近生物学的异步型人工神经网络，主要是因为它们试图模拟神经膜和动作电位随时间的动态变化。snn以脉冲序列的形式接收和处理信息。因此，它们为硅视网膜测量的传入事件模式的有效处理和分类提供了合适的候选者。在这篇论文中，我们回顾了注意机制背后的生物学背景，并介绍了一个案例研究事件视频分类与SNNs，使用一个基于生物学的低水平计算注意机制，并有有趣的初步结果。
<details>	<summary>英文摘要</summary>	Visual attention can be defined as the behavioral and cognitive process of selectively focusing on a discrete aspect of sensory cues while disregarding other perceivable information. This biological mechanism, more specifically saliency detection, has long been used in multimedia indexing to drive the analysis only on relevant parts of images or videos for further processing. The recent advent of silicon retinas (or event cameras -- sensors that measure pixel-wise changes in brightness and output asynchronous events accordingly) raises the question of how to adapt attention and saliency to the unconventional type of such sensors' output. Silicon retina aims to reproduce the biological retina behaviour. In that respect, they produce punctual events in time that can be construed as neural spikes and interpreted as such by a neural network. In particular, Spiking Neural Networks (SNNs) represent an asynchronous type of artificial neural network closer to biology than traditional artificial networks, mainly because they seek to mimic the dynamics of neural membrane and action potentials over time. SNNs receive and process information in the form of spike trains. Therefore, they make for a suitable candidate for the efficient processing and classification of incoming event patterns measured by silicon retinas. In this paper, we review the biological background behind the attentional mechanism, and introduce a case study of event videos classification with SNNs, using a biology-grounded low-level computational attention mechanism, with interesting preliminary results. </details>
<details>	<summary>注释</summary>	6 pages, 3 figures. To be published in Content-Based Multimedia Indexing (CBMI) 2021, Lille, France. This work was supported by the European Union's ERA-NET CHIST-ERA 2018 research and innovation programme under grant agreement ANR-19-CHR3-0008 </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 137、脉冲时变塑性训练脉冲神经网络的泛化特征
- [ ] Characterization of Generalizability of Spike Time Dependent Plasticity trained Spiking Neural Networks 
时间：2021年05月31日                         第一作者：Biswadeep Chakraborty                       [链接](https://arxiv.org/abs/2105.14677).                     
## 摘要：脉冲时间依赖可塑性（STDP）训练的脉冲神经网络（SNN）是一种神经启发的无监督学习方法，适用于各种机器学习应用。本文利用学习算法轨迹的Hausdorff维数研究了STDP学习过程的概化性质。本文分析了STDP学习模型和相关超参数对SNN可概化性质的影响，刻画了SNN的可概化性与可学习性的权衡。该分析被用来开发一种贝叶斯优化方法来优化STDP模型的超参数，以提高SNN的泛化性能。
<details>	<summary>英文摘要</summary>	A Spiking Neural Network (SNN) trained with Spike Time Dependent Plasticity (STDP) is a neuro-inspired unsupervised learning method for various machine learning applications. This paper studies the generalizability properties of the STDP learning processes using the Hausdorff dimension of the trajectories of the learning algorithm. The paper analyzes the effects of STDP learning models and associated hyper-parameters on the generalizability properties of an SNN and characterizes the generalizability vs learnability trade-off in an SNN. The analysis is used to develop a Bayesian optimization approach to optimize the hyper-parameters for an STDP model to improve the generalizability properties of an SNN. </details>
<details>	<summary>注释</summary>	15 pages, submitted to Frontiers in Neuroscience. arXiv admin note: text overlap with arXiv:2010.08195, arXiv:2006.09313 by other authors </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 136、利用生物似然奖赏传播调整卷积脉冲神经网络
- [ ] Tuning Convolutional Spiking Neural Network with Biologically-plausible Reward Propagation 
时间：2021年05月31日                         第一作者：Tielin Zhang                        [链接](https://arxiv.org/abs/2010.04434).                     
<details>	<summary>注释</summary>	Final Version. Accepted by IEEE Transactions on Neural Networks and Learning Systems </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 135、在脉冲卷积神经网络中实现一个中心凹激发滤波器的初步研究
- [ ] Implementing a foveal-pit inspired filter in a Spiking Convolutional Neural Network: a preliminary study 
时间：2021年05月29日                         第一作者：Shriya T.P. Gupta                       [链接](https://arxiv.org/abs/2105.14326).                     
## 摘要：我们提出了一个脉冲卷积神经网络（SCNN），它结合了视网膜中央凹的高斯滤波器和秩序编码的启发差异。该模型是用一种适应于脉冲神经元的反向传播算法来训练的，如在Nengo库中实现的那样。我们已经在两个公开的数据集上评估了我们的模型的性能-一个用于数字识别任务，另一个用于车辆识别任务。该网络已达到90%的精度，其中损失是计算使用交叉熵函数。这是一个提高了57%左右的准确率获得的替代方法执行分类没有任何类型的神经滤波。总的来说，我们的概念验证研究表明，在现有的SCNN结构中引入生物学上合理的滤波可以很好地处理有噪声的输入图像，例如在我们的车辆识别任务中。基于我们的研究结果，我们计划通过在秩排序之前集成基于侧抑制的冗余约简来增强SCNN，这将进一步提高网络的分类精度。
<details>	<summary>英文摘要</summary>	We have presented a Spiking Convolutional Neural Network (SCNN) that incorporates retinal foveal-pit inspired Difference of Gaussian filters and rank-order encoding. The model is trained using a variant of the backpropagation algorithm adapted to work with spiking neurons, as implemented in the Nengo library. We have evaluated the performance of our model on two publicly available datasets - one for digit recognition task, and the other for vehicle recognition task. The network has achieved up to 90% accuracy, where loss is calculated using the cross-entropy function. This is an improvement over around 57% accuracy obtained with the alternate approach of performing the classification without any kind of neural filtering. Overall, our proof-of-concept study indicates that introducing biologically plausible filtering in existing SCNN architecture will work well with noisy input images such as those in our vehicle recognition task. Based on our results, we plan to enhance our SCNN by integrating lateral inhibition-based redundancy reduction prior to rank-ordering, which will further improve the classification accuracy by the network. </details>
<details>	<summary>注释</summary>	8 pages, 8 figures, 4 tables. 2020 International Joint Conference on Neural Networks (IJCNN) ACM-class: I.2.10; I.4.5; I.4.10 DOI: 10.1109/IJCNN48605.2020.9207612 </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 134、DVS脉冲反应的中心凹激发滤波
- [ ] Foveal-pit inspired filtering of DVS spike response 
时间：2021年05月29日                         第一作者：Shriya T.P. Gupta                       [链接](https://arxiv.org/abs/2105.14331).                     
## 摘要：在本文中，我们提出了处理动态视觉传感器（DVS）记录的视觉模式与视网膜模型的基础上，中心凹坑启发差异高斯（狗）滤波器。用不同空间频率的垂直白条和黑条以恒定速度水平移动来刺激DVS传感器。由DVS传感器产生的输出脉冲被作为输入应用到一组狗过滤器，其灵感来自灵长类视觉通路的感受野结构。特别是，这些滤光片模拟了侏儒和副交感神经节细胞（视网膜的脉冲神经元）的感受野，这些细胞为中央凹的光受体服务。用中心凹模型提取的特征被用于进一步的分类，使用了一个适应于脉冲神经网络的反向传播变量训练的脉冲卷积神经网络。
<details>	<summary>英文摘要</summary>	In this paper, we present results of processing Dynamic Vision Sensor (DVS) recordings of visual patterns with a retinal model based on foveal-pit inspired Difference of Gaussian (DoG) filters. A DVS sensor was stimulated with varying number of vertical white and black bars of different spatial frequencies moving horizontally at a constant velocity. The output spikes generated by the DVS sensor were applied as input to a set of DoG filters inspired by the receptive field structure of the primate visual pathway. In particular, these filters mimic the receptive fields of the midget and parasol ganglion cells (spiking neurons of the retina) that sub-serve the photo-receptors of the foveal-pit. The features extracted with the foveal-pit model are used for further classification using a spiking convolutional neural network trained with a backpropagation variant adapted for spiking neural networks. </details>
<details>	<summary>注释</summary>	6 pages, 4 figures, 2 tables. 2021 55th Annual Conference on Information Sciences and Systems (CISS), 2021 ACM-class: I.2.10; I.4.5; I.4.10 DOI: 10.1109/CISS50987.2021.9400245 </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 133、基于突触级强化学习的无梯度神经网络训练
- [ ] Gradient-Free Neural Network Training via Synaptic-Level Reinforcement Learning 
时间：2021年05月29日                         第一作者：Aman Bhargava                       [链接](https://arxiv.org/abs/2105.14383).                     
## 摘要：神经信息处理中的一个持续挑战是：神经元如何调整它们的连接性以随着时间的推移提高任务绩效（即实现学习）？人们普遍认为，在实现学习的特定大脑区域有一个一致的、突触水平的学习机制。然而，这一机制的确切性质仍不清楚。本文提出了一种基于强化学习（RL）的多层感知器（MLP）模型的简单突触级学习策略。在该算法中，每个MLP突触的动作空间由对突触权重的微小增加、减少或零动作组成，每个突触的状态由最后两个动作和奖赏信号组成。二元奖励信号表示任务绩效的改善或恶化。相对于自适应策略，静态策略产生更好的训练效果，并且与激活函数、网络形状和任务无关。训练mlp产生的字符识别性能可与梯度下降训练的同形网络相媲美。0隐单位字符识别测试的平均验证准确率为88.28%，比用梯度下降法训练的同一MLP高1.86$\pm$0.47%。32个隐单位字符识别测试的平均验证准确率为88.45%，比用梯度下降法训练的同一MLP低1.11$\pm$0.79%。鲁棒性和对梯度计算的不依赖性为训练难以区分的人工神经网络（如脉冲神经网络（SNNs）和递归神经网络（RNNs））的新技术打开了大门。此外，该方法的简单性为进一步开发类似于元胞自动机的机器智能局部规则驱动的多代理连接模型提供了独特的机会。
<details>	<summary>英文摘要</summary>	An ongoing challenge in neural information processing is: how do neurons adjust their connectivity to improve task performance over time (i.e., actualize learning)? It is widely believed that there is a consistent, synaptic-level learning mechanism in specific brain regions that actualizes learning. However, the exact nature of this mechanism remains unclear. Here we propose an algorithm based on reinforcement learning (RL) to generate and apply a simple synaptic-level learning policy for multi-layer perceptron (MLP) models. In this algorithm, the action space for each MLP synapse consists of a small increase, decrease, or null action on the synapse weight, and the state for each synapse consists of the last two actions and reward signals. A binary reward signal indicates improvement or deterioration in task performance. The static policy produces superior training relative to the adaptive policy and is agnostic to activation function, network shape, and task. Trained MLPs yield character recognition performance comparable to identically shaped networks trained with gradient descent. 0 hidden unit character recognition tests yielded an average validation accuracy of 88.28%, 1.86$\pm$0.47% higher than the same MLP trained with gradient descent. 32 hidden unit character recognition tests yielded an average validation accuracy of 88.45%, 1.11$\pm$0.79% lower than the same MLP trained with gradient descent. The robustness and lack of reliance on gradient computations opens the door for new techniques for training difficult-to-differentiate artificial neural networks such as spiking neural networks (SNNs) and recurrent neural networks (RNNs). Further, the method's simplicity provides a unique opportunity for further development of local rule-driven multi-agent connectionist models for machine intelligence analogous to cellular automata. </details>
<details>	<summary>注释</summary>	10 pages, 3 figures, submitted to NeurIPS 2021 MSC-class: 68T07 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年06月01日</details>

# 132、BSNN：实现人工神经网络向双稳态神经元脉冲神经网络的更快更好的转换
- [ ] BSNN: Towards Faster and Better Conversion of Artificial Neural Networks to Spiking Neural Networks with Bistable Neurons 
时间：2021年05月27日                         第一作者：Yang Li                       [链接](https://arxiv.org/abs/2105.12917).                     
## 摘要：脉冲神经网络（SNN）通过离散的二进制事件来计算和传递信息。在新兴的神经形态硬件中，它被认为比人工神经网络（ANN）在生物学上更合理、更节能。然而，由于SNN的不连续性和不可微性，训练SNN是一项相对具有挑战性的任务。最近的工作通过将ANN转换为SNN，在获得优异性能方面取得了重要进展。由于信息处理的差异，转换后的深度SNN通常会遭受严重的性能损失和较大的时延。本文分析了性能下降的原因，提出了一种新的双稳态脉冲神经网络（BSNN），解决了由相位超前和相位滞后引起的失活神经元脉冲问题。同时，基于ResNet结构的神经网络在转换时，由于快捷路径的快速传递，输出神经元的信息是不完整的。我们设计了同步神经元（SN）来帮助有效地提高性能。实验结果表明，该方法仅需1/4-1/10的时间步长即可实现几乎无损的转换。我们在具有挑战性的数据集（包括CIFAR-10（95.16%top-1）、CIFAR-100（78.12%top-1）和ImageNet（72.64%top-1））上演示了VGG16、ResNet20和ResNet34最先进的ANN-SNN转换。
<details>	<summary>英文摘要</summary>	The spiking neural network (SNN) computes and communicates information through discrete binary events. It is considered more biologically plausible and more energy-efficient than artificial neural networks (ANN) in emerging neuromorphic hardware. However, due to the discontinuous and non-differentiable characteristics, training SNN is a relatively challenging task. Recent work has achieved essential progress on an excellent performance by converting ANN to SNN. Due to the difference in information processing, the converted deep SNN usually suffers serious performance loss and large time delay. In this paper, we analyze the reasons for the performance loss and propose a novel bistable spiking neural network (BSNN) that addresses the problem of spikes of inactivated neurons (SIN) caused by the phase lead and phase lag. Also, when ResNet structure-based ANNs are converted, the information of output neurons is incomplete due to the rapid transmission of the shortcut path. We design synchronous neurons (SN) to help efficiently improve performance. Experimental results show that the proposed method only needs 1/4-1/10 of the time steps compared to previous work to achieve nearly lossless conversion. We demonstrate state-of-the-art ANN-SNN conversion for VGG16, ResNet20, and ResNet34 on challenging datasets including CIFAR-10 (95.16% top-1), CIFAR-100 (78.12% top-1), and ImageNet (72.64% top-1). </details>
<details>	<summary>邮件日期</summary>	2021年05月28日</details>

# 131、BackEISNN：一种具有自适应自反馈和平衡兴奋抑制神经元的深脉冲神经网络
- [ ] BackEISNN: A Deep Spiking Neural Network with Adaptive Self-Feedback and Balanced Excitatory-Inhibitory Neurons 
时间：2021年05月27日                         第一作者：Dongcheng Zhao                       [链接](https://arxiv.org/abs/2105.13004).                     
## 摘要：脉冲神经网络（SNNs）通过离散脉冲传递信息，在处理时空信息方面有很好的表现。由于snn的不可微性，设计性能良好的snn仍然存在困难。最近，由于梯度近似的提出，用反向传播训练的snn表现出了优越的性能。然而，对于复杂任务的处理性能，离深度神经网络还有很大的距离。我们从大脑中连接有自反馈连接的棘波神经元的自陷性中得到启发，在膜电位上应用自适应延时自反馈来调节棘波的精确度。同时，我们运用平衡的兴奋性和抑制性神经元机制来动态控制脉冲神经元的输出。结合这两种机制，我们提出了一种具有自适应自反馈和平衡兴奋和抑制神经元的深脉冲神经网络（BackEISNN）。在多个标准数据集上的实验结果表明，这两个模块不仅加快了网络的收敛速度，而且提高了精度。对于MNIST、FashionMNIST和N-MNIST数据集，我们的模型实现了最先进的性能。对于CIFAR10数据集，我们的BackEISNN在相对较轻的结构上也获得了显著的性能，与最先进的snn竞争。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) transmit information through discrete spikes, which performs well in processing spatial-temporal information. Due to the non-differentiable characteristic, there still exist difficulties in designing well-performed SNNs. Recently, SNNs trained with backpropagation have shown superior performance due to the proposal of the gradient approximation. However, the performance on complex tasks is still far away from the deep neural networks. Taking inspiration from the autapse in the brain which connects the spiking neurons with a self-feedback connection, we apply an adaptive time-delayed self-feedback on the membrane potential to regulate the spike precisions. As well as, we apply the balanced excitatory and inhibitory neurons mechanism to control the spiking neurons' output dynamically. With the combination of the two mechanisms, we propose a deep spiking neural network with adaptive self-feedback and balanced excitatory and inhibitory neurons (BackEISNN). The experimental results on several standard datasets have shown that the two modules not only accelerate the convergence of the network but also improve the accuracy. For the MNIST, FashionMNIST, and N-MNIST datasets, our model has achieved state-of-the-art performance. For the CIFAR10 dataset, our BackEISNN also gets remarkable performance on a relatively light structure that competes against state-of-the-art SNNs. </details>
<details>	<summary>邮件日期</summary>	2021年05月28日</details>

# 130、基于时序神经网络的在线学习微体系结构实现框架
- [ ] A Microarchitecture Implementation Framework for Online Learning with Temporal Neural Networks 
时间：2021年05月27日                         第一作者：Harideep Nair                       [链接](https://arxiv.org/abs/2105.13262).                     
## 摘要：时间神经网络（TNNs）是一种利用时间作为资源来表示和处理信息的脉冲神经网络，类似于哺乳动物的大脑皮层。与采用单独训练和推理阶段的计算密集型深度神经网络相比，TNNs能够非常有效地进行在线增量/连续学习，是构建边缘本地感官处理单元的优秀候选。本文提出了一个用标准CMOS实现TNNs的微体系结构框架。给出了三个关键模块的门级实现：1）多突触神经元，2）多神经元列，3）基于脉冲时间依赖可塑性（STDP）的无监督和有监督在线学习算法。TNN微体系结构体现在一组特征标度方程中，用于评估任何TNN设计的门计数、面积、延迟和功耗。在45nmcmos中给出了设计的后合成结果，并证明了其在线增量学习能力。
<details>	<summary>英文摘要</summary>	Temporal Neural Networks (TNNs) are spiking neural networks that use time as a resource to represent and process information, similar to the mammalian neocortex. In contrast to compute-intensive Deep Neural Networks that employ separate training and inference phases, TNNs are capable of extremely efficient online incremental/continuous learning and are excellent candidates for building edge-native sensory processing units. This work proposes a microarchitecture framework for implementing TNNs using standard CMOS. Gate-level implementations of three key building blocks are presented: 1) multi-synapse neurons, 2) multi-neuron columns, and 3) unsupervised and supervised online learning algorithms based on Spike Timing Dependent Plasticity (STDP). The TNN microarchitecture is embodied in a set of characteristic scaling equations for assessing the gate count, area, delay and power consumption for any TNN design. Post-synthesis results (in 45nm CMOS) for the proposed designs are presented, and their online incremental learning capability is demonstrated. </details>
<details>	<summary>注释</summary>	To be published in ISVLSI 2021. arXiv admin note: substantial text overlap with arXiv:2009.00457 </details>
<details>	<summary>邮件日期</summary>	2021年05月28日</details>

# 129、用于深脉冲神经网络快速准确推理的最佳ANN-SNN转换
- [ ] Optimal ANN-SNN Conversion for Fast and Accurate Inference in Deep Spiking Neural Networks 
时间：2021年05月25日                         第一作者：Jianhao Ding                       [链接](https://arxiv.org/abs/2105.11654).                     
## 摘要：脉冲神经网络（Spiking Neural Networks，SNNs）作为一种受生物启发的节能神经网络，受到了研究者和工业界的广泛关注。训练深层SNN最有效的方法是通过ANN-SNN转换。然而，这种转换通常存在精度损失和推理时间长的问题，阻碍了SNN的实际应用。本文从理论上分析了ANN-SNN变换，给出了最佳变换的充分条件。为了更好地关联ANN-SNN并获得更高的准确度，我们提出了速率范数层来代替源ANN训练中的ReLU激活函数，实现了从训练的ANN到SNN的直接转换。此外，我们提出了一个最佳拟合曲线来量化源神经网络的激活值与目标SNN的实际发射率之间的拟合。结果表明，通过优化修正后的神经网络拟合曲线的上界，可以缩短推理时间，实现快速推理。我们的理论可以解释现有的快速推理工作，得到更好的结果。实验结果表明，该方法在VGG-16、PreActResNet-18和较深的结构上实现了近无损耗的转换。在典型方法能耗为0.265倍的情况下，推理速度提高了8.6倍。代码可在https://github.com/DingJianhao/OptSNNConvertion-RNL-RIL.
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs), as bio-inspired energy-efficient neural networks, have attracted great attentions from researchers and industry. The most efficient way to train deep SNNs is through ANN-SNN conversion. However, the conversion usually suffers from accuracy loss and long inference time, which impede the practical application of SNN. In this paper, we theoretically analyze ANN-SNN conversion and derive sufficient conditions of the optimal conversion. To better correlate ANN-SNN and get greater accuracy, we propose Rate Norm Layer to replace the ReLU activation function in source ANN training, enabling direct conversion from a trained ANN to an SNN. Moreover, we propose an optimal fit curve to quantify the fit between the activation value of source ANN and the actual firing rate of target SNN. We show that the inference time can be reduced by optimizing the upper bound of the fit curve in the revised ANN to achieve fast inference. Our theory can explain the existing work on fast reasoning and get better results. The experimental results show that the proposed method achieves near loss less conversion with VGG-16, PreActResNet-18, and deeper structures. Moreover, it can reach 8.6x faster reasoning performance under 0.265x energy consumption of the typical method. The code is available at https://github.com/DingJianhao/OptSNNConvertion-RNL-RIL. </details>
<details>	<summary>注释</summary>	9 pages, 7 figures, 2 tables. To appear in the 30th International Joint Conference on Artificial Intelligence (IJCAI 2021) </details>
<details>	<summary>邮件日期</summary>	2021年05月26日</details>

# 128、深度脉冲神经网络的梯度重排剪枝
- [ ] Pruning of Deep Spiking Neural Networks through Gradient Rewiring 
时间：2021年05月20日                         第一作者：Yanqi Chen                       [链接](https://arxiv.org/abs/2105.04916).                     
<details>	<summary>注释</summary>	9 pages, 7 figures, 4 tables. To appear in the 30th International Joint Conference on Artificial Intelligence (IJCAI 2021) </details>
<details>	<summary>邮件日期</summary>	2021年05月21日</details>

# 127、结合反向传播和STDP的半监督学习：STDP通过反向传播在脉冲神经网络中使用少量的标记数据来增强学习
- [ ] Semi-supervised learning combining backpropagation and STDP: STDP enhances learning by backpropagation with a small amount of labeled data in a spiking neural network 
时间：2021年05月19日                         第一作者：Kotaro Furuya                        [链接](https://arxiv.org/abs/2102.10530).                     
<details>	<summary>注释</summary>	9 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 126、稀疏脉冲梯度下降
- [ ] Sparse Spiking Gradient Descent 
时间：2021年05月18日                         第一作者：Nicolas Perez-Nieves                        [链接](https://arxiv.org/abs/2105.08810).                     
## 摘要：在神经形态计算设备上模拟脉冲神经网络（Spiking Neural Networks，SNNs）由于其能耗低而受到越来越多的关注。最近的进展使得训练snn在精确度方面开始与传统的人工神经网络（ANNs）竞争，同时在神经形态硬件上运行时具有能量效率。然而，训练snn的过程仍然是基于最初为ann开发的稠密张量运算，而ann没有利用snn的时空稀疏性。我们在这里提出了第一个稀疏SNN反向传播算法，该算法实现了与当前最先进的方法相同或更好的精度，同时显著提高了速度和内存效率。我们在不同复杂度的真实数据集（时尚MNIST、神经性MNIST和海德堡数字脉冲）上展示了我们的方法的有效性，实现了高达70倍的向后传递加速，并且在不损失准确度的情况下提高了40%的内存效率。
<details>	<summary>英文摘要</summary>	There is an increasing interest in emulating Spiking Neural Networks (SNNs) on neuromorphic computing devices due to their low energy consumption. Recent advances have allowed training SNNs to a point where they start to compete with traditional Artificial Neural Networks (ANNs) in terms of accuracy, while at the same time being energy efficient when run on neuromorphic hardware. However, the process of training SNNs is still based on dense tensor operations originally developed for ANNs which do not leverage the spatiotemporally sparse nature of SNNs. We present here the first sparse SNN backpropagation algorithm which achieves the same or better accuracy as current state of the art methods while being significantly faster and more memory efficient. We show the effectiveness of our method on real datasets of varying complexity (Fashion-MNIST, Neuromophic-MNIST and Spiking Heidelberg Digits) achieving a speedup in the backward pass of up to 70x, and 40% more memory efficient, without losing accuracy. </details>
<details>	<summary>邮件日期</summary>	2021年05月20日</details>

# 125、PLSM：一种用于无意行为检测的并行化液态机
- [ ] PLSM: A Parallelized Liquid State Machine for Unintentional Action Detection 
时间：2021年05月06日                         第一作者：Dipayan Das                       [链接](https://arxiv.org/abs/2105.09909).                     
## 摘要：水库计算（RC）为在低端嵌入式系统平台上部署人工智能算法提供了一个可行的选择。液体状态机（LSM）是一种仿生RC模型，它模拟大脑皮层微电路，使用可直接在神经形态硬件上实现的脉冲神经网络（SNN）。在本文中，我们提出了一种新的并行LSM（PLSM）架构，它结合了时空读出层和模型输出的语义约束。据我们所知，这样一个公式在文献中还是第一次，它提供了一个比传统的深度学习模型计算量更轻的替代方案。此外，我们还提出了一个完整的算法来实现与GPU兼容的可并行snn和lsm。利用Oops数据集实现了PLSM模型对无意/意外视频片段进行分类。从视频中无意行为的检测实验结果可以看出，本文提出的模型优于自监督模型和完全监督的传统深度学习模型。所有实现的代码都可以在我们的存储库中找到https://github.com/anonymoussentience2020/Parallelized_LSM_for_Unintentional_Action_Recognition.
<details>	<summary>英文摘要</summary>	Reservoir Computing (RC) offers a viable option to deploy AI algorithms on low-end embedded system platforms. Liquid State Machine (LSM) is a bio-inspired RC model that mimics the cortical microcircuits and uses spiking neural networks (SNN) that can be directly realized on neuromorphic hardware. In this paper, we present a novel Parallelized LSM (PLSM) architecture that incorporates spatio-temporal read-out layer and semantic constraints on model output. To the best of our knowledge, such a formulation has been done for the first time in literature, and it offers a computationally lighter alternative to traditional deep-learning models. Additionally, we also present a comprehensive algorithm for the implementation of parallelizable SNNs and LSMs that are GPU-compatible. We implement the PLSM model to classify unintentional/accidental video clips, using the Oops dataset. From the experimental results on detecting unintentional action in video, it can be observed that our proposed model outperforms a self-supervised model and a fully supervised traditional deep learning model. All the implemented codes can be found at our repository https://github.com/anonymoussentience2020/Parallelized_LSM_for_Unintentional_Action_Recognition. </details>
<details>	<summary>邮件日期</summary>	2021年05月21日</details>

# 124、具有第一脉冲时间的快速节能神经形态深度学习
- [ ] Fast and energy-efficient neuromorphic deep learning with first-spike times 
时间：2021年05月17日                         第一作者：Julian G\"oltz                       [链接](https://arxiv.org/abs/1912.11443).                     
<details>	<summary>注释</summary>	24 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 123、SpikE：基于SpikE的多关系图数据嵌入
- [ ] SpikE: spike-based embeddings for multi-relational graph data 
时间：2021年05月17日                         第一作者：Dominik Dold                       [链接](https://arxiv.org/abs/2104.13398).                     
<details>	<summary>注释</summary>	Accepted for publication at IJCNN 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月18日</details>

# 122、基于遗传算法的皮层脉冲神经网络多目标优化
- [ ] Multi-Objective Optimisation of Cortical Spiking Neural Networks With Genetic Algorithms 
时间：2021年05月14日                         第一作者：James Fitzgerald                        [链接](https://arxiv.org/abs/2105.06824).                     
## 摘要：脉冲神经网络（SNNs）通过神经元的全部或无脉冲活动进行通信。然而，在生物实验中，将大量SNN模型参数与观察到的神经活动模式相匹配仍然是一个挑战。以前使用遗传算法（GA）优化特定有效SNN模型的工作，使用Izhikevich神经元模型，仅限于单个参数和目标。这项工作应用了一种称为非支配排序遗传算法（NSGA-III）的遗传算法，以证明对同一SNN进行多目标优化的可行性，重点是搜索网络连接参数，以实现兴奋性和抑制性神经元类型的目标放电率，包括跨不同网络连接的稀疏性。我们表明，NSGA-III可以很容易地优化各种发射率。值得注意的是，当兴奋性神经放电率高于或等于抑制性神经元时，误差很小。此外，当连接稀疏性作为优化参数时，最优解需要稀疏的网络连接。我们还发现，对于兴奋性神经元的放电率低于抑制性神经元，误差通常较大。总的来说，我们成功地证明了对递归和稀疏SNN网络参数进行多目标遗传优化的可行性。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) communicate through the all-or-none spiking activity of neurons. However, fitting the large number of SNN model parameters to observed neural activity patterns, for example, in biological experiments, remains a challenge. Previous work using genetic algorithm (GA) optimisation on a specific efficient SNN model, using the Izhikevich neuronal model, was limited to a single parameter and objective. This work applied a version of GA, called non-dominated sorting GA (NSGA-III), to demonstrate the feasibility of performing multi-objective optimisation on the same SNN, focusing on searching network connectivity parameters to achieve target firing rates of excitatory and inhibitory neuronal types, including across different network connectivity sparsity. We showed that NSGA-III could readily optimise for various firing rates. Notably, when the excitatory neural firing rates were higher than or equal to that of inhibitory neurons, the errors were small. Moreover, when connectivity sparsity was considered as a parameter to be optimised, the optimal solutions required sparse network connectivity. We also found that for excitatory neural firing rates lower than that of inhibitory neurons, the errors were generally larger. Overall, we have successfully demonstrated the feasibility of implementing multi-objective GA optimisation on network parameters of recurrent and sparse SNN. </details>
<details>	<summary>注释</summary>	In: 32nd Irish Signals and Systems Conference (ISSC) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月17日</details>

# 121、基于基数编码的高效脉冲神经网络
- [ ] Efficient Spiking Neural Networks with Radix Encoding 
时间：2021年05月14日                         第一作者：Zhehui Wang                       [链接](https://arxiv.org/abs/2105.06943).                     
## 摘要：与传统的人工神经网络相比，脉冲神经网络（SNNs）由于其事件驱动的计算机制和用加法代替能耗加权乘法，在延迟和能量效率方面具有优势。然而，为了达到神经网络的精度，通常需要长脉冲序列来保证精度。传统上，一个脉冲序列需要大约一千个时间步才能达到与人工神经网络相似的精度。这抵消了snn带来的计算效率，因为更长的脉冲序列意味着更多的操作和更长的延迟。本文提出了一种具有超短脉冲序列的基数编码SNN。在新模型中，脉冲列车只需不到10个时间步。实验结果表明，与VGG-16网络体系结构和CIFAR-10数据集的最新研究成果相比，该方法具有25倍的加速比和1.1%的精度提高。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have advantages in latency and energy efficiency over traditional artificial neural networks (ANNs) due to its event-driven computation mechanism and replacement of energy-consuming weight multiplications with additions. However, in order to reach accuracy of its ANN counterpart, it usually requires long spike trains to ensure the accuracy. Traditionally, a spike train needs around one thousand time steps to approach similar accuracy as its ANN counterpart. This offsets the computation efficiency brought by SNNs because longer spike trains mean a larger number of operations and longer latency. In this paper, we propose a radix encoded SNN with ultra-short spike trains. In the new model, the spike train takes less than ten time steps. Experiments show that our method demonstrates 25X speedup and 1.1% increment on accuracy, compared with the state-of-the-art work on VGG-16 network architecture and CIFAR-10 dataset. </details>
<details>	<summary>邮件日期</summary>	2021年05月17日</details>

# 120、SpikeMS：用于运动分割的深脉冲神经网络
- [ ] SpikeMS: Deep Spiking Neural Network for Motion Segmentation 
时间：2021年05月13日                         第一作者：Chethan M. Parameshwara                       [链接](https://arxiv.org/abs/2105.06562).                     
## 摘要：脉冲神经网络（SNN）是所谓的第三代神经网络，它试图更紧密地匹配生物大脑的功能。它们固有地对时间数据进行编码，允许以较少的能量使用进行训练，并且当在神经形态硬件上编码时，可以非常节能。此外，它们非常适合于涉及基于事件的传感器的任务，这与SNN基于事件的特性相匹配。然而，由于算法和训练的复杂性，snn还没有像标准人工神经网络（ANNs）那样有效地应用于实际的大规模任务中。为了使情况进一步恶化，输入表示是非常规的，需要仔细分析和深入理解。在本文中，我们提出了第一个深度编码-解码器SNN体系结构\textit{SpikeMS}，用于解决以基于事件的DVS摄像机为输入的大规模运动分割问题。为了实现这一点，我们引入了一种新的时空损失公式，它包括脉冲计数和分类标签，并结合使用新的SNN反向传播技术。此外，我们还证明了\textit{SpikeMS}能够进行\textit{incremental predictions}，或者从比训练数据量更小的测试数据中进行预测。这对于为低延迟应用程序和需要快速预测的应用程序提供部分输入数据的输出是非常宝贵的。我们对来自EV-IMO、EED和MOD数据集的具有挑战性的合成和真实世界序列进行了评估，并取得了与可比较的ANN方法相当的结果，但使用的功率可能减少了50倍。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNN) are the so-called third generation of neural networks which attempt to more closely match the functioning of the biological brain. They inherently encode temporal data, allowing for training with less energy usage and can be extremely energy efficient when coded on neuromorphic hardware. In addition, they are well suited for tasks involving event-based sensors, which match the event-based nature of the SNN. However, SNNs have not been as effectively applied to real-world, large-scale tasks as standard Artificial Neural Networks (ANNs) due to the algorithmic and training complexity. To exacerbate the situation further, the input representation is unconventional and requires careful analysis and deep understanding. In this paper, we propose \textit{SpikeMS}, the first deep encoder-decoder SNN architecture for the real-world large-scale problem of motion segmentation using the event-based DVS camera as input. To accomplish this, we introduce a novel spatio-temporal loss formulation that includes both spike counts and classification labels in conjunction with the use of new techniques for SNN backpropagation. In addition, we show that \textit{SpikeMS} is capable of \textit{incremental predictions}, or predictions from smaller amounts of test data than it is trained on. This is invaluable for providing outputs even with partial input data for low-latency applications and those requiring fast predictions. We evaluated \textit{SpikeMS} on challenging synthetic and real-world sequences from EV-IMO, EED and MOD datasets and achieving results on a par with a comparable ANN method, but using potentially 50 times less power. </details>
<details>	<summary>注释</summary>	7 pages, 6 figures, 3 tables, Under review IROS 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月17日</details>

# 119、基于深度连续局部学习的深脉冲卷积神经网络单目标定位
- [ ] Deep Spiking Convolutional Neural Network for Single Object Localization Based On Deep Continuous Local Learning 
时间：2021年05月12日                         第一作者：Sami Barchid                       [链接](https://arxiv.org/abs/2105.05609).                     
## 摘要：随着神经形态硬件的出现，脉冲神经网络可以成为人工神经网络的一个很好的节能替代品。然而，使用脉冲神经网络来执行计算机视觉任务仍然是有限的，主要集中在简单的任务，如数字识别。由于针对这些任务的深脉冲神经网络的研究较少，因此很难处理更复杂的任务（如分割、目标检测）。本论文的目的是利用监督脉冲神经网络向现代计算机视觉迈出第一步。提出了一种用于灰度图像中单个目标定位的深度卷积脉冲神经网络。我们提出了一种基于decole的网络，decole是一种基于局部代理梯度学习的脉冲模型。Oxford IIIT Pet上报告的令人鼓舞的结果验证了脉冲神经网络在未来更精细的视觉任务中的应用。
<details>	<summary>英文摘要</summary>	With the advent of neuromorphic hardware, spiking neural networks can be a good energy-efficient alternative to artificial neural networks. However, the use of spiking neural networks to perform computer vision tasks remains limited, mainly focusing on simple tasks such as digit recognition. It remains hard to deal with more complex tasks (e.g. segmentation, object detection) due to the small number of works on deep spiking neural networks for these tasks. The objective of this paper is to make the first step towards modern computer vision with supervised spiking neural networks. We propose a deep convolutional spiking neural network for the localization of a single object in a grayscale image. We propose a network based on DECOLLE, a spiking model that enables local surrogate gradient-based learning. The encouraging results reported on Oxford-IIIT-Pet validates the exploitation of spiking neural networks with a supervised learning approach for more elaborate vision tasks in the future. </details>
<details>	<summary>邮件日期</summary>	2021年05月13日</details>

# 118、深度脉冲神经网络的梯度重排剪枝
- [ ] Pruning of Deep Spiking Neural Networks through Gradient Rewiring 
时间：2021年05月11日                         第一作者：Yanqi Chen                       [链接](https://arxiv.org/abs/2105.04916).                     
## 摘要：脉冲神经网络（Spiking Neural Networks，SNNs）因其在神经形态芯片上的生物合理性和高能量利用率而备受关注。由于这些芯片通常是资源受限的，因此snn的压缩在snn的实际应用中至关重要。现有的方法大多直接将人工神经网络中的剪枝方法应用于snn，忽略了ann与snn的区别，从而限制了被剪枝snn的性能。此外，这些方法只适用于浅层snn。本文受神经系统中突触形成和突触消除的启发，提出了一种基于连通性和权值的SNNs联合学习算法gradr（gradr），使我们能够在不受再训练的情况下无缝地优化网络结构。我们的关键创新是重新定义梯度到一个新的突触参数，允许通过充分利用剪枝和连接再生之间的竞争来更好地探索网络结构。实验结果表明，该方法在MNIST和CIFAR-10数据集上实现了最小的SNNs性能损失。此外，在前所未有的0.73%连通度下，其精度损失达到3.5%，显示了SNNs显著的结构细化能力。我们的工作表明，在深度snn中存在着极高的冗余度。我们的代码可从\url获得{https://github.com/Yanqi-Chen/Gradient-Rewiring}.
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have been attached great importance due to their biological plausibility and high energy-efficiency on neuromorphic chips. As these chips are usually resource-constrained, the compression of SNNs is thus crucial along the road of practical use of SNNs. Most existing methods directly apply pruning approaches in artificial neural networks (ANNs) to SNNs, which ignore the difference between ANNs and SNNs, thus limiting the performance of the pruned SNNs. Besides, these methods are only suitable for shallow SNNs. In this paper, inspired by synaptogenesis and synapse elimination in the neural system, we propose gradient rewiring (Grad R), a joint learning algorithm of connectivity and weight for SNNs, that enables us to seamlessly optimize network structure without retrain. Our key innovation is to redefine the gradient to a new synaptic parameter, allowing better exploration of network structures by taking full advantage of the competition between pruning and regrowth of connections. The experimental results show that the proposed method achieves minimal loss of SNNs' performance on MNIST and CIFAR-10 dataset so far. Moreover, it reaches a $\sim$3.5% accuracy loss under unprecedented 0.73% connectivity, which reveals remarkable structure refining capability in SNNs. Our work suggests that there exists extremely high redundancy in deep SNNs. Our codes are available at \url{https://github.com/Yanqi-Chen/Gradient-Rewiring}. </details>
<details>	<summary>注释</summary>	9 pages,7 figures. Accepted by IJCAI 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月12日</details>

# 117、神经形态处理器上多层脉冲神经网络的硬件学习
- [ ] In-Hardware Learning of Multilayer Spiking Neural Networks on a Neuromorphic Processor 
时间：2021年05月08日                         第一作者：Amar Shrestha                       [链接](https://arxiv.org/abs/2105.03649).                     
## 摘要：尽管反向传播在机器学习中有着广泛的应用，但它不能直接应用于SNN训练，在模拟生物神经元和突触的神经形态处理器上也不可行。本文提出了一种具有生物似然局部更新规则的基于脉冲的反向传播算法，并对其进行了改进以适应神经形态硬件中的约束条件。该算法在intelloihi芯片上实现，实现了移动应用中多层snn的低功耗硬件监督在线学习。我们在MNIST、Fashion MNIST、CIFAR-10和MSTAR数据集上测试了这个实现，并展示了用这个实现进行增量在线学习的可能性。
<details>	<summary>英文摘要</summary>	Although widely used in machine learning, backpropagation cannot directly be applied to SNN training and is not feasible on a neuromorphic processor that emulates biological neuron and synapses. This work presents a spike-based backpropagation algorithm with biological plausible local update rules and adapts it to fit the constraint in a neuromorphic hardware. The algorithm is implemented on Intel Loihi chip enabling low power in-hardware supervised online learning of multilayered SNNs for mobile applications. We test this implementation on MNIST, Fashion-MNIST, CIFAR-10 and MSTAR datasets with promising performance and energy-efficiency, and demonstrate a possibility of incremental online learning with the implementation. </details>
<details>	<summary>注释</summary>	6 pages, 5 figures, accepted for Design Automation Conference (DAC) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年05月11日</details>

# 116、Izhikevich激发的具有兴奋和抑制输入的光电神经元用于高效能光子脉冲神经网络
- [ ] Izhikevich-Inspired Optoelectronic Neurons with Excitatory and Inhibitory Inputs for Energy-Efficient Photonic Spiking Neural Networks 
时间：2021年05月03日                         第一作者：Yun-jhu Lee                       [链接](https://arxiv.org/abs/2105.02809).                     
## 摘要：据我们所知，我们第一次设计、原型化和实验演示了一个受Izhikevich模型启发的光电脉冲神经元，该模型结合了兴奋性和抑制性光脉冲输入，并相应地产生光脉冲输出。光电神经元由三个晶体管组成，作为电脉冲电路，一个垂直腔面发射激光器（VCSEL）用于光脉冲输出，两个光电探测器用于激发和抑制光脉冲输入。附加的电容器和电阻完成了伊日克维奇启发的光电神经元，接收兴奋性和抑制性光脉冲作为其他光电神经元的输入。我们在Verilog-a中建立了一个详细的光电神经元模型，并模拟了各种情况下具有兴奋性输入和抑制性输入信号的电路级操作。实验结果与模拟结果非常相似，证明了兴奋性输入是如何触发光脉冲输出的，而抑制性输入是如何抑制光脉冲输出的。利用模拟的神经元模型，我们使用完全连接（FC）和卷积神经网络（CNN）进行了模拟。用MNIST手写体数字识别的仿真结果表明，无监督学习的识别率为90%，改进的有监督FC神经网络的识别率为97%。我们进一步设计了一个利用量子阻抗转换的纳米级光电神经元，其中200aj/脉冲输入可以触发具有10fj/脉冲的片上纳米激光器的输出。纳米级神经元在神经网络中以10 GSpikes/s的速度运行时，可以支持约80的扇出或克服19 dB的多余光损耗，与最先进的电子神经形态硬件（如Loihi和NeuroGrid）相比，这相当于100倍的吞吐量和1000倍的能效提高。
<details>	<summary>英文摘要</summary>	We designed, prototyped, and experimentally demonstrated, for the first time to our knowledge, an optoelectronic spiking neuron inspired by the Izhikevich model incorporating both excitatory and inhibitory optical spiking inputs and producing optical spiking outputs accordingly. The optoelectronic neurons consist of three transistors acting as electrical spiking circuits, a vertical-cavity surface-emitting laser (VCSEL) for optical spiking outputs, and two photodetectors for excitatory and inhibitory optical spiking inputs. Additional inclusion of capacitors and resistors complete the Izhikevich-inspired optoelectronic neurons, which receive excitatory and inhibitory optical spikes as inputs from other optoelectronic neurons. We developed a detailed optoelectronic neuron model in Verilog-A and simulated the circuit-level operation of various cases with excitatory input and inhibitory input signals. The experimental results closely resemble the simulated results and demonstrate how the excitatory inputs trigger the optical spiking outputs while the inhibitory inputs suppress the outputs. Utilizing the simulated neuron model, we conducted simulations using fully connected (FC) and convolutional neural networks (CNN). The simulation results using MNIST handwritten digits recognition show 90% accuracy on unsupervised learning and 97% accuracy on a supervised modified FC neural network. We further designed a nanoscale optoelectronic neuron utilizing quantum impedance conversion where a 200 aJ/spike input can trigger the output from on-chip nanolasers with 10 fJ/spike. The nanoscale neuron can support a fanout of ~80 or overcome 19 dB excess optical loss while running at 10 GSpikes/second in the neural network, which corresponds to 100x throughput and 1000x energy-efficiency improvement compared to state-of-art electrical neuromorphic hardware such as Loihi and NeuroGrid. </details>
<details>	<summary>注释</summary>	24 pages, 13 figures </details>
<details>	<summary>邮件日期</summary>	2021年05月07日</details>

# 115、神经形态计算中的动态可靠性管理
- [ ] Dynamic Reliability Management in Neuromorphic Computing 
时间：2021年05月05日                         第一作者：Shihao Song                       [链接](https://arxiv.org/abs/2105.02038).                     
## 摘要：神经形态计算系统利用非易失性存储器（NVM）实现高密度、低能的突触存储。操作nvm所需的电压和电流升高会导致每个神经元中基于CMOS的晶体管和硬件中的突触电路老化，使晶体管的参数偏离其标称值。激进的设备缩放增加功率密度和温度，加速老化，挑战神经形态系统的可靠运行。现有的面向可靠性的技术周期性地以固定的时间间隔消除硬件中所有神经元和突触电路的压力，假设最坏的运行条件，而不实际跟踪它们在运行时的老化情况。为了消除这些电路的应力，必须中断正常操作，这会在脉冲生成和传播中引入延迟，影响脉冲间隔，从而影响性能，例如精度。我们提出了一种新的架构技术，通过设计一个智能运行时管理器（NCRTM）来缓解神经形态系统中与老化相关的可靠性问题，该管理器在执行机器学习工作负载的过程中，针对CMOS晶体管的短期老化，动态地降低神经元和突触电路的压力，以达到可靠性目标。NCRTM仅在绝对必要时才对这些电路进行去应力处理，否则，通过将去应力操作安排在关键路径之外来降低性能影响。我们评估NCRTM与国家的最先进的机器学习工作负荷的神经形态硬件。我们的结果表明，NCRTM显著提高了神经形态硬件的可靠性，对性能的影响很小。
<details>	<summary>英文摘要</summary>	Neuromorphic computing systems uses non-volatile memory (NVM) to implement high-density and low-energy synaptic storage. Elevated voltages and currents needed to operate NVMs cause aging of CMOS-based transistors in each neuron and synapse circuit in the hardware, drifting the transistor's parameters from their nominal values. Aggressive device scaling increases power density and temperature, which accelerates the aging, challenging the reliable operation of neuromorphic systems. Existing reliability-oriented techniques periodically de-stress all neuron and synapse circuits in the hardware at fixed intervals, assuming worst-case operating conditions, without actually tracking their aging at run time. To de-stress these circuits, normal operation must be interrupted, which introduces latency in spike generation and propagation, impacting the inter-spike interval and hence, performance, e.g., accuracy. We propose a new architectural technique to mitigate the aging-related reliability problems in neuromorphic systems, by designing an intelligent run-time manager (NCRTM), which dynamically destresses neuron and synapse circuits in response to the short-term aging in their CMOS transistors during the execution of machine learning workloads, with the objective of meeting a reliability target. NCRTM de-stresses these circuits only when it is absolutely necessary to do so, otherwise reducing the performance impact by scheduling de-stress operations off the critical path. We evaluate NCRTM with state-of-the-art machine learning workloads on a neuromorphic hardware. Our results demonstrate that NCRTM significantly improves the reliability of neuromorphic hardware, with marginal impact on performance. </details>
<details>	<summary>注释</summary>	Accepted in ACM JETC </details>
<details>	<summary>邮件日期</summary>	2021年05月06日</details>

# 114、neuroxplorer1.0：一个带脉冲神经网络的可扩展架构探索框架
- [ ] NeuroXplorer 1.0: An Extensible Framework for Architectural Exploration with Spiking Neural Networks 
时间：2021年05月04日                         第一作者：Adarsha Balaji                        [链接](https://arxiv.org/abs/2105.01795).                     
## 摘要：最近，工业界和学术界都提出了许多不同的神经形态结构来执行用脉冲神经网络（SNN）设计的应用程序。因此，越来越需要一个可扩展的仿真框架来进行SNNs的架构探索，包括当今硬件的基于平台的设计，以及未来的软硬件协同设计和设计技术协同优化。我们介绍了NeuroXplorer，这是一个快速且可扩展的框架，它基于一个通用的模板来建模一个神经形态的架构，该架构可以注入给定硬件和/或技术的特定细节。NeuroXplorer可以执行低级别的周期精确架构模拟和数据流抽象的高级分析。NeuroXplorer的优化引擎可以结合面向硬件的指标，如能量、吞吐量和延迟，以及面向SNN的指标，如脉冲间隔失真和脉冲紊乱，这些指标直接影响SNN性能。我们通过许多最先进的机器学习模型的案例研究来展示NeuroXplorer的架构探索能力。
<details>	<summary>英文摘要</summary>	Recently, both industry and academia have proposed many different neuromorphic architectures to execute applications that are designed with Spiking Neural Network (SNN). Consequently, there is a growing need for an extensible simulation framework that can perform architectural explorations with SNNs, including both platform-based design of today's hardware, and hardware-software co-design and design-technology co-optimization of the future. We present NeuroXplorer, a fast and extensible framework that is based on a generalized template for modeling a neuromorphic architecture that can be infused with the specific details of a given hardware and/or technology. NeuroXplorer can perform both low-level cycle-accurate architectural simulations and high-level analysis with data-flow abstractions. NeuroXplorer's optimization engine can incorporate hardware-oriented metrics such as energy, throughput, and latency, as well as SNN-oriented metrics such as inter-spike interval distortion and spike disorder, which directly impact SNN performance. We demonstrate the architectural exploration capabilities of NeuroXplorer through case studies with many state-of-the-art machine learning models. </details>
<details>	<summary>邮件日期</summary>	2021年05月06日</details>

# 113、基于层次图像分割和关系预测的医院和实验室液体样品计算机视觉
- [ ] Computer vision for liquid samples in hospitals and medical labs using hierarchical image segmentation and relations prediction 
时间：2021年05月04日                         第一作者：Sagi Eppel                       [链接](https://arxiv.org/abs/2105.01456).                     
## 摘要：这项工作探讨了如何利用计算机视觉对透明容器（如试管、注射器、输液袋）中的医用液体样本进行图像分割和分类。处理输液、血液和尿样等液体是医学实验室和医院工作的重要组成部分。从图像中准确识别和分割液体和盛装液体的容器的能力有助于实现这些过程的自动化。现代计算机视觉通常涉及到在大量的带注释图像数据集上训练深层神经网络。这项工作提出了一个新的数据集，其中包含1300个医学样本的注释图像，包括含有液体和固体物质的血管。图像标注有液体类型（如血液、尿液）、材料相（如液体、固体、泡沫、悬浮液）、血管类型（如注射器、管子、杯子、输液瓶/袋）和血管属性（透明、不透明）。此外，容器零件（如软木塞、标签、钉子和阀门）也会进行注释。容器和材料之间的关系和层次结构也会被注释，例如哪个容器包含哪个材料，或者哪个容器相互链接或包含。在数据集上训练三个神经网络：一个网络学习检测血管，第二个网络检测每个血管内的材料和零件，第三个网络识别血管之间的关系和连通性。
<details>	<summary>英文摘要</summary>	This work explores the use of computer vision for image segmentation and classification of medical fluid samples in transparent containers (for example, tubes, syringes, infusion bags). Handling fluids such as infusion fluids, blood, and urine samples is a significant part of the work carried out in medical labs and hospitals. The ability to accurately identify and segment the liquids and the vessels that contain them from images can help in automating such processes. Modern computer vision typically involves training deep neural nets on large datasets of annotated images. This work presents a new dataset containing 1,300 annotated images of medical samples involving vessels containing liquids and solid material. The images are annotated with the type of liquid (e.g., blood, urine), the phase of the material (e.g., liquid, solid, foam, suspension), the type of vessel (e.g., syringe, tube, cup, infusion bottle/bag), and the properties of the vessel (transparent, opaque). In addition, vessel parts such as corks, labels, spikes, and valves are annotated. Relations and hierarchies between vessels and materials are also annotated, such as which vessel contains which material or which vessels are linked or contain each other. Three neural networks are trained on the dataset: One network learns to detect vessels, a second net detects the materials and parts inside each vessel, and a third net identifies relationships and connectivity between vessels. </details>
<details>	<summary>邮件日期</summary>	2021年05月05日</details>

# 112、在神经形态处理器Loihi上利用脉冲神经网络实现资源受限导航
- [ ] Simplified Klinokinesis using Spiking Neural Networks for Resource-Constrained Navigation on the Neuromorphic Processor Loihi 
时间：2021年05月04日                         第一作者：Apoorv Kishore                       [链接](https://arxiv.org/abs/2105.01358).                     
## 摘要：C。线虫通过Klingokinesis显示趋化性，蠕虫基于单个浓度传感器感知浓度，计算浓度梯度，通过梯度上升/下降到目标浓度，然后进行轮廓跟踪，从而进行觅食。仿生的实现需要具有多离子通道动力学的复杂神经元以及中间神经元来控制。虽然这是自主机器人的一项关键功能，但在英特尔的Loihi等节能神经形态硬件上实现这一功能需要对网络进行自适应，以适应特定于硬件的约束，但这一点尚未实现。在本文中，我们证明了基于klinokinesis的趋化性对Loihi的适应性，通过仅用LIF神经元实现必要的神经元动力学以及所有功能（如Heaviside函数和减法）的完全基于脉冲的实现。我们的结果表明，Loihi实现在性能方面与Python上的软件相当——无论是在觅食还是轮廓跟踪过程中。Loihi结果在噪声环境中也具有弹性。因此，我们证明了Loihi上趋化性的成功适应-现在可以结合丰富的SNN块阵列用于基于SNN的复杂机器人控制。
<details>	<summary>英文摘要</summary>	C. elegans shows chemotaxis using klinokinesis where the worm senses the concentration based on a single concentration sensor to compute the concentration gradient to perform foraging through gradient ascent/descent towards the target concentration followed by contour tracking. The biomimetic implementation requires complex neurons with multiple ion channel dynamics as well as interneurons for control. While this is a key capability of autonomous robots, its implementation on energy-efficient neuromorphic hardware like Intel's Loihi requires adaptation of the network to hardware-specific constraints, which has not been achieved. In this paper, we demonstrate the adaptation of chemotaxis based on klinokinesis to Loihi by implementing necessary neuronal dynamics with only LIF neurons as well as a complete spike-based implementation of all functions e.g. Heaviside function and subtractions. Our results show that Loihi implementation is equivalent to the software counterpart on Python in terms of performance - both during foraging and contour tracking. The Loihi results are also resilient in noisy environments. Thus, we demonstrate a successful adaptation of chemotaxis on Loihi - which can now be combined with the rich array of SNN blocks for SNN based complex robotic control. </details>
<details>	<summary>邮件日期</summary>	2021年05月05日</details>

# 111、光谱机器学习在胰腺肿块影像分类中的应用
- [ ] Spectral Machine Learning for Pancreatic Mass Imaging Classification 
时间：2021年05月03日                         第一作者：Yiming Liu                       [链接](https://arxiv.org/abs/2105.00728).                     
## 摘要：我们提出了一种新的光谱机器学习（SML）方法用于胰腺肿块的CT筛查。我们的算法是基于公共数据源，用250名患者（50名胰腺正常患者和200名胰腺异常患者）的大约30000张图像进行训练的。样本外诊断分类的准确率为94.6%，基于113例患者共约15000张图像，其中32例胰腺正常患者和81例胰腺异常患者中有26例得到正确诊断。SML能够在诊断分类中自动选择基本图像（平均每个病人5或9张图像），达到上述精度。在一台具有标准CPU运行环境的笔记本电脑上，诊断113例患者的计算时间为75秒。影响光谱学习与机器学习结合的因素包括：1）在分类训练中，利用样本协方差矩阵的几个最大特征值对应的特征向量（脉冲特征向量）来选择输入属性，只考虑原始图像的基本信息，噪声较小；2） 基于平均水平谱检验去除无关像素，在保持较高分类精度的同时，降低了对存储容量的挑战，提高了计算效率；3） 采用最先进的机器学习分类、梯度提升和随机森林。我们的方法展示了在人工智能时代胰腺肿块筛查的实用性和提高的图像诊断准确率。
<details>	<summary>英文摘要</summary>	We present a novel spectral machine learning (SML) method in screening for pancreatic mass using CT imaging. Our algorithm is trained with approximately 30,000 images from 250 patients (50 patients with normal pancreas and 200 patients with abnormal pancreas findings) based on public data sources. A test accuracy of 94.6 percents was achieved in the out-of-sample diagnosis classification based on a total of approximately 15,000 images from 113 patients, whereby 26 out of 32 patients with normal pancreas and all 81 patients with abnormal pancreas findings were correctly diagnosed. SML is able to automatically choose fundamental images (on average 5 or 9 images for each patient) in the diagnosis classification and achieve the above mentioned accuracy. The computational time is 75 seconds for diagnosing 113 patients in a laptop with standard CPU running environment. Factors that influenced high performance of a well-designed integration of spectral learning and machine learning included: 1) use of eigenvectors corresponding to several of the largest eigenvalues of sample covariance matrix (spike eigenvectors) to choose input attributes in classification training, taking into account only the fundamental information of the raw images with less noise; 2) removal of irrelevant pixels based on mean-level spectral test to lower the challenges of memory capacity and enhance computational efficiency while maintaining superior classification accuracy; 3) adoption of state-of-the-art machine learning classification, gradient boosting and random forest. Our methodology showcases practical utility and improved accuracy of image diagnosis in pancreatic mass screening in the era of AI. </details>
<details>	<summary>注释</summary>	17 pages, 3 figures MSC-class: 62P10 </details>
<details>	<summary>邮件日期</summary>	2021年05月04日</details>

# 110、Neko：探索神经形态学习规则的图书馆
- [ ] Neko: a Library for Exploring Neuromorphic Learning Rules 
时间：2021年05月01日                         第一作者：Zixuan Zhao                       [链接](https://arxiv.org/abs/2105.00324).                     
## 摘要：神经形态计算领域正处于一个积极探索的时期。虽然已经开发了许多工具来模拟神经元动力学或将深层网络转换为脉冲模型，但用于学习规则的通用软件库仍然没有得到充分的探索。这在一定程度上是由于设计新的学习规则的多样性和挑战性，从编码方法到梯度近似，从模仿贝叶斯大脑的群体方法到部署在忆阻器横杆上的约束学习算法。为了解决这一差距，我们提出了Neko，一个模块化的，可扩展的图书馆，重点是帮助设计新的学习算法。我们在三个例子中展示了Neko的实用性：在线局部学习、概率学习和模拟设备学习。我们的结果表明，Neko可以复制最先进的算法，并且在一种情况下，在精度和速度上有显著的优势。此外，它还提供了包括梯度比较在内的工具，可以帮助开发新的算法变体。Neko是一个开源的Python库，支持PyTorch和TensorFlow后端。
<details>	<summary>英文摘要</summary>	The field of neuromorphic computing is in a period of active exploration. While many tools have been developed to simulate neuronal dynamics or convert deep networks to spiking models, general software libraries for learning rules remain underexplored. This is partly due to the diverse, challenging nature of efforts to design new learning rules, which range from encoding methods to gradient approximations, from population approaches that mimic the Bayesian brain to constrained learning algorithms deployed on memristor crossbars. To address this gap, we present Neko, a modular, extensible library with a focus on aiding the design of new learning algorithms. We demonstrate the utility of Neko in three exemplar cases: online local learning, probabilistic learning, and analog on-device learning. Our results show that Neko can replicate the state-of-the-art algorithms and, in one case, lead to significant outperformance in accuracy and speed. Further, it offers tools including gradient comparison that can help develop new algorithmic variants. Neko is an open source Python library that supports PyTorch and TensorFlow backends. </details>
<details>	<summary>邮件日期</summary>	2021年05月04日</details>

# 109、一种新的脉冲神经网络近似汉明权值计算方法：一种FPGA友好结构
- [ ] A Novel Approximate Hamming Weight Computing for Spiking Neural Networks: an FPGA Friendly Architecture 
时间：2021年04月29日                         第一作者：Kaveh Akbarzadeh-Sherbaf                       [链接](https://arxiv.org/abs/2104.14594).                     
## 摘要：稀疏长二值向量的Hamming权值是许多科学应用中的重要模块，特别是在我们感兴趣的脉冲神经网络中。为了提高FPGA实现的面积和延迟，我们从突触传输失败的角度出发，提出了一种利用FPGA查找表压缩长输入向量的方法。为了评估这种方法的有效性，我们使用一个简单的线性加法器来计算压缩向量的'1'个数。我们将压缩器分为具有两级以上查找表的浅压缩器和具有两级以上查找表的深压缩器。该方法生成的体系结构显示，对于不同配置的浅层压缩器，面积和延迟分别减少了82%和35%。此外，我们的模拟结果显示，仅使用深压缩器计算一个脉冲神经网络1024位向量的汉明权值，保留了网络的混沌行为，但对学习性能影响不大。
<details>	<summary>英文摘要</summary>	Hamming weights of sparse and long binary vectors are important modules in many scientific applications, particularly in spiking neural networks that are of our interest. To improve both area and latency of their FPGA implementations, we propose a method inspired from synaptic transmission failure for exploiting FPGA lookup tables to compress long input vectors. To evaluate the effectiveness of this approach, we count the number of `1's of the compressed vector using a simple linear adder. We classify the compressors into shallow ones with up to two levels of lookup tables and deep ones with more than two levels. The architecture generated by this approach shows up to 82% and 35% reductions for different configurations of shallow compressors in area and latency respectively. Moreover, our simulation results show that calculating the Hamming weight of a 1024-bit vector of a spiking neural network by the use of only deep compressors preserves the chaotic behavior of the network while slightly impacts on the learning performance. </details>
<details>	<summary>邮件日期</summary>	2021年05月03日</details>

# 108、Hessian感知的脉冲神经网络量化
- [ ] Hessian Aware Quantization of Spiking Neural Networks 
时间：2021年04月29日                         第一作者：Hin Wai Lui                        [链接](https://arxiv.org/abs/2104.14117).                     
## 摘要：为了实现脉冲神经网络（SNNs）的低延迟、高吞吐量和能量效率优势，减少在神经形态硬件上运行时的内存和计算需求是一个重要的步骤。神经形态结构允许大规模并行计算，具有可变和局部位精度。然而，如何将不同的比特精度分配给网络的不同层或连接并非易事。在这项工作中，我们演示了分层Hessian跟踪分析如何测量损耗对层权重的任何扰动的敏感性，这可以用来指导量化SNN时特定于层的比特精度的分配。另外，目前基于梯度的SNN训练方法采用的是多状态变量的复杂神经元模型，计算效率和记忆效率都不理想。为了应对这一挑战，我们提出了一个简化的神经元模型，该模型将状态变量的数量减少了4倍，同时仍然兼容基于梯度的训练。我们发现，使用逐层位精度时对模型精度的影响与该层的Hessian轨迹密切相关。优化后的量化网络精度仅下降了0.2%，但网络规模却减小了58%。这减少了内存使用，并允许使用具有更简单数字电路的定点算法，从而提高了总体吞吐量和能源效率。
<details>	<summary>英文摘要</summary>	To achieve the low latency, high throughput, and energy efficiency benefits of Spiking Neural Networks (SNNs), reducing the memory and compute requirements when running on a neuromorphic hardware is an important step. Neuromorphic architecture allows massively parallel computation with variable and local bit-precisions. However, how different bit-precisions should be allocated to different layers or connections of the network is not trivial. In this work, we demonstrate how a layer-wise Hessian trace analysis can measure the sensitivity of the loss to any perturbation of the layer's weights, and this can be used to guide the allocation of a layer-specific bit-precision when quantizing an SNN. In addition, current gradient based methods of SNN training use a complex neuron model with multiple state variables, which is not ideal for compute and memory efficiency. To address this challenge, we present a simplified neuron model that reduces the number of state variables by 4-fold while still being compatible with gradient based training. We find that the impact on model accuracy when using a layer-wise bit-precision correlated well with that layer's Hessian trace. The accuracy of the optimal quantized network only dropped by 0.2%, yet the network size was reduced by 58%. This reduces memory usage and allows fixed-point arithmetic with simpler digital circuits to be used, increasing the overall throughput and energy efficiency. </details>
<details>	<summary>邮件日期</summary>	2021年04月30日</details>

# 107、用于语音分类的液态机中硬件友好的突触顺序和时间标度
- [ ] Hardware-Friendly Synaptic Orders and Timescales in Liquid State Machines for Speech Classification 
时间：2021年04月29日                         第一作者：Vivek Saraswat                       [链接](https://arxiv.org/abs/2104.14264).                     
## 摘要：液体状态机是一种受大脑启发的脉冲神经网络（SNNs），具有随机存储连接和仿生神经元和突触模型。为了解决时间分类问题，提出了储层计算网络作为深度神经网络的替代方法。以往的研究表明，二阶（双指数）突触波形是实现高精度的TI-46语音数字识别的关键。长时间范围（ms）仿生突触波形的提出是对紧凑且节能的神经形态硬件的挑战。在这项工作中，我们分析了突触顺序的作用，即：{\delta}（单时间步的高输出），0th（有限脉冲宽度的矩形），一阶（指数下降）和二阶（指数上升和下降）以及突触时间尺度对水库输出响应和更全面参数扫描下TI-46语音数字分类精度的影响。我们发现最佳操作点与油藏中的最佳峰值活动范围相关。此外，提出的第0级突触的表现与生物学上合理的第2级突触相当。这对电路设计者来说是一个很大的放松，因为突触是SNN内存中实现中最丰富的组件。强调了0阶synapse模拟和混合信号实现的电路优势，通过消除运算放大器和数模转换器电路，在面积和功耗方面节省了2-3个数量级。这对一个完整的神经网络实现有着重要的影响，重点在于外围限制和克服这些限制的算法简化。
<details>	<summary>英文摘要</summary>	Liquid State Machines are brain inspired spiking neural networks (SNNs) with random reservoir connectivity and bio-mimetic neuronal and synaptic models. Reservoir computing networks are proposed as an alternative to deep neural networks to solve temporal classification problems. Previous studies suggest 2nd order (double exponential) synaptic waveform to be crucial for achieving high accuracy for TI-46 spoken digits recognition. The proposal of long-time range (ms) bio-mimetic synaptic waveforms is a challenge to compact and power efficient neuromorphic hardware. In this work, we analyze the role of synaptic orders namely: {\delta} (high output for single time step), 0th (rectangular with a finite pulse width), 1st (exponential fall) and 2nd order (exponential rise and fall) and synaptic timescales on the reservoir output response and on the TI-46 spoken digits classification accuracy under a more comprehensive parameter sweep. We find the optimal operating point to be correlated to an optimal range of spiking activity in the reservoir. Further, the proposed 0th order synapses perform at par with the biologically plausible 2nd order synapses. This is substantial relaxation for circuit designers as synapses are the most abundant components in an in-memory implementation for SNNs. The circuit benefits for both analog and mixed-signal realizations of 0th order synapse are highlighted demonstrating 2-3 orders of savings in area and power consumptions by eliminating Op-Amps and Digital to Analog Converter circuits. This has major implications on a complete neural network implementation with focus on peripheral limitations and algorithmic simplifications to overcome them. </details>
<details>	<summary>邮件日期</summary>	2021年04月30日</details>

# 106、神经形态计算是图灵完备的
- [ ] Neuromorphic Computing is Turing-Complete 
时间：2021年04月28日                         第一作者：Prasanna Date                       [链接](https://arxiv.org/abs/2104.13983).                     
## 摘要：神经形态计算是一种非冯诺依曼计算范式，通过模拟人脑来执行计算。神经形态的系统是非常节能的，并且已知消耗的能量比CPU和GPU少数千倍。它们有可能在未来推动诸如自动车辆、边缘计算和物联网等关键用例。出于这个原因，它们被寻求成为未来计算领域不可或缺的一部分。神经形态系统主要用于基于脉冲的机器学习应用，尽管在图论、微分方程和基于脉冲的仿真中也有一些非机器学习的应用。这些应用表明，神经形态计算可能是通用计算能力。然而，神经形态计算的通用可计算性尚未建立。在这项工作中，我们证明了神经形态计算是图灵完备的，因此能够进行通用计算。具体来说，我们提出了一个神经形态计算模型，只有两个神经元参数（阈值和泄漏）和两个突触参数（权重和延迟）。我们设计了用于计算所有{mu}递归函数（即常数、后继函数和投影函数）和所有{mu}递归算子（即合成算子、本原递归算子和极小化算子）的神经形态电路。鉴于{\mu}-递归函数和算子正是可以用图灵机计算的函数和算子，本文建立了神经形态计算的图灵完备性。
<details>	<summary>英文摘要</summary>	Neuromorphic computing is a non-von Neumann computing paradigm that performs computation by emulating the human brain. Neuromorphic systems are extremely energy-efficient and known to consume thousands of times less power than CPUs and GPUs. They have the potential to drive critical use cases such as autonomous vehicles, edge computing and internet of things in the future. For this reason, they are sought to be an indispensable part of the future computing landscape. Neuromorphic systems are mainly used for spike-based machine learning applications, although there are some non-machine learning applications in graph theory, differential equations, and spike-based simulations. These applications suggest that neuromorphic computing might be capable of general-purpose computing. However, general-purpose computability of neuromorphic computing has not been established yet. In this work, we prove that neuromorphic computing is Turing-complete and therefore capable of general-purpose computing. Specifically, we present a model of neuromorphic computing, with just two neuron parameters (threshold and leak), and two synaptic parameters (weight and delay). We devise neuromorphic circuits for computing all the {\mu}-recursive functions (i.e., constant, successor and projection functions) and all the {\mu}-recursive operators (i.e., composition, primitive recursion and minimization operators). Given that the {\mu}-recursive functions and operators are precisely the ones that can be computed using a Turing machine, this work establishes the Turing-completeness of neuromorphic computing. </details>
<details>	<summary>邮件日期</summary>	2021年04月30日</details>

# 105、SpikE：基于SpikE的多关系图数据嵌入
- [ ] SpikE: spike-based embeddings for multi-relational graph data 
时间：2021年04月27日                         第一作者：Dominik Dold                       [链接](https://arxiv.org/abs/2104.13398).                     
## 摘要：尽管最近成功地将基于脉冲的编码与错误反向传播算法相结合，脉冲神经网络仍然主要应用于源于感觉处理的任务，操作于传统的数据结构，如视觉或听觉数据。一种在工业和研究中得到广泛应用的丰富数据表示是所谓的知识图——一种基于图的结构，其中实体被描述为节点，实体之间的关系被描述为边。像分子、社会网络和工业工厂系统这样的复杂系统可以用知识图的公共语言来描述，允许使用图嵌入算法在这些信息密集的环境中进行上下文感知的预测。我们提出了一种基于脉冲的算法，其中图中的节点由神经元群体的单脉冲时间表示，而群体之间的关系则由脉冲时间差表示。学习这种基于脉冲的嵌入只需要有关脉冲时间和脉冲时间差的知识，这与最近提出的训练脉冲神经网络的框架是一致的。所提出的模型可以很容易地映射到当前的神经形态硬件系统，从而将知识图上的推理转移到这些体系结构蓬勃发展的领域，为这项技术开辟了一个很有前途的工业应用领域。
<details>	<summary>英文摘要</summary>	Despite the recent success of reconciling spike-based coding with the error backpropagation algorithm, spiking neural networks are still mostly applied to tasks stemming from sensory processing, operating on traditional data structures like visual or auditory data. A rich data representation that finds wide application in industry and research is the so-called knowledge graph - a graph-based structure where entities are depicted as nodes and relations between them as edges. Complex systems like molecules, social networks and industrial factory systems can be described using the common language of knowledge graphs, allowing the usage of graph embedding algorithms to make context-aware predictions in these information-packed environments. We propose a spike-based algorithm where nodes in a graph are represented by single spike times of neuron populations and relations as spike time differences between populations. Learning such spike-based embeddings only requires knowledge about spike times and spike time differences, compatible with recently proposed frameworks for training spiking neural networks. The presented model is easily mapped to current neuromorphic hardware systems and thereby moves inference on knowledge graphs into a domain where these architectures thrive, unlocking a promising industrial application area for this technology. </details>
<details>	<summary>注释</summary>	Accepted for publication at IJCNN 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月29日</details>

# 104、基于稀疏脉冲卷积神经网络的事件摄像机学习
- [ ] Learning from Event Cameras with Sparse Spiking Convolutional Neural Networks 
时间：2021年04月26日                         第一作者：Lo\"ic Cordone                       [链接](https://arxiv.org/abs/2104.12579).                     
## 摘要：卷积神经网络（CNNs）由于其令人印象深刻的结果和易于学习的特点，现已成为解决计算机视觉问题的实际方法。这些网络由一层层相互连接的单元组成，这些单元被称为人工神经元，松散地模拟了生物大脑中的神经元。然而，它们在传统硬件（CPU/GPU）上的实现导致了高功耗，使得它们在嵌入式系统上的集成变得困难。以汽车为例，嵌入式算法在能量、延迟和精度方面都有很高的限制。为了设计更有效的计算机视觉算法，我们建议采用事件摄像机和脉冲神经网络（SNNs）的端到端生物启发方法。事件摄像机输出异步和稀疏的事件，提供了一个非常有效的数据源，但是用同步和密集的算法（如CNNs）处理这些事件并没有产生任何显著的好处。为了解决这一局限性，我们使用了脉冲神经网络（SNNs），这是一种更具生物真实感的神经网络，其中单位使用离散脉冲进行通信。由于它们的操作性质，它们对硬件友好且节能，但培训它们仍然是一项挑战。我们的方法使用流行的深度学习框架PyTorch，直接在事件数据上训练稀疏脉冲卷积神经网络。在流行的DVS128手势数据集上，在准确性、稀疏性和训练时间方面的性能使得这种仿生方法有可能在低功耗的神经形态硬件上嵌入实时应用程序。
<details>	<summary>英文摘要</summary>	Convolutional neural networks (CNNs) are now the de facto solution for computer vision problems thanks to their impressive results and ease of learning. These networks are composed of layers of connected units called artificial neurons, loosely modeling the neurons in a biological brain. However, their implementation on conventional hardware (CPU/GPU) results in high power consumption, making their integration on embedded systems difficult. In a car for example, embedded algorithms have very high constraints in term of energy, latency and accuracy. To design more efficient computer vision algorithms, we propose to follow an end-to-end biologically inspired approach using event cameras and spiking neural networks (SNNs). Event cameras output asynchronous and sparse events, providing an incredibly efficient data source, but processing these events with synchronous and dense algorithms such as CNNs does not yield any significant benefits. To address this limitation, we use spiking neural networks (SNNs), which are more biologically realistic neural networks where units communicate using discrete spikes. Due to the nature of their operations, they are hardware friendly and energy-efficient, but training them still remains a challenge. Our method enables the training of sparse spiking convolutional neural networks directly on event data, using the popular deep learning framework PyTorch. The performances in terms of accuracy, sparsity and training time on the popular DVS128 Gesture Dataset make it possible to use this bio-inspired approach for the future embedding of real-time applications on low-power neuromorphic hardware. </details>
<details>	<summary>注释</summary>	Accepted to the International Joint Conference on Neural Networks (IJCNN) 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月27日</details>

# 103、脉冲神经网络第二部分：时空模式检测
- [ ] Spiking Neural Networks -- Part II: Detecting Spatio-Temporal Patterns 
时间：2021年04月26日                         第一作者：Nicolas Skatchkovsky                       [链接](https://arxiv.org/abs/2010.14217).                     
<details>	<summary>注释</summary>	The first two authors have equally contributed to this work. This version corrects some errors in the published paper </details>
<details>	<summary>邮件日期</summary>	2021年04月27日</details>

# 102、STDP在联想信息存储和检索中的神经动力学作用
- [ ] Neurodynamical Role of STDP in Storage and Retrieval of Associative Information 
时间：2021年04月25日                         第一作者：Hongkyu Yoon                        [链接](https://arxiv.org/abs/2104.12249).                     
## 摘要：棘突时间依赖性可塑性（STDP）是一个生物学过程，在这个过程中，神经元棘突的精确顺序和时间影响突触修饰的程度。虽然已经有许多研究集中于STDP在神经编码中的作用，但STDP在大脑宏观层面的功能意义尚未得到充分的探索。在这项工作中，我们提出STDP在一个脉冲神经元群中呈现以“记忆平面”的形式存储高维信息。基于STDP的神经活动将周期性的时空输入模式转换为相应的记忆平面，在该记忆平面中，存储的信息可以通过适当的提示动态恢复。利用显示输入和记忆平面之间解析关系的动力系统理论，我们能够演示高维关联数据集的特定记忆过程。在自联想记忆任务中，可以从振荡的神经状态中提取连续流到系统中的一组图像。第二个应用程序处理从句子中嵌入语义记忆成分的过程。结果表明，词汇可以同时回忆多个句子，也可以只回忆一个句子，这取决于它们的语法关系。这意味着该框架易于处理多组具有复合结构的联想记忆。
<details>	<summary>英文摘要</summary>	Spike-timing-dependent plasticity (STDP) is a biological process in which the precise order and timing of neuronal spikes affect the degree of synaptic modification. While there has been numerous research focusing on the role of STDP in neural coding, the functional implications of STDP at the macroscopic level in the brain have not been fully explored yet. In this work, we propose that STDP in an ensemble of spiking neurons renders storing high dimensional information in the form of a `memory plane'. Neural activity based on STDP transforms periodic spatio-temporal input patterns into the corresponding memory plane, where the stored information can be dynamically revived with a proper cue. Using the dynamical systems theory that shows the analytic relation between the input and the memory plane, we were able to demonstrate a specific memory process for high-dimensional associative data sets. In the auto-associative memory task, a group of images that were continuously streamed to the system can be retrieved from the oscillating neural state. The second application deals with the process of semantic memory components that are embedded from sentences. The results show that words can recall multiple sentences simultaneously or one exclusively, depending on their grammatical relations. This implies that the proposed framework is apt to process multiple groups of associative memories with a composite structure. </details>
<details>	<summary>注释</summary>	14 pages of main text followed by 19 pages of supplements. Source for simplified MATLAB programs performing two numerical tests presented in this article can be found in the following link: https://github.com/hkyoon94/NRSTDP.git </details>
<details>	<summary>邮件日期</summary>	2021年04月27日</details>

# 101、基于生物激励优化器的深度神经网络学习
- [ ] Learning in Deep Neural Networks Using a Biologically Inspired Optimizer 
时间：2021年04月23日                         第一作者：Giorgia Dellaferrera                       [链接](https://arxiv.org/abs/2104.11604).                     
## 摘要：众所周知，大脑中的可塑性回路通过突触整合和突触强度的局部调节机制受到突触重量分布的影响。然而，迄今为止设计的大多数人工神经网络训练算法都忽略了刺激依赖性可塑性与局部学习信号之间的复杂相互作用。在这里，我们提出了一个新的生物启发优化人工神经网络（ANNs）和脉冲神经网络（SNNs），它结合了在皮层神经元树突中观察到的突触整合的关键原理：GRAPES（调整错误信号传播的群体责任）。GRAPES在神经网络的每个节点上对误差信号进行依赖于权值分布的调制。结果表明，这种生物激励机制使神经网络的收敛速度得到了系统的提高，并通过前馈结构和递归结构大大提高了神经网络和snn的分类精度。此外，我们证明了GRAPES支持复杂度不断增加的模型的性能可伸缩性，并通过使网络基于先前获得的知识泛化到不可见的任务来减轻灾难性遗忘。GRAPES的本地特性最小化了所需的内存资源，使其最适合专用硬件实现。总的来说，我们的工作表明协调神经生理学和机器智能是提高神经网络性能的关键。
<details>	<summary>英文摘要</summary>	Plasticity circuits in the brain are known to be influenced by the distribution of the synaptic weights through the mechanisms of synaptic integration and local regulation of synaptic strength. However, the complex interplay of stimulation-dependent plasticity with local learning signals is disregarded by most of the artificial neural network training algorithms devised so far. Here, we propose a novel biologically inspired optimizer for artificial (ANNs) and spiking neural networks (SNNs) that incorporates key principles of synaptic integration observed in dendrites of cortical neurons: GRAPES (Group Responsibility for Adjusting the Propagation of Error Signals). GRAPES implements a weight-distribution dependent modulation of the error signal at each node of the neural network. We show that this biologically inspired mechanism leads to a systematic improvement of the convergence rate of the network, and substantially improves classification accuracy of ANNs and SNNs with both feedforward and recurrent architectures. Furthermore, we demonstrate that GRAPES supports performance scalability for models of increasing complexity and mitigates catastrophic forgetting by enabling networks to generalize to unseen tasks based on previously acquired knowledge. The local characteristics of GRAPES minimize the required memory resources, making it optimally suited for dedicated hardware implementations. Overall, our work indicates that reconciling neurophysiology insights with machine intelligence is key to boosting the performance of neural networks. </details>
<details>	<summary>邮件日期</summary>	2021年04月26日</details>

# 100、膜电位和激活阈稳态的持续学习和适应
- [ ] Continuous Learning and Adaptation with Membrane Potential and Activation Threshold Homeostasis 
时间：2021年04月22日                         第一作者：Alex                       [链接](https://arxiv.org/abs/2104.10851).                     
## 摘要：大多数经典（非脉冲）神经网络模型忽略了内部神经元动力学，将神经元视为简单的输入积分器。然而，生物神经元的内部状态受复杂动力学控制，在学习、适应以及整个网络活动和行为中起着至关重要的作用。本文提出了一种膜电位和激活阈稳态（MPATH）神经元模型，该模型结合了多种生物激励机制，用一个类似于生物神经元膜时间常数的参数来有效地模拟神经元内部动力学。该模型允许神经元在输入波动时通过自动调节其活动来维持一种形式的动态平衡。MPATH模型的一个结果是，它给神经元灌输了一种时间感，而不需要反复连接，为建立依赖于神经元活动时间方面的过程模型铺平了道路。实验证明了该模型能够适应并不断地从输入中学习。
<details>	<summary>英文摘要</summary>	Most classical (non-spiking) neural network models disregard internal neuron dynamics and treat neurons as simple input integrators. However, biological neurons have an internal state governed by complex dynamics that plays a crucial role in learning, adaptation and the overall network activity and behaviour. This paper presents the Membrane Potential and Activation Threshold Homeostasis (MPATH) neuron model, which combines several biologically inspired mechanisms to efficiently simulate internal neuron dynamics with a single parameter analogous to the membrane time constant in biological neurons. The model allows neurons to maintain a form of dynamic equilibrium by automatically regulating their activity when presented with fluctuating input. One consequence of the MPATH model is that it imbues neurons with a sense of time without recurrent connections, paving the way for modelling processes that depend on temporal aspects of neuron activity. Experiments demonstrate the model's ability to adapt to and continually learn from its input. </details>
<details>	<summary>注释</summary>	19 pages </details>
<details>	<summary>邮件日期</summary>	2021年04月23日</details>

# 99、具有时间信息的抗噪声深脉冲神经网络
- [ ] Noise-Robust Deep Spiking Neural Networks with Temporal Information 
时间：2021年04月22日                         第一作者：Seongsik Park                       [链接](https://arxiv.org/abs/2104.11169).                     
## 摘要：脉冲神经网络（SNNs）是一种具有时间信息的节能神经网络。SNNs在神经形态器件上表现出了优越的效率，但是这种器件容易受到噪声的影响，阻碍了其在实际应用中的应用。一些研究提高了噪声鲁棒性，但大多数研究既没有考虑深度snn，也没有考虑时间信息。本文采用不同的神经编码方法研究了噪声对深度SNN的影响，提出了一种具有时间信息的抗噪声深度SNN。通过这些方法，我们实现了一个对脉冲删除和抖动有效且鲁棒的深度SNN。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) have emerged as energy-efficient neural networks with temporal information. SNNs have shown a superior efficiency on neuromorphic devices, but the devices are susceptible to noise, which hinders them from being applied in real-world applications. Several studies have increased noise robustness, but most of them considered neither deep SNNs nor temporal information. In this paper, we investigate the effect of noise on deep SNNs with various neural coding methods and present a noise-robust deep SNN with temporal information. With the proposed methods, we have achieved a deep SNN that is efficient and robust to spike deletion and jitter. </details>
<details>	<summary>注释</summary>	Accepted to DAC 2021 </details>
<details>	<summary>邮件日期</summary>	2021年04月23日</details>

# 98、一种用于高能效目标检测的全脉冲混合神经网络
- [ ] A Fully Spiking Hybrid Neural Network for Energy-Efficient Object Detection 
时间：2021年04月21日                         第一作者：Biswadeep Chakraborty                       [链接](https://arxiv.org/abs/2104.10719).                     
## 摘要：该文提出了一种用于资源受限平台中能量有效且鲁棒的目标检测的全脉冲混合神经网络（FSHNN）。该网络结构基于卷积SNN，采用漏泄集成火灾神经元模型。该模型将无监督脉冲时变塑性（STDP）学习与反向传播（STBP）学习方法相结合，并采用蒙特卡罗方法对不确定性误差进行估计。与基于DNN的目标探测器相比，FSHNN提供了更好的精度，同时具有150倍的能效。当受到噪声输入数据和标记较少的训练数据的影响时，它的性能也优于这些目标检测器，具有较低的不确定性误差。
<details>	<summary>英文摘要</summary>	This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for energy-efficient and robust object detection in resource-constrained platforms. The network architecture is based on Convolutional SNN using leaky-integrate-fire neuron models. The model combines unsupervised Spike Time-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning methods and also uses Monte Carlo Dropout to get an estimate of the uncertainty error. FSHNN provides better accuracy compared to DNN based object detectors while being 150X energy-efficient. It also outperforms these object detectors, when subjected to noisy input data and less labeled training data with a lower uncertainty error. </details>
<details>	<summary>注释</summary>	10 pages, Submitted Manuscript </details>
<details>	<summary>邮件日期</summary>	2021年04月23日</details>

# 97、时间模式学习的神经形态算法硬件协同设计
- [ ] Neuromorphic Algorithm-hardware Codesign for Temporal Pattern Learning 
时间：2021年04月21日                         第一作者：Haowen Fang                       [链接](https://arxiv.org/abs/2104.10712).                     
## 摘要：神经形态计算和脉冲神经网络（SNN）模拟生物系统的行为，以其高能量效率完成认知任务的潜力引起了人们的兴趣。然而，时间动力学和脉冲计时等因素对信息处理至关重要，但往往被现有的研究所忽视，限制了神经形态计算的性能和应用。一方面，由于缺乏有效的SNN训练算法，很难利用时态神经动力学。许多现有的算法仍然统计处理神经元激活。另一方面，利用时间神经动力学也对硬件设计提出了挑战。突触表现出时间的动态性，作为保存历史信息的记忆单位，但通常被简化为与重量的联系。目前大多数的模型将突触激活整合到一些存储介质中，以表示膜电位，并在神经元发出脉冲信号后重新设置膜电位。这样做是因为它的硬件简单，只需要一个“清晰”的信号来擦除存储介质，但会破坏存储在神经元中的时间信息。在这项工作中，我们提出了一个有效的训练算法，可以训练SNN学习复杂的时空模式。我们在两个复杂的数据集上获得了有竞争力的精确度。我们还通过一个新的时间模式关联任务证明了该模型的优越性。利用该算法，我们开发了一个基于记忆器的神经元和突触网络的CMOS电路实现，该网络保留了关键的神经动力学特性，降低了复杂度。对神经元模型的电路实现进行了仿真，验证了该模型对具有自适应阈值的时间脉冲模式的反应能力。
<details>	<summary>英文摘要</summary>	Neuromorphic computing and spiking neural networks (SNN) mimic the behavior of biological systems and have drawn interest for their potential to perform cognitive tasks with high energy efficiency. However, some factors such as temporal dynamics and spike timings prove critical for information processing but are often ignored by existing works, limiting the performance and applications of neuromorphic computing. On one hand, due to the lack of effective SNN training algorithms, it is difficult to utilize the temporal neural dynamics. Many existing algorithms still treat neuron activation statistically. On the other hand, utilizing temporal neural dynamics also poses challenges to hardware design. Synapses exhibit temporal dynamics, serving as memory units that hold historical information, but are often simplified as a connection with weight. Most current models integrate synaptic activations in some storage medium to represent membrane potential and institute a hard reset of membrane potential after the neuron emits a spike. This is done for its simplicity in hardware, requiring only a "clear" signal to wipe the storage medium, but destroys temporal information stored in the neuron. In this work, we derive an efficient training algorithm for Leaky Integrate and Fire neurons, which is capable of training a SNN to learn complex spatial temporal patterns. We achieved competitive accuracy on two complex datasets. We also demonstrate the advantage of our model by a novel temporal pattern association task. Codesigned with this algorithm, we have developed a CMOS circuit implementation for a memristor-based network of neuron and synapses which retains critical neural dynamics with reduced complexity. This circuit implementation of the neuron model is simulated to demonstrate its ability to react to temporal spiking patterns with an adaptive threshold. </details>
<details>	<summary>邮件日期</summary>	2021年04月23日</details>

# 96、脉冲神经网络无监督模式识别的权值散度易化原理
- [ ] The principle of weight divergence facilitation for unsupervised pattern recognition in spiking neural networks 
时间：2021年04月20日                         第一作者：Oleg Nikitin                       [链接](https://arxiv.org/abs/2104.09943).                     
## 摘要：信号处理任务和生物神经元之间的相似性使我们理解了输入信号识别的自组织优化原理。在本文中，我们讨论了生物系统和技术系统之间的相似性。我们建议在众所周知的STDP突触可塑性规则的基础上，将权值调整指向与背景噪声和相关信号之间的最大差异相关的状态。物理约束的重量增长原理被用作控制重量变化的基础。有人提出，生物突触的直接修饰受到可塑性发育所需的生化物质的存在和产生的限制。本文利用信噪比信息来控制这类物质的产生和储存，并驱动神经元的突触压力达到信噪比最佳的状态。通过几个不同输入信号模式的实验来了解该方法的功能。
<details>	<summary>英文摘要</summary>	Parallels between the signal processing tasks and biological neurons lead to an understanding of the principles of self-organized optimization of input signal recognition. In the present paper, we discuss such similarities among biological and technical systems. We propose the addition to the well-known STDP synaptic plasticity rule to directs the weight modification towards the state associated with the maximal difference between the background noise and correlated signals. The principle of physically constrained weight growth is used as a basis for such control of the modification of the weights. It is proposed, that biological synaptic straight modification is restricted by the existence and production of bio-chemical 'substances' needed for plasticity development. In this paper, the information about the noise-to-signal ratio is used to control such a substances' production and storage and to drive the neuron's synaptic pressures towards the state with the best signal-to-noise ratio. Several experiments with different input signal regimes are considered to understand the functioning of the proposed approach. </details>
<details>	<summary>注释</summary>	9 pages, 5 figures, submitted to the conference ICANN 2021 MSC-class: 68T05 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年04月21日</details>

# 95、脉冲神经网络的基本组成模型
- [ ] A Basic Compositional Model for Spiking Neural Networks 
时间：2021年04月20日                         第一作者：Nancy Lynch                        [链接](https://arxiv.org/abs/1808.03884).                     
<details>	<summary>邮件日期</summary>	2021年04月21日</details>

# 94、基于原始语音训练的CNNs中间卷积层解释
- [ ] Interpreting intermediate convolutional layers of CNNs trained on raw speech 
时间：2021年04月19日                         第一作者：Ga\v{s}per Begu\v{s}                        [链接](https://arxiv.org/abs/2104.09489).                     
## 摘要：提出了一种基于原始语音数据的CNNs中间层的非监督解释与可视化技术。我们表明，在每个卷积层的ReLU激活后，对特征映射进行平均可以得到可解释的时间序列数据。提出的技术可以对中间卷积层进行声学分析。为了揭示语音中有意义的表征是如何在cnn的中间层被编码的，我们将个体的潜在变量操纵到训练范围之外的边缘水平。我们在两个模型上训练和探索内部表示——一个是裸GAN架构，另一个是ciwGAN扩展，它迫使生成器输出信息数据，并导致出现有语言意义的表示。对语音的三个基本声学特性进行了解释和可视化：周期振动（对应元音）、非周期噪声振动（对应擦音）和沉默（对应停止）。我们还认为，所提出的技术允许对中间层进行声学分析，这与对人类语音数据的声学分析类似：我们可以从中间层提取F0、强度、持续时间、共振峰和其他声学特性，以便测试CNNs在何处以及如何编码各种类型的信息。这些模型是在两个不同复杂度的语音过程上训练的：一个简单的[s]和一个计算复杂的重叠（复制材料）。观察插值和中间层变化之间的因果关系可以揭示单个变量如何转化为中间层激活的脉冲。利用这种技术，我们可以分析语音中有意义的单位是如何在不同的卷积层中被编码的。
<details>	<summary>英文摘要</summary>	This paper presents a technique to interpret and visualize intermediate layers in CNNs trained on raw speech data in an unsupervised manner. We show that averaging over feature maps after ReLU activation in each convolutional layer yields interpretable time-series data. The proposed technique enables acoustic analysis of intermediate convolutional layers. To uncover how meaningful representation in speech gets encoded in intermediate layers of CNNs, we manipulate individual latent variables to marginal levels outside of the training range. We train and probe internal representations on two models -- a bare GAN architecture and a ciwGAN extension which forces the Generator to output informative data and results in emergence of linguistically meaningful representations. Interpretation and visualization is performed for three basic acoustic properties of speech: periodic vibration (corresponding to vowels), aperiodic noise vibration (corresponding to fricatives), and silence (corresponding to stops). We also argue that the proposed technique allows acoustic analysis of intermediate layers that parallels the acoustic analysis of human speech data: we can extract F0, intensity, duration, formants, and other acoustic properties from intermediate layers in order to test where and how CNNs encode various types of information. The models are trained on two speech processes with different degrees of complexity: a simple presence of [s] and a computationally complex presence of reduplication (copied material). Observing the causal effect between interpolation and the resulting changes in intermediate layers can reveal how individual variables get transformed into spikes in activation in intermediate layers. Using the proposed technique, we can analyze how linguistically meaningful units in speech get encoded in different convolutional layers. </details>
<details>	<summary>邮件日期</summary>	2021年04月20日</details>

# 93、一种新的视觉处理器神经元模型
- [ ] A Novel Neuron Model of Visual Processor 
时间：2021年04月15日                         第一作者：Jizhao Liu                       [链接](https://arxiv.org/abs/2104.07257).                     
## 摘要：模拟和模仿人类或哺乳动物的神经网络是模式识别和计算机视觉领域多年来研究的热点。受猫初级视皮层神经元传导特性的启发，脉冲耦合神经网络（PCNNs）可以表现出同步振荡行为，无需训练即可处理数字图像。然而，根据对猫初级视皮层单个细胞的研究，当一个神经元受到外部周期性信号的刺激时，脉冲间间隔（ISI）分布表现为多峰分布。这种现象不能用所有的PCNN模型来解释。通过分析PCNN的工作机制，提出了一种新的由连续耦合神经网络（CCNN）构成的初级视皮层神经元模型。该模型继承了原PCNN模型的阈值指数衰减和同步脉冲振荡特性，能表现出与猫初级视皮层神经元测试结果一致的混沌行为。因此，我们的CCNN模型更接近真实的视觉神经网络。对于图像分割任务，基于CCNN模型的算法比现有的视觉皮层神经网络模型具有更好的性能。我们的方法的优势在于，它有助于神经生理学家进一步了解初级视觉皮层是如何工作的，并可用于定量预测真实神经网络的时空行为。CCNN还可能激励工程师为人工智能目的创建大脑启发的深度学习网络。
<details>	<summary>英文摘要</summary>	Simulating and imitating the neuronal network of humans or mammals is a popular topic that has been explored for many years in the fields of pattern recognition and computer vision. Inspired by neuronal conduction characteristics in the primary visual cortex of cats, pulse-coupled neural networks (PCNNs) can exhibit synchronous oscillation behavior, which can process digital images without training. However, according to the study of single cells in the cat primary visual cortex, when a neuron is stimulated by an external periodic signal, the interspike-interval (ISI) distributions represent a multimodal distribution. This phenomenon cannot be explained by all PCNN models. By analyzing the working mechanism of the PCNN, we present a novel neuron model of the primary visual cortex consisting of a continuous-coupled neural network (CCNN). Our model inherited the threshold exponential decay and synchronous pulse oscillation property of the original PCNN model, and it can exhibit chaotic behavior consistent with the testing results of cat primary visual cortex neurons. Therefore, our CCNN model is closer to real visual neural networks. For image segmentation tasks, the algorithm based on CCNN model has better performance than the state-of-art of visual cortex neural network model. The strength of our approach is that it helps neurophysiologists further understand how the primary visual cortex works and can be used to quantitatively predict the temporal-spatial behavior of real neural networks. CCNN may also inspire engineers to create brain-inspired deep learning networks for artificial intelligence purposes. </details>
<details>	<summary>邮件日期</summary>	2021年04月16日</details>

# 92、与神经形态处理器兼容的误差传播脉冲神经网络
- [ ] An error-propagation spiking neural network compatible with neuromorphic processors 
时间：2021年04月12日                         第一作者：Matteo Cartiglia                       [链接](https://arxiv.org/abs/2104.05241).                     
## 摘要：脉冲神经网络在低功耗感知处理和边缘计算硬件平台的设计中显示出巨大的潜力。然而，在这种结构上实现片上学习算法仍然是一个开放的挑战，特别是对于依赖于反向传播算法的多层网络。本文提出了一种基于峰值的学习方法，该方法利用局部权值更新机制来逼近反向传播，并与模拟/数字混合神经形态电路兼容。我们介绍了一种网络结构，它使突触权值更新机制能够跨层反向传播错误信号，并提出了一种网络，该网络可以被训练来区分具有相同平均放电率但不同脉冲计时的两种基于脉冲的模式。这项工作是朝着设计超低功耗混合信号神经形态处理系统迈出的第一步，该系统具有片上学习电路，可被训练以识别不同的脉冲活动时空模式（例如，由基于事件的视觉或听觉传感器产生）。
<details>	<summary>英文摘要</summary>	Spiking neural networks have shown great promise for the design of low-power sensory-processing and edge-computing hardware platforms. However, implementing on-chip learning algorithms on such architectures is still an open challenge, especially for multi-layer networks that rely on the back-propagation algorithm. In this paper, we present a spike-based learning method that approximates back-propagation using local weight update mechanisms and which is compatible with mixed-signal analog/digital neuromorphic circuits. We introduce a network architecture that enables synaptic weight update mechanisms to back-propagate error signals across layers and present a network that can be trained to distinguish between two spike-based patterns that have identical mean firing rates, but different spike-timings. This work represents a first step towards the design of ultra-low power mixed-signal neuromorphic processing systems with on-chip learning circuits that can be trained to recognize different spatio-temporal patterns of spiking activity (e.g. produced by event-based vision or auditory sensors). </details>
<details>	<summary>注释</summary>	2020 2nd IEEE International Conference on Artificial Intelligence Circuits and Systems (AICAS) </details>
<details>	<summary>邮件日期</summary>	2021年04月13日</details>

# 91、实值输入到脉冲序列的自适应转换
- [ ] Adaptive conversion of real-valued input into spike trains 
时间：2021年04月12日                         第一作者：Alex                       [链接](https://arxiv.org/abs/2104.05401).                     
## 摘要：本文提出了一种生物学上可行的方法，将实值输入转化为脉冲序列，用脉冲神经网络进行处理。所提出的方法模仿视网膜神经节细胞的适应性行为，并允许输入神经元适应其对输入统计量变化的反应。因此，输入层不是被动地接收值并将其转发到隐藏层和输出层，而是充当一个自我调节滤波器，它强调与平均值的偏差，同时允许输入神经元对平均值本身有效地脱敏。该方法的另一个优点是每个变量只需要一个输入神经元，而不是像常用的基于高斯感受野的转换方法那样需要整个神经元群。此外，由于输入的统计数据随着时间的推移而自然出现，因此在将数据馈送到网络之前不必对其进行预处理。这使得脉冲神经网络能够处理原始的、非标准化的流数据。通过概念验证实验验证了该方法的有效性。
<details>	<summary>英文摘要</summary>	This paper presents a biologically plausible method for converting real-valued input into spike trains for processing with spiking neural networks. The proposed method mimics the adaptive behaviour of retinal ganglion cells and allows input neurons to adapt their response to changes in the statistics of the input. Thus, rather than passively receiving values and forwarding them to the hidden and output layers, the input layer acts as a self-regulating filter which emphasises deviations from the average while allowing the input neurons to become effectively desensitised to the average itself. Another merit of the proposed method is that it requires only one input neuron per variable, rather than an entire population of neurons as in the case of the commonly used conversion method based on Gaussian receptive fields. In addition, since the statistics of the input emerge naturally over time, it becomes unnecessary to pre-process the data before feeding it to the network. This enables spiking neural networks to process raw, non-normalised streaming data. A proof-of-concept experiment is performed to demonstrate that the proposed method operates as expected. </details>
<details>	<summary>注释</summary>	8 pages Journal-ref: 2016 International Joint Conference on Neural Networks DOI: 10.1109/IJCNN.2016.7727314 </details>
<details>	<summary>邮件日期</summary>	2021年04月13日</details>

# 90、PrivateSNN：完全隐私保护的脉冲神经网络
- [ ] PrivateSNN: Fully Privacy-Preserving Spiking Neural Networks 
时间：2021年04月07日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2104.03414).                     
## 摘要：我们如何在边缘设备上为神经系统带来隐私和能源效率？在本文中，我们提出了PrivateSNN，其目的是从预先训练好的ANN模型中构建低功耗的脉冲神经网络（SNNs），而不泄漏数据集中包含的敏感信息。在这里，我们解决两种类型的泄漏问题：1）在ANN-SNN转换过程中，网络访问真实训练数据时引起的数据泄漏。2） 类泄漏是由网络参数重构类相关特征时产生的泄漏的概念。为了解决数据泄漏问题，我们从预训练的神经网络生成合成图像，并利用生成的图像将神经网络转换为snn。然而，由于权重参数相对于ANN参数具有相同（或缩放）的值，转换后的snn对于类泄漏仍然是脆弱的。因此，我们通过使用基于时间脉冲的学习规则训练SNN来加密SNN权重。利用时间数据更新权值参数，使得网络在空间域难以解释。我们观察到，加密的PrivateSNN不仅可以在没有巨大性能下降（小于~5%）的情况下实现，而且具有显著的能量效率增益（与标准ANN相比大约x60）。我们在各种数据集上进行了广泛的实验，包括CIFAR10、CIFAR100和tinyimagnet，强调了隐私保护SNN训练的重要性。
<details>	<summary>英文摘要</summary>	How can we bring both privacy and energy-efficiency to a neural system on edge devices? In this paper, we propose PrivateSNN, which aims to build low-power Spiking Neural Networks (SNNs) from a pre-trained ANN model without leaking sensitive information contained in a dataset. Here, we tackle two types of leakage problems: 1) Data leakage caused when the networks access real training data during an ANN-SNN conversion process. 2) Class leakage is the concept of leakage caused when class-related features can be reconstructed from network parameters. In order to address the data leakage issue, we generate synthetic images from the pre-trained ANNs and convert ANNs to SNNs using generated images. However, converted SNNs are still vulnerable with respect to the class leakage since the weight parameters have the same (or scaled) value with respect to ANN parameters. Therefore, we encrypt SNN weights by training SNNs with a temporal spike-based learning rule. Updating weight parameters with temporal data makes networks difficult to be interpreted in the spatial domain. We observe that the encrypted PrivateSNN can be implemented not only without the huge performance drop (less than ~5%) but also with significant energy-efficiency gain (about x60 compared to the standard ANN). We conduct extensive experiments on various datasets including CIFAR10, CIFAR100, and TinyImageNet, highlighting the importance of privacy-preserving SNN training. </details>
<details>	<summary>邮件日期</summary>	2021年04月09日</details>

# 89、基于神经形态立体视觉系统的实时立体深度估计
- [ ] Instantaneous Stereo Depth Estimation of Real-World Stimuli with a Neuromorphic Stereo-Vision Setup 
时间：2021年04月06日                         第一作者：Nicoletta Risi                       [链接](https://arxiv.org/abs/2104.02541).                     
## 摘要：在生物学中，立体匹配问题得到了有效的解决，即在两个不同的视图中匹配相应的特征来重建深度。然而，它仍然是经典机器视觉方法的计算瓶颈。利用事件摄像机的特性，最近提出的立体视觉脉冲神经网络（SNN）结构有可能简化立体匹配问题。一些结合事件摄像机和基于峰值的神经形态处理器的解决方案已经存在。然而，他们要么在数字硬件上模拟，要么在简化的刺激物上测试。在这项工作中，我们使用动态视觉传感器3D人体姿势数据集（DHP19）来验证一个基于大脑启发事件的立体匹配架构，该架构是在一个混合信号神经形态处理器上实现的，并具有真实世界的数据。我们的实验表明，这种由重合检测器和视差敏感神经元组成的SNN结构能够在瞬间提供对输入视差的粗略估计，从而实时检测到深度移动的刺激的存在。
<details>	<summary>英文摘要</summary>	The stereo-matching problem, i.e., matching corresponding features in two different views to reconstruct depth, is efficiently solved in biology. Yet, it remains the computational bottleneck for classical machine vision approaches. By exploiting the properties of event cameras, recently proposed Spiking Neural Network (SNN) architectures for stereo vision have the potential of simplifying the stereo-matching problem. Several solutions that combine event cameras with spike-based neuromorphic processors already exist. However, they are either simulated on digital hardware or tested on simplified stimuli. In this work, we use the Dynamic Vision Sensor 3D Human Pose Dataset (DHP19) to validate a brain-inspired event-based stereo-matching architecture implemented on a mixed-signal neuromorphic processor with real-world data. Our experiments show that this SNN architecture, composed of coincidence detectors and disparity sensitive neurons, is able to provide a coarse estimate of the input disparity instantaneously, thereby detecting the presence of a stimulus moving in depth in real-time. </details>
<details>	<summary>邮件日期</summary>	2021年04月07日</details>

# 88、乘性突触脉冲神经网络的非线性计算
- [ ] Nonlinear computations in spiking neural networks through multiplicative synapses 
时间：2021年03月31日                         第一作者：Michele Nardin                       [链接](https://arxiv.org/abs/2009.03857).                     
<details>	<summary>邮件日期</summary>	2021年04月01日</details>

# 87、在BrainScaleS-2移动系统上演示模拟推理
- [ ] Demonstrating Analog Inference on the BrainScaleS-2 Mobile System 
时间：2021年03月29日                         第一作者：Yannik Stradmann                       [链接](https://arxiv.org/abs/2103.15960).                     
## 摘要：我们提出了BrainScaleS-2mobile系统作为一个基于BrainScaleS-2asic的小型模拟推理机，并展示了它在医学心电图数据集分类方面的能力。利用ASIC的模拟网络核心实现了卷积深度神经网络的乘法累加运算。我们测量了ASIC的总能量消耗192uJ，并实现了每个心电图患者样本276us的分类时间。房颤患者在14.0（10%）假阳性时的检出率为93.7%（7%）。该系统具有体积小、功耗高、I/O灵活等优点，可直接应用于边缘推理应用。未来可能的应用还可以在一个BrainScaleS-2asic上，将传统的机器学习层与神经网络的在线学习结合起来。该系统已成功参与德国联邦教育和研究部（BMBF）独立评审的“Pilotinnovationswettbewerb‘Energieeffizientes KI system’”竞赛，并被证明运行可靠。
<details>	<summary>英文摘要</summary>	We present the BrainScaleS-2 mobile system as a compact analog inference engine based on the BrainScaleS-2 ASIC and demonstrate its capabilities at classifying a medical electrocardiogram dataset. The analog network core of the ASIC is utilized to perform the multiply-accumulate operations of a convolutional deep neural network. We measure a total energy consumption of 192uJ for the ASIC and achieve a classification time of 276us per electrocardiographic patient sample. Patients with atrial fibrillation are correctly identified with a detection rate of 93.7(7)% at 14.0(10)% false positives. The system is directly applicable to edge inference applications due to its small size, power envelope and flexible I/O capabilities. Possible future applications can furthermore combine conventional machine learning layers with online-learning in spiking neural networks on a single BrainScaleS-2 ASIC. The system has successfully participated and proven to operate reliably in the independently judged competition "Pilotinnovationswettbewerb 'Energieeffizientes KI-System'" of the German Federal Ministry of Education and Research (BMBF). </details>
<details>	<summary>邮件日期</summary>	2021年03月31日</details>

# 86、基于峰值模式识别的蛋白质结构库计算
- [ ] Protein Structured Reservoir computing for Spike-based Pattern Recognition 
时间：2021年03月29日                         第一作者：Karolos-Alex                       [链接](https://arxiv.org/abs/2008.03330).                     
<details>	<summary>注释</summary>	15 pages, 9 figures DOI: 10.1109/TPDS.2021.3068826 </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 85、能量衰减网络（EDeN）
- [ ] Energy Decay Network (EDeN) 
时间：2021年03月10日                         第一作者：Jamie Nicholas Shelley                       [链接](https://arxiv.org/abs/2103.15552).                     
## 摘要：本文和伴随的Python和C++框架是作者感知到的问题与狭窄的（基于歧视的）人工智能的产物。（人工智能）该框架试图通过使用共同的调节/交换值（能量）的潜在结构表达来开发经验的遗传转移，从而创建一个模型，通过遗传和实时信号处理影响，神经结构和所有单元过程相互依赖地共同发展；成功的路径是由每个时期的穗分布的稳定性来定义的，它受遗传编码的形态发育的影响偏见。这些这些原则的目的是建立一个多样化和强大的网络，能够适应一般的任务，通过在一个旨在将学习转移到其他领域的模拟训练规模的媒介。
<details>	<summary>英文摘要</summary>	This paper and accompanying Python and C++ Framework is the product of the authors perceived problems with narrow (Discrimination based) AI. (Artificial Intelligence) The Framework attempts to develop a genetic transfer of experience through potential structural expressions using a common regulation/exchange value (energy) to create a model whereby neural architecture and all unit processes are co-dependently developed by genetic and real time signal processing influences; successful routes are defined by stability of the spike distribution per epoch which is influenced by genetically encoded morphological development biases.These principles are aimed towards creating a diverse and robust network that is capable of adapting to general tasks by training within a simulation designed for transfer learning to other mediums at scale. </details>
<details>	<summary>邮件日期</summary>	2021年03月30日</details>

# 84、利用脉冲间隔对脉冲神经网络的直观解释
- [ ] Visual Explanations from Spiking Neural Networks using Interspike Intervals 
时间：2021年03月26日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2103.14441).                     
## 摘要：脉冲神经网络（SNNs）计算并与异步二进制时态事件进行通信，通过神经形态硬件可以显著节省能量。最近在训练snn方面的算法研究表明，snn在各种分类任务上都有很好的性能。然而，目前还没有一种可视化的工具来分析和解释这种时间深度SNNs的内部脉冲行为。在这篇论文中，我们提出了一个新的概念，为SNNs的生物合理的可视化，称为峰激活图（SAM）。提出的SAM避免了脉冲神经元的不可微性，不需要计算梯度来获得直观的解释。相反，SAM通过在不同的时间步长上向前传播输入峰值来计算时间可视化图。SAM通过突出显示具有短峰间期活动的神经元，产生与输入数据的每个时间步相对应的注意图。有趣的是，没有反向传播过程和类标签，SAM在捕获细粒度细节的同时突出了图像的区分区域。利用SAM，我们首次全面分析了内部脉冲在各种SNN训练配置中的工作方式，这取决于优化类型、泄漏行为，以及面对对手的例子时。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) compute and communicate with asynchronous binary temporal events that can lead to significant energy savings with neuromorphic hardware. Recent algorithmic efforts on training SNNs have shown competitive performance on a variety of classification tasks. However, a visualization tool for analysing and explaining the internal spike behavior of such temporal deep SNNs has not been explored. In this paper, we propose a new concept of bio-plausible visualization for SNNs, called Spike Activation Map (SAM). The proposed SAM circumvents the non-differentiable characteristic of spiking neurons by eliminating the need for calculating gradients to obtain visual explanations. Instead, SAM calculates a temporal visualization map by forward propagating input spikes over different time-steps. SAM yields an attention map corresponding to each time-step of input data by highlighting neurons with short inter-spike interval activity. Interestingly, without both the backpropagation process and the class label, SAM highlights the discriminative region of the image while capturing fine-grained details. With SAM, for the first time, we provide a comprehensive analysis on how internal spikes work in various SNN training configurations depending on optimization types, leak behavior, as well as when faced with adversarial examples. </details>
<details>	<summary>邮件日期</summary>	2021年03月29日</details>

# 83、基于粗糙度指数和粗糙距离的医学图像分割基准研究
- [ ] Roughness Index and Roughness Distance for Benchmarking Medical Segmentation 
时间：2021年03月23日                         第一作者：Vidhiwar Singh Rathour                       [链接](https://arxiv.org/abs/2103.12350).                     
## 摘要：医学图像分割是医学图像分析中最具挑战性的任务之一，在许多临床应用中得到了广泛的发展。现有的大多数度量方法都是先为自然图像设计，然后扩展到医学图像。虽然物体表面在医学分割和定量分析中起着重要的作用，即分析脑肿瘤表面、测量灰质体积，但现有的大多数测量方法在分析物体表面时，特别是在判断给定体积物体的表面光滑度或粗糙度或分析其表面粗糙度时，都是有限的拓扑错误。本文首先分析了现有的各种医学图像分割方法的优缺点，特别是对体数据的分割。在此基础上，提出了一个适合于医学图像分割分析和评价的粗糙度指数和粗糙度距离。我们提出的方法解决了两类分割错误，即（i）边界/曲面上的拓扑错误和（ii）边界/曲面上的不规则性。这项工作的贡献是四个方面：（i）检测表面上的不规则脉冲/孔，（ii）提出粗糙度指数来测量给定物体的表面粗糙度，（iii）提出粗糙度距离来测量两个边界/表面之间的距离，利用提出的粗糙度指数和（iv）提出一种有助于去除的算法使表面光滑的不规则的尖刺/孔。我们提出的粗糙度指数和粗糙度距离是建立在固体表面粗糙度参数已在土木工程中成功开发。
<details>	<summary>英文摘要</summary>	Medical image segmentation is one of the most challenging tasks in medical image analysis and has been widely developed for many clinical applications. Most of the existing metrics have been first designed for natural images and then extended to medical images. While object surface plays an important role in medical segmentation and quantitative analysis i.e. analyze brain tumor surface, measure gray matter volume, most of the existing metrics are limited when it comes to analyzing the object surface, especially to tell about surface smoothness or roughness of a given volumetric object or to analyze the topological errors. In this paper, we first analysis both pros and cons of all existing medical image segmentation metrics, specially on volumetric data. We then propose an appropriate roughness index and roughness distance for medical image segmentation analysis and evaluation. Our proposed method addresses two kinds of segmentation errors, i.e. (i)topological errors on boundary/surface and (ii)irregularities on the boundary/surface. The contribution of this work is four-fold: (i) detect irregular spikes/holes on a surface, (ii) propose roughness index to measure surface roughness of a given object, (iii) propose a roughness distance to measure the distance of two boundaries/surfaces by utilizing the proposed roughness index and (iv) suggest an algorithm which helps to remove the irregular spikes/holes to smooth the surface. Our proposed roughness index and roughness distance are built upon the solid surface roughness parameter which has been successfully developed in the civil engineering. </details>
<details>	<summary>注释</summary>	Paper has been accepted at BIOIMAGING2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 82、论系统软件在神经形态计算能量管理中的作用
- [ ] On the Role of System Software in Energy Management of Neuromorphic Computing 
时间：2021年03月22日                         第一作者：Twisha Titirsha                       [链接](https://arxiv.org/abs/2103.12231).                     
## 摘要：最近，DYNAPs和Loihi等神经形态计算系统被引入计算机界，以提高机器学习程序的性能和能量效率，特别是那些使用脉冲神经网络（Spiking Neural Network，SNN）实现的程序。神经形态系统的系统软件的作用是对一个大型机器学习模型（例如，有许多神经元和突触）进行聚类，并将这些聚类映射到硬件的计算资源。在这项工作中，我们建立了一个神经形态硬件的能量消耗，考虑了神经元和突触所消耗的能量，以及互连上通信脉冲所消耗的能量。基于这样的公式，我们首先评估了系统软件在管理神经形态系统能量消耗方面的作用。接下来，我们提出一个简单的启发式映射方法，将神经元和突触放置到计算资源上，以减少能量消耗。我们用10个机器学习应用来评估我们的方法，并证明所提出的映射方法可以显著降低神经形态计算系统的能量消耗。
<details>	<summary>英文摘要</summary>	Neuromorphic computing systems such as DYNAPs and Loihi have recently been introduced to the computing community to improve performance and energy efficiency of machine learning programs, especially those that are implemented using Spiking Neural Network (SNN). The role of a system software for neuromorphic systems is to cluster a large machine learning model (e.g., with many neurons and synapses) and map these clusters to the computing resources of the hardware. In this work, we formulate the energy consumption of a neuromorphic hardware, considering the power consumed by neurons and synapses, and the energy consumed in communicating spikes on the interconnect. Based on such formulation, we first evaluate the role of a system software in managing the energy consumption of neuromorphic systems. Next, we formulate a simple heuristic-based mapping approach to place the neurons and synapses onto the computing resources to reduce energy consumption. We evaluate our approach with 10 machine learning applications and demonstrate that the proposed mapping approach leads to a significant reduction of energy consumption of neuromorphic computing systems. </details>
<details>	<summary>注释</summary>	To appear in 18th Computer Frontiers 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 81、皮层振荡在脉冲神经网络中实现了基于采样的计算
- [ ] Cortical oscillations implement a backbone for sampling-based computation in spiking neural networks 
时间：2021年03月22日                         第一作者：Agnes Korcsak-Gorzo                       [链接](https://arxiv.org/abs/2006.11099).                     
<details>	<summary>注释</summary>	30 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 80、融合流网：使用传感器融合和深度融合脉冲模拟网络架构的节能光流估计
- [ ] Fusion-FlowNet: Energy-Efficient Optical Flow Estimation using Sensor Fusion and Deep Fused Spiking-Analog Network Architectures 
时间：2021年03月19日                         第一作者：Chankyu Lee                       [链接](https://arxiv.org/abs/2103.10592).                     
## 摘要：标准的基于帧的相机采样光强度帧严重影响了高速运动的运动模糊，当动态范围较大时，无法准确地感知场景。另一方面，基于事件的相机通过异步检测单个像素强度的变化来克服这些限制。但是，事件摄影机仅提供运动中像素的信息，导致数据稀疏。因此，估计像素的整体密集行为是困难的。为了解决与传感器相关的问题，我们提出了Fusion FlowNet，一个传感器融合框架，用于利用基于帧和基于事件的传感器的能量高效光流估计，利用它们的互补特性。我们提出的网络结构也是一个融合了脉冲神经网络（SNNs）和模拟神经网络（ANNs）的网络，其中每个网络分别被设计为同时处理异步事件流和基于规则帧的图像。我们的网络使用无监督学习进行端到端训练，以避免昂贵的视频注释。该方法在不同的环境（快速运动和具有挑战性的光照条件）下具有良好的通用性，并在多车辆立体事件相机（MVSEC）数据集上展示了最先进的光流预测。此外，我们的网络在网络参数数量和计算能量成本方面提供了大量的节省。
<details>	<summary>英文摘要</summary>	Standard frame-based cameras that sample light intensity frames are heavily impacted by motion blur for high-speed motion and fail to perceive scene accurately when the dynamic range is high. Event-based cameras, on the other hand, overcome these limitations by asynchronously detecting the variation in individual pixel intensities. However, event cameras only provide information about pixels in motion, leading to sparse data. Hence, estimating the overall dense behavior of pixels is difficult. To address such issues associated with the sensors, we present Fusion-FlowNet, a sensor fusion framework for energy-efficient optical flow estimation using both frame- and event-based sensors, leveraging their complementary characteristics. Our proposed network architecture is also a fusion of Spiking Neural Networks (SNNs) and Analog Neural Networks (ANNs) where each network is designed to simultaneously process asynchronous event streams and regular frame-based images, respectively. Our network is end-to-end trained using unsupervised learning to avoid expensive video annotations. The method generalizes well across distinct environments (rapid motion and challenging lighting conditions) and demonstrates state-of-the-art optical flow prediction on the Multi-Vehicle Stereo Event Camera (MVSEC) dataset. Furthermore, our network offers substantial savings in terms of the number of network parameters and computational energy cost. </details>
<details>	<summary>邮件日期</summary>	2021年03月22日</details>

# 79、皮层振荡在脉冲神经网络中实现了基于采样的计算
- [ ] Cortical oscillations implement a backbone for sampling-based computation in spiking neural networks 
时间：2021年03月19日                         第一作者：Agnes Korcsak-Gorzo                       [链接](https://arxiv.org/abs/2006.11099).                     
<details>	<summary>注释</summary>	33 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月22日</details>

# 78、基于自适应脉冲递归神经网络的精确高效时域分类
- [ ] Accurate and efficient time-domain classification with adaptive spiking recurrent neural networks 
时间：2021年03月12日                         第一作者：Bojian Yin                       [链接](https://arxiv.org/abs/2103.12593).                     
## 摘要：受生物神经元更详细模型的启发，脉冲神经网络（Spiking neural networks，SNNs）作为一种更具生物合理性和潜在更强大的神经计算模型被研究，其目的是提取生物神经元的能量效率；然而，与传统的人工神经网络（ANNs）相比，这种网络的性能仍然很差。在这里，我们展示了一个新的替代梯度与可调和自适应脉冲神经元的循环网络相结合如何在时域的挑战性基准（如语音和手势识别）上产生snn的最新技术。这也超过了标准的经典递归神经网络（RNN）的性能，接近现代最好的人工神经网络。由于这些snn表现出稀疏脉冲，我们证明它们在理论上比具有类似性能的rnn计算效率高一到三个数量级。总之，SNNs是人工智能硬件实现的一个有吸引力的解决方案。
<details>	<summary>英文摘要</summary>	Inspired by more detailed modeling of biological neurons, Spiking neural networks (SNNs) have been investigated both as more biologically plausible and potentially more powerful models of neural computation, and also with the aim of extracting biological neurons' energy efficiency; the performance of such networks however has remained lacking compared to classical artificial neural networks (ANNs). Here, we demonstrate how a novel surrogate gradient combined with recurrent networks of tunable and adaptive spiking neurons yields state-of-the-art for SNNs on challenging benchmarks in the time-domain, like speech and gesture recognition. This also exceeds the performance of standard classical recurrent neural networks (RNNs) and approaches that of the best modern ANNs. As these SNNs exhibit sparse spiking, we show that they theoretically are one to three orders of magnitude more computationally efficient compared to RNNs with comparable performance. Together, this positions SNNs as an attractive solution for AI hardware implementations. </details>
<details>	<summary>注释</summary>	11 pages </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 77、脉冲神经元的线性约束学习
- [ ] Linear Constraints Learning for Spiking Neurons 
时间：2021年03月10日                         第一作者：Huy Le Nguyen                       [链接](https://arxiv.org/abs/2103.12564).                     
## 摘要：利用脉冲编码神经元对信息进行精确的脉冲定时编码，已经被证明比速率编码方法具有更强的计算能力。然而，现有的针对脉冲神经元的监督学习算法大多比较复杂，时间复杂度较差。针对这些局限性，我们提出了一种有监督的多脉冲学习算法，减少了所需的训练迭代次数。我们通过将大量的权值更新描述为一个线性约束满足问题来实现这一点，可以有效地解决这个问题。实验结果表明，该方法在MNIST数据集上比现有算法具有更好的效率。此外，我们提供了LIF神经元模型的分类能力的实验结果，与系统的几个参数有关。
<details>	<summary>英文摘要</summary>	Encoding information with precise spike timings using spike-coded neurons has been shown to be more computationally powerful than rate-coded approaches. However, most existing supervised learning algorithms for spiking neurons are complicated and offer poor time complexity. To address these limitations, we propose a supervised multi-spike learning algorithm which reduces the required number of training iterations. We achieve this by formulating a large number of weight updates as a linear constraint satisfaction problem, which can be solved efficiently. Experimental results show this method offers better efficiency compared to existing algorithms on the MNIST dataset. Additionally, we provide experimental results on the classification capacity of the LIF neuron model, relative to several parameters of the system. </details>
<details>	<summary>注释</summary>	17 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月24日</details>

# 76、约束塑性储备作为神经网络频率和权值控制的一种自然方法
- [ ] Constrained plasticity reserve as a natural way to control frequency and weights in spiking neural networks 
时间：2021年03月15日                         第一作者：Oleg Nikitin                        [链接](https://arxiv.org/abs/2103.08143).                     
## 摘要：生物神经元具有自适应性，并进行复杂的计算，包括过滤冗余信息。这种处理通常与贝叶斯推理相联系。然而，最常见的神经细胞模型，包括生物学上合理的模型，如霍奇金-赫胥黎或伊兹克维奇，在单个细胞水平上并不具备预测动力学。现代的突触可塑性或互联适应规则也不能为神经元适应不断变化的输入信号强度的能力提供基础。虽然自然神经元突触生长受到蛋白质供应和循环的精确控制和限制，但广泛使用的STDP等重量校正规则在变化率和范围上是有效的无限制的。在这篇文章中，我们将介绍一种新的机制，通过抽象蛋白质储备限制的STDP生长，并通过细胞内优化算法来控制神经元放电率稳态和重量变化之间的联系。我们将展示，这些细胞动力学如何帮助神经元过滤强烈的信号，帮助神经元保持稳定的放电频率。我们还将检验这种滤波不影响神经元在无监督模式下识别相关输入的能力。这种方法可用于机器学习领域，以提高人工智能系统的鲁棒性。
<details>	<summary>英文摘要</summary>	Biological neurons have adaptive nature and perform complex computations involving the filtering of redundant information. Such processing is often associated with Bayesian inference. Yet most common models of neural cells, including biologically plausible, such as Hodgkin-Huxley or Izhikevich do not possess predictive dynamics on the level of a single cell. The modern rules of synaptic plasticity or interconnections weights adaptation also do not provide grounding for the ability of neurons to adapt to the ever-changing input signal intensity. While natural neuron synaptic growth is precisely controlled and restricted by protein supply and recycling, weight correction rules such as widely used STDP are efficiently unlimited in change rate and scale. In the present article, we will introduce new mechanics of interconnection between neuron firing rate homeostasis and weight change by means of STDP growth bounded by abstract protein reserve, controlled by the intracellular optimization algorithm. We will show, how these cellular dynamics help neurons to filter out the intense signals to help neurons keep a stable firing rate. We will also examine that such filtering does not affect the ability of neurons to recognize the correlated inputs in unsupervised mode. Such an approach might be used in the machine learning domain to improve the robustness of AI systems. </details>
<details>	<summary>注释</summary>	24 pages, 12 figures MSC-class: 68T05 ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年03月16日</details>

# 75、事件摄影机的时间顺序最近事件（TORE）卷
- [ ] Time-Ordered Recent Event (TORE) Volumes for Event Cameras 
时间：2021年03月10日                         第一作者：R. Wes Baldwin                       [链接](https://arxiv.org/abs/2103.06108).                     
## 摘要：事件摄像机是一种令人兴奋的新型传感器，能够以极低的延迟和宽的动态范围实现高速成像。不幸的是，大多数机器学习体系结构都不是直接处理稀疏数据的，比如从事件摄像机生成的数据。许多最先进的事件摄像机算法依赖于内插事件表示法——模糊了关键的定时信息，增加了数据量，限制了整体网络性能。本文详细介绍了一种称为时间顺序最近事件（TORE）卷的事件表示。存储卷被设计成以最小的信息丢失紧凑地存储原始峰值定时信息。这种仿生设计内存效率高，计算速度快，避免时间阻塞（即固定和预定义的帧速率），并包含来自过去数据的“本地内存”。该设计在一系列具有挑战性的任务（如事件去噪、图像重建、分类和人体姿态估计）上进行了评估，并显示出显著提高了最先进的性能。存储卷是当前使用事件表示的任何算法的易于实现的替代品。
<details>	<summary>英文摘要</summary>	Event cameras are an exciting, new sensor modality enabling high-speed imaging with extremely low-latency and wide dynamic range. Unfortunately, most machine learning architectures are not designed to directly handle sparse data, like that generated from event cameras. Many state-of-the-art algorithms for event cameras rely on interpolated event representations - obscuring crucial timing information, increasing the data volume, and limiting overall network performance. This paper details an event representation called Time-Ordered Recent Event (TORE) volumes. TORE volumes are designed to compactly store raw spike timing information with minimal information loss. This bio-inspired design is memory efficient, computationally fast, avoids time-blocking (i.e. fixed and predefined frame rates), and contains "local memory" from past data. The design is evaluated on a wide range of challenging tasks (e.g. event denoising, image reconstruction, classification, and human pose estimation) and is shown to dramatically improve state-of-the-art performance. TORE volumes are an easy-to-implement replacement for any algorithm currently utilizing event representations. </details>
<details>	<summary>邮件日期</summary>	2021年03月11日</details>

# 74、具有耐久性的脉冲神经网络到神经形态硬件的映射
- [ ] Endurance-Aware Mapping of Spiking Neural Networks to Neuromorphic Hardware 
时间：2021年03月09日                         第一作者：Twisha Titirsha                       [链接](https://arxiv.org/abs/2103.05707).                     
## 摘要：神经形态计算系统正在采用忆阻器来实现高密度和低功耗的突触存储，如硬件中的交叉阵列。这些系统在执行脉冲神经网络（SNNs）时是节能的。我们观察到记忆型纵横制中的长位线和字线是寄生电压降的主要来源，寄生电压降会造成电流不对称。通过电路仿真，我们发现了这种不对称性导致的显著耐久性变化。因此，如果临界忆阻器（耐久性较低的忆阻器）被过度利用，它们可能会导致交叉杆寿命的缩短。我们提出eSpine，这是一种新的技术，通过在映射机器学习工作负载的每个纵横杆中加入耐久性变化来提高寿命，确保具有更高激活的突触总是在具有更高耐久性的忆阻器上实现，反之亦然。eSpine分两步工作。首先，它使用Kernighan-Lin图划分算法将工作负载划分为神经元和突触的簇，每个簇可以放在一个横杆中。第二，利用粒子群优化算法（PSO）的一个实例将簇映射到分片上，通过分析簇在工作负载中的激活情况，将簇的突触放置到十字杆的忆阻器上。我们评估了一个国家的最先进的神经形态硬件模型与相变记忆（PCM）为基础的忆阻器eSpine。使用10个SNN工作负载，我们证明了有效生存期的显著改进。
<details>	<summary>英文摘要</summary>	Neuromorphic computing systems are embracing memristors to implement high density and low power synaptic storage as crossbar arrays in hardware. These systems are energy efficient in executing Spiking Neural Networks (SNNs). We observe that long bitlines and wordlines in a memristive crossbar are a major source of parasitic voltage drops, which create current asymmetry. Through circuit simulations, we show the significant endurance variation that results from this asymmetry. Therefore, if the critical memristors (ones with lower endurance) are overutilized, they may lead to a reduction of the crossbar's lifetime. We propose eSpine, a novel technique to improve lifetime by incorporating the endurance variation within each crossbar in mapping machine learning workloads, ensuring that synapses with higher activation are always implemented on memristors with higher endurance, and vice versa. eSpine works in two steps. First, it uses the Kernighan-Lin Graph Partitioning algorithm to partition a workload into clusters of neurons and synapses, where each cluster can fit in a crossbar. Second, it uses an instance of Particle Swarm Optimization (PSO) to map clusters to tiles, where the placement of synapses of a cluster to memristors of a crossbar is performed by analyzing their activation within the workload. We evaluate eSpine for a state-of-the-art neuromorphic hardware model with phase-change memory (PCM)-based memristors. Using 10 SNN workloads, we demonstrate a significant improvement in the effective lifetime. </details>
<details>	<summary>注释</summary>	Accepted for publication in IEEE Transactions on Parallel and Distributed Systems (TPDS) </details>
<details>	<summary>邮件日期</summary>	2021年03月11日</details>

# 73、一种用于无监督特征学习的高并行度类初启脉冲神经网络
- [ ] High-parallelism Inception-like Spiking Neural Networks for Unsupervised Feature Learning 
时间：2021年03月09日                         第一作者：Mingyuan Meng                       [链接](https://arxiv.org/abs/2001.01680).                     
<details>	<summary>注释</summary>	Published at Neurocomputing DOI: 10.1016/j.neucom.2021.02.027 </details>
<details>	<summary>邮件日期</summary>	2021年03月10日</details>

# 72、基于在线进化脉冲神经网络的流数据无监督异常检测
- [ ] Unsupervised Anomaly Detection in Stream Data with Online Evolving Spiking Neural Networks 
时间：2021年03月08日                         第一作者：Piotr S. Maci\k{a}g (1)                       [链接](https://arxiv.org/abs/1912.08785).                     
<details>	<summary>注释</summary>	52 pages Journal-ref: Neural Networks, Volume 139, 2021, Pages 118-139 DOI: 10.1016/j.neunet.2021.02.017 </details>
<details>	<summary>邮件日期</summary>	2021年03月10日</details>

# 71、一点点能量就有很大的帮助：高效节能，从卷积神经网络到脉冲神经网络的精确转换
- [ ] A Little Energy Goes a Long Way: Energy-Efficient, Accurate Conversion from Convolutional Neural Networks to Spiking Neural Networks 
时间：2021年03月06日                         第一作者：Dengyu Wu                       [链接](https://arxiv.org/abs/2103.00944).                     
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 70、神经形态平台上强化学习的双记忆结构
- [ ] A Dual-Memory Architecture for Reinforcement Learning on Neuromorphic Platforms 
时间：2021年03月05日                         第一作者：Wilkie Olin-Ammentorp                       [链接](https://arxiv.org/abs/2103.04780).                     
## 摘要：强化学习（RL）是生物系统中学习的基础，并提供了一个框架来解决现实世界人工智能应用的众多挑战。RL技术的有效实现允许部署在边缘用例中的代理获得新的能力，例如改进的导航、理解复杂情况和关键决策。为了实现这个目标，我们描述了一个灵活的架构来在神经形态平台上进行强化学习。该体系结构是使用Intel神经形态处理器实现的，并演示了如何使用脉冲动力学解决各种任务。我们的研究为现实世界的RL应用提出了一个可用的节能解决方案，并证明了神经形态平台对RL问题的适用性。
<details>	<summary>英文摘要</summary>	Reinforcement learning (RL) is a foundation of learning in biological systems and provides a framework to address numerous challenges with real-world artificial intelligence applications. Efficient implementations of RL techniques could allow for agents deployed in edge-use cases to gain novel abilities, such as improved navigation, understanding complex situations and critical decision making. Towards this goal, we describe a flexible architecture to carry out reinforcement learning on neuromorphic platforms. This architecture was implemented using an Intel neuromorphic processor and demonstrated solving a variety of tasks using spiking dynamics. Our study proposes a usable energy efficient solution for real-world RL applications and demonstrates applicability of the neuromorphic platforms for RL problems. </details>
<details>	<summary>注释</summary>	20 pages, 6 figures ACM-class: I.2 </details>
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 69、基于在线元学习的脉冲神经网络快速设备自适应
- [ ] Fast On-Device Adaptation for Spiking Neural Networks via Online-Within-Online Meta-Learning 
时间：2021年02月21日                         第一作者：Bleema Rosenfeld                       [链接](https://arxiv.org/abs/2103.03901).                     
## 摘要：脉冲神经网络（Spiking Neural Networks，SNNs）由于其低功耗特性，近年来在移动医疗管理和自然语言处理等应用中作为边缘智能的机器学习模型得到了广泛的应用。在这种高度个人化的用例中，模型必须能够用最少的训练数据来适应个体的独特特征。元学习被认为是一种训练模型的方法，这种模型能够快速适应新的任务。为数不多的现有snn元学习解决方案离线运行，并且需要某种形式的反向传播，这与当前的神经形态边缘设备不兼容。在这篇论文中，我们提出了一个SNN的在线内在线元学习规则OWOML-SNN，它能够在任务流上实现终身学习，并且依赖于本地的、无后台的、嵌套的更新。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have recently gained popularity as machine learning models for on-device edge intelligence for applications such as mobile healthcare management and natural language processing due to their low power profile. In such highly personalized use cases, it is important for the model to be able to adapt to the unique features of an individual with only a minimal amount of training data. Meta-learning has been proposed as a way to train models that are geared towards quick adaptation to new tasks. The few existing meta-learning solutions for SNNs operate offline and require some form of backpropagation that is incompatible with the current neuromorphic edge-devices. In this paper, we propose an online-within-online meta-learning rule for SNNs termed OWOML-SNN, that enables lifelong learning on a stream of tasks, and relies on local, backprop-free, nested updates. </details>
<details>	<summary>注释</summary>	Accepted for publication at DSLW 2021 </details>
<details>	<summary>邮件日期</summary>	2021年03月09日</details>

# 68、机器人神经形态感知工具箱
- [ ] A toolbox for neuromorphic sensing in robotics 
时间：2021年03月03日                         第一作者：Julien Dupeyroux                       [链接](https://arxiv.org/abs/2103.02751).                     
## 摘要：由神经形态计算引入的第三代人工智能（AI）正在彻底改变机器人和自主系统感知世界、处理信息以及与环境交互的方式。神经形态系统的高灵活性、能量效率和鲁棒性的承诺得到了模拟脉冲神经网络的软件工具和硬件集成（神经形态处理器）的广泛支持。然而，尽管人们在神经形态视觉（基于事件的摄像机）方面做出了努力，但值得注意的是，大多数机器人可用的传感器与神经形态计算（信息被编码成脉冲）本质上仍然不兼容。为了方便传统传感器的使用，我们需要将输出信号转换成脉冲流，即一系列事件（+1，-1）及其相应的时间戳。在这篇论文中，我们从机器人学的角度对编码算法进行了回顾，并进一步通过一个基准测试来评估它们的性能。我们还引入了ROS（机器人操作系统）工具箱来编码和解码来自机器人上任何类型传感器的输入信号。这项倡议旨在刺激和促进神经形态人工智能的机器人集成，并有机会使传统的现成传感器适应最强大的机器人工具之一ROS中的脉冲神经网络。
<details>	<summary>英文摘要</summary>	The third generation of artificial intelligence (AI) introduced by neuromorphic computing is revolutionizing the way robots and autonomous systems can sense the world, process the information, and interact with their environment. The promises of high flexibility, energy efficiency, and robustness of neuromorphic systems is widely supported by software tools for simulating spiking neural networks, and hardware integration (neuromorphic processors). Yet, while efforts have been made on neuromorphic vision (event-based cameras), it is worth noting that most of the sensors available for robotics remain inherently incompatible with neuromorphic computing, where information is encoded into spikes. To facilitate the use of traditional sensors, we need to convert the output signals into streams of spikes, i.e., a series of events (+1, -1) along with their corresponding timestamps. In this paper, we propose a review of the coding algorithms from a robotics perspective and further supported by a benchmark to assess their performance. We also introduce a ROS (Robot Operating System) toolbox to encode and decode input signals coming from any type of sensor available on a robot. This initiative is meant to stimulate and facilitate robotic integration of neuromorphic AI, with the opportunity to adapt traditional off-the-shelf sensors to spiking neural nets within one of the most powerful robotic tools, ROS. </details>
<details>	<summary>注释</summary>	7 pages, 3 figures, 3 tables, 7 algorithms </details>
<details>	<summary>邮件日期</summary>	2021年03月05日</details>

# 67、基于事件的合成孔径成像
- [ ] Event-based Synthetic Aperture Imaging 
时间：2021年03月03日                         第一作者：Xiang Zhang                       [链接](https://arxiv.org/abs/2103.02376).                     
## 摘要：合成孔径成像（syntheticapertureimaging，SAI）是通过模糊掉离焦的前景遮挡，并从多视点图像中重建出在焦遮挡的目标，从而达到透视效果。然而，非常密集的遮挡和极端的光照条件可能会对基于传统帧相机的SAI带来显著的干扰，导致性能退化。为了解决这些问题，我们提出了一种基于事件摄像机的SAI系统，它可以产生具有极低延迟和高动态范围的异步事件。因此，它可以通过几乎连续的视图来消除密集遮挡的干扰，同时解决过度/不足曝光的问题。为了重建遮挡目标，提出了一种由脉冲神经网络（SNNs）和卷积神经网络（CNNs）组成的混合编解码网络。在混合网络中，首先对采集到的事件的时空信息进行SNN层编码，然后通过样式转换CNN解码器将其转换为遮挡目标的视觉图像。实验结果表明，该方法在处理高密度遮挡和极端光照条件下具有良好的性能，可以用纯事件数据重建高质量的视觉图像。
<details>	<summary>英文摘要</summary>	Synthetic aperture imaging (SAI) is able to achieve the see through effect by blurring out the off-focus foreground occlusions and reconstructing the in-focus occluded targets from multi-view images. However, very dense occlusions and extreme lighting conditions may bring significant disturbances to SAI based on conventional frame-based cameras, leading to performance degeneration. To address these problems, we propose a novel SAI system based on the event camera which can produce asynchronous events with extremely low latency and high dynamic range. Thus, it can eliminate the interference of dense occlusions by measuring with almost continuous views, and simultaneously tackle the over/under exposure problems. To reconstruct the occluded targets, we propose a hybrid encoder-decoder network composed of spiking neural networks (SNNs) and convolutional neural networks (CNNs). In the hybrid network, the spatio-temporal information of the collected events is first encoded by SNN layers, and then transformed to the visual image of the occluded targets by a style-transfer CNN decoder. Through experiments, the proposed method shows remarkable performance in dealing with very dense occlusions and extreme lighting conditions, and high quality visual images can be reconstructed using pure event data. </details>
<details>	<summary>注释</summary>	9 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年03月04日</details>

# 66、一点点能量就有很大的帮助：高效节能，从卷积神经网络到脉冲神经网络的精确转换
- [ ] A Little Energy Goes a Long Way: Energy-Efficient, Accurate Conversion from Convolutional Neural Networks to Spiking Neural Networks 
时间：2021年03月01日                         第一作者：Dengyu Wu                       [链接](https://arxiv.org/abs/2103.00944).                     
## 摘要：脉冲神经网络（SNNs）提供了一种处理时空数据的固有能力，也就是说，处理现实世界中的感官数据，但是很难训练出高精度的模型。SNN的一个主要研究方向是将预先训练好的卷积神经网络（CNN）转换成具有相同结构的SNN。最先进的转换方法正在接近精度极限，即SNN对原始CNN的精度损失接近于零。然而，我们注意到，只有当处理输入消耗的能量显著增加时，这才有可能实现。在本文中，我们认为这种“能量换精度”的趋势是不必要的——一点点能量可以大大提高精度损失的接近零。具体来说，我们提出了一种新的CNN到SNN转换方法，该方法能够使用合理的短脉冲序列（例如，CIFAR10图像的256个时间步）来实现接近零的精度损失。新的转换方法称为显式电流控制（ECC），包含三种技术（电流归一化、残差消除阈值和批量归一化一致性维护），以便在处理输入时显式控制流经SNN的电流。我们将ECC实现到一个昵称为SpKeras的工具中，该工具可以方便地导入Keras-CNN模型并将其转换为snn。我们使用该工具进行了一系列广泛的实验——使用VGG16和各种数据集，如CIFAR10和CIFAR100——并与最先进的转换方法进行了比较。结果表明，ECC是一种很有前途的方法，它可以同时优化系统的能耗和精度损失。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) offer an inherent ability to process spatial-temporal data, or in other words, realworld sensory data, but suffer from the difficulty of training high accuracy models. A major thread of research on SNNs is on converting a pre-trained convolutional neural network (CNN) to an SNN of the same structure. State-of-the-art conversion methods are approaching the accuracy limit, i.e., the near-zero accuracy loss of SNN against the original CNN. However, we note that this is made possible only when significantly more energy is consumed to process an input. In this paper, we argue that this trend of ''energy for accuracy'' is not necessary -- a little energy can go a long way to achieve the near-zero accuracy loss. Specifically, we propose a novel CNN-to-SNN conversion method that is able to use a reasonably short spike train (e.g., 256 timesteps for CIFAR10 images) to achieve the near-zero accuracy loss. The new conversion method, named as explicit current control (ECC), contains three techniques (current normalisation, thresholding for residual elimination, and consistency maintenance for batch-normalisation), in order to explicitly control the currents flowing through the SNN when processing inputs. We implement ECC into a tool nicknamed SpKeras, which can conveniently import Keras CNN models and convert them into SNNs. We conduct an extensive set of experiments with the tool -- working with VGG16 and various datasets such as CIFAR10 and CIFAR100 -- and compare with state-of-the-art conversion methods. Results show that ECC is a promising method that can optimise over energy consumption and accuracy loss simultaneously. </details>
<details>	<summary>邮件日期</summary>	2021年03月02日</details>

# 65、SpikeDyn：一种动态环境下具有连续无监督学习能力的节能型Spiking神经网络框架
- [ ] SpikeDyn: A Framework for Energy-Efficient Spiking Neural Networks with Continual and Unsupervised Learning Capabilities in Dynamic Environments 
时间：2021年02月28日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2103.00424).                     
## 摘要：脉冲神经网络（Spiking Neural Networks，SNNs）由于其生物学上的合理性，具有高效无监督和持续学习能力的潜力，但其复杂性仍然是一个严重的研究挑战，以使其能够针对资源受限的场景（如嵌入式系统、物联网边缘等）进行节能设计。我们提出了SpikeDyn，一个在动态环境中具有连续和无监督学习能力的节能snn的综合框架，用于训练和推理阶段。它是通过以下多种不同的机制实现的：1）减少神经元的操作，用直接的侧抑制代替抑制神经元；2）一种记忆和能量受限的SNN模型搜索算法，该算法利用分析模型来估计不同候选SNN的记忆足迹和能量消耗建立并选择一个Pareto最优SNN模型；3）一个轻量级的连续无监督学习算法，采用自适应学习率、自适应膜阈值电位、权值衰减和减少虚假更新。实验结果表明，对于一个由400个兴奋神经元组成的网络，我们的SpikeDyn在训练和推理方面的平均能耗分别比现有的方法降低了51%和37%。由于改进的学习算法，SpikeDyn对最近学习的任务进行分类，平均比最新技术提高了21%，对以前学习的任务平均提高了8%。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) bear the potential of efficient unsupervised and continual learning capabilities because of their biological plausibility, but their complexity still poses a serious research challenge to enable their energy-efficient design for resource-constrained scenarios (like embedded systems, IoT-Edge, etc.). We propose SpikeDyn, a comprehensive framework for energy-efficient SNNs with continual and unsupervised learning capabilities in dynamic environments, for both the training and inference phases. It is achieved through the following multiple diverse mechanisms: 1) reduction of neuronal operations, by replacing the inhibitory neurons with direct lateral inhibitions; 2) a memory- and energy-constrained SNN model search algorithm that employs analytical models to estimate the memory footprint and energy consumption of different candidate SNN models and selects a Pareto-optimal SNN model; and 3) a lightweight continual and unsupervised learning algorithm that employs adaptive learning rates, adaptive membrane threshold potential, weight decay, and reduction of spurious updates. Our experimental results show that, for a network with 400 excitatory neurons, our SpikeDyn reduces the energy consumption on average by 51% for training and by 37% for inference, as compared to the state-of-the-art. Due to the improved learning algorithm, SpikeDyn provides on avg. 21% accuracy improvement over the state-of-the-art, for classifying the most recently learned task, and by 8% on average for the previously learned tasks. </details>
<details>	<summary>注释</summary>	To appear at the 58th IEEE/ACM Design Automation Conference (DAC), December 2021, San Francisco, CA, USA </details>
<details>	<summary>邮件日期</summary>	2021年03月02日</details>

# 64、传统人工神经网络到脉冲神经网络的优化转换
- [ ] Optimal Conversion of Conventional Artificial Neural Networks to Spiking Neural Networks 
时间：2021年02月28日                         第一作者：Shikuang Deng                       [链接](https://arxiv.org/abs/2103.00476).                     
## 摘要：脉冲神经网络（SNNs）是一种受生物启发的人工神经网络（ANNs），由脉冲神经元组成，用于处理异步离散信号。由于snn的离散性，使得snn在功耗和推理速度上都有很大的提高，但通常很难直接从零开始训练。另一种方法是通过复制神经网络的权值和调整snn中神经元的峰值阈值电位，将传统的ann转化为snn。研究人员设计了新的SNN结构和转换算法来减小转换误差。然而，一个有效的转换应该解决SNN和ANN体系结构之间的差异，用一个有效的损失函数的近似值，这个函数在这个领域是缺失的。在这项工作中，我们分析了递归归约到分层求和的转换误差，并提出了一种新的策略管道，通过结合阈值平衡和软重置机制将权值转移到目标SNN。这种流水线使得转换后的SNN和传统的ann之间几乎没有精度损失，只有典型SNN模拟时间的$\sim1/10$。我们的方法有望在能量和内存有限的情况下，更好地支持SNNs，并将其移植到嵌入式平台上。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are biology-inspired artificial neural networks (ANNs) that comprise of spiking neurons to process asynchronous discrete signals. While more efficient in power consumption and inference speed on the neuromorphic hardware, SNNs are usually difficult to train directly from scratch with spikes due to the discreteness. As an alternative, many efforts have been devoted to converting conventional ANNs into SNNs by copying the weights from ANNs and adjusting the spiking threshold potential of neurons in SNNs. Researchers have designed new SNN architectures and conversion algorithms to diminish the conversion error. However, an effective conversion should address the difference between the SNN and ANN architectures with an efficient approximation \DSK{of} the loss function, which is missing in the field. In this work, we analyze the conversion error by recursive reduction to layer-wise summation and propose a novel strategic pipeline that transfers the weights to the target SNN by combining threshold balance and soft-reset mechanisms. This pipeline enables almost no accuracy loss between the converted SNNs and conventional ANNs with only $\sim1/10$ of the typical SNN simulation time. Our method is promising to get implanted onto embedded platforms with better support of SNNs with limited energy and memory. </details>
<details>	<summary>邮件日期</summary>	2021年03月02日</details>

# 63、SparkXD：一种基于近似DRAM的弹性高效脉冲神经网络推理框架
- [ ] SparkXD: A Framework for Resilient and Energy-Efficient Spiking Neural Network Inference using Approximate DRAM 
时间：2021年02月28日                         第一作者：Rachmad Vidya Wicaksana Putra                       [链接](https://arxiv.org/abs/2103.00421).                     
## 摘要：脉冲神经网络（SNNs）由于其生物稀疏性，具有实现低能耗的潜力。一些研究表明，片外存储器（DRAM）访问是SNN处理中最消耗能量的操作。然而，SNN系统中的最新技术并没有优化每个访问的DRAM能量，因此阻碍了实现高能量效率。为了最大限度地减少每次存取的DRAM能量，一个按键旋钮用于降低DRAM电源电压，但这可能会导致DRAM错误（即所谓的近似DRAM）。针对这一点，我们提出了SparkXD，这是一个新的框架，它提供了一个综合的联合解决方案，用于使用低功耗dram在电压引起的错误下进行弹性和节能SNN推断。SparkXD的关键机制是：（1）通过考虑近似DRAM误码的容错训练来提高SNN的容错性；（2）分析改进的SNN模型的容错性，找到满足目标精度约束的最大可容忍误码率；（3）能量有效的DRAM数据映射对于弹性SNN模型，该模型将权重映射到适当的DRAM位置以最小化DRAM访问能量。通过这些机制，SparkXD减轻了DRAM（近似）错误的负面影响，并提供了所需的精度。实验结果表明，当目标精度在基线设计的1%以内（即SNN没有DRAM错误）时，SparkXD在不同网络规模下平均降低DRAM能量约40%。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) have the potential for achieving low energy consumption due to their biologically sparse computation. Several studies have shown that the off-chip memory (DRAM) accesses are the most energy-consuming operations in SNN processing. However, state-of-the-art in SNN systems do not optimize the DRAM energy-per-access, thereby hindering achieving high energy-efficiency. To substantially minimize the DRAM energy-per-access, a key knob is to reduce the DRAM supply voltage but this may lead to DRAM errors (i.e., the so-called approximate DRAM). Towards this, we propose SparkXD, a novel framework that provides a comprehensive conjoint solution for resilient and energy-efficient SNN inference using low-power DRAMs subjected to voltage-induced errors. The key mechanisms of SparkXD are: (1) improving the SNN error tolerance through fault-aware training that considers bit errors from approximate DRAM, (2) analyzing the error tolerance of the improved SNN model to find the maximum tolerable bit error rate (BER) that meets the targeted accuracy constraint, and (3) energy-efficient DRAM data mapping for the resilient SNN model that maps the weights in the appropriate DRAM location to minimize the DRAM access energy. Through these mechanisms, SparkXD mitigates the negative impact of DRAM (approximation) errors, and provides the required accuracy. The experimental results show that, for a target accuracy within 1% of the baseline design (i.e., SNN without DRAM errors), SparkXD reduces the DRAM energy by ca. 40% on average across different network sizes. </details>
<details>	<summary>注释</summary>	To appear at the 58th IEEE/ACM Design Automation Conference (DAC), December 2021, San Francisco, CA, USA </details>
<details>	<summary>邮件日期</summary>	2021年03月02日</details>

# 62、结合脉冲神经网络和人工神经网络的图像增强分类
- [ ] Combining Spiking Neural Network and Artificial Neural Network for Enhanced Image Classification 
时间：2021年02月28日                         第一作者：Naoya Muramatsu                        [链接](https://arxiv.org/abs/2102.10592).                     
<details>	<summary>注释</summary>	This paper written for DEIM 2021 (https://db-event.jpn.org/deim2021/) has 12 pages, 6 figures and 3 tables MSC-class: 68T05 (Primary) 68T05 (Secondary) ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年03月02日</details>

# 61、癫痫发作预测的神经形态计算新方法
- [ ] A New Neuromorphic Computing Approach for Epileptic Seizure Prediction 
时间：2021年02月25日                         第一作者：Fengshi Tian                       [链接](https://arxiv.org/abs/2102.12773).                     
## 摘要：报道了几种利用卷积神经网络（CNNs）预测癫痫发作的高特异性和敏感性方法。然而，cnn的计算成本很高，耗电量也很大。这些不便使得基于CNN的方法很难在可穿戴设备上实现。基于能量有效的脉冲神经网络（SNNs），提出了一种用于癫痫发作预测的神经形态计算方法。该方法利用设计的高斯随机离散编码器从脑电样本中产生脉冲序列，并在结合了CNNs和SNNs优点的脉冲卷积神经网络（spiking-convolutional neural network，spiking-CNN）中进行预测。实验结果表明，该方法的灵敏度、特异性和AUC分别为95.1%、99.2%和0.912%，计算复杂度比CNN降低了98.58%，表明该方法硬件友好，精度高。
<details>	<summary>英文摘要</summary>	Several high specificity and sensitivity seizure prediction methods with convolutional neural networks (CNNs) are reported. However, CNNs are computationally expensive and power hungry. These inconveniences make CNN-based methods hard to be implemented on wearable devices. Motivated by the energy-efficient spiking neural networks (SNNs), a neuromorphic computing approach for seizure prediction is proposed in this work. This approach uses a designed gaussian random discrete encoder to generate spike sequences from the EEG samples and make predictions in a spiking convolutional neural network (Spiking-CNN) which combines the advantages of CNNs and SNNs. The experimental results show that the sensitivity, specificity and AUC can remain 95.1%, 99.2% and 0.912 respectively while the computation complexity is reduced by 98.58% compared to CNN, indicating that the proposed Spiking-CNN is hardware friendly and of high precision. </details>
<details>	<summary>注释</summary>	Accepted to 2021 IEEE International Symposium on Circuits and Systems (ISCAS) Journal-ref: 2021 IEEE International Symposium on Circuits and Systems (ISCAS) </details>
<details>	<summary>邮件日期</summary>	2021年02月26日</details>

# 60、皮层振荡在脉冲神经网络中实现了基于采样的计算
- [ ] Cortical oscillations implement a backbone for sampling-based computation in spiking neural networks 
时间：2021年02月23日                         第一作者：Agnes Korcsak-Gorzo                       [链接](https://arxiv.org/abs/2006.11099).                     
<details>	<summary>注释</summary>	28 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2021年02月24日</details>

# 59、STDP通过反向传播增强脉冲神经网络的学习
- [ ] STDP enhances learning by backpropagation in a spiking neural network 
时间：2021年02月21日                         第一作者：Kotaro Furuya                        [链接](https://arxiv.org/abs/2102.10530).                     
## 摘要：提出了一种用于脉冲神经网络的半监督学习方法。该方法由反向传播的有监督学习和脉冲时间依赖可塑性（STDP）的无监督学习组成，STDP是一种生物学上合理的学习规则。数值实验表明，在使用少量标记数据的情况下，该方法在不增加标记的情况下提高了精度。现有的判别模型半监督学习方法还没有实现这一特性。对于事件驱动系统，可以实现所提出的学习方法。因此，如果在神经形态硬件上实现，它在实时性问题上会非常高效。结果表明，STDP在监督学习后的应用中除了自组织外，还起着重要的作用，这不同于以往将STDP作为预训练解释为自组织的方法。
<details>	<summary>英文摘要</summary>	A semi-supervised learning method for spiking neural networks is proposed. The proposed method consists of supervised learning by backpropagation and subsequent unsupervised learning by spike-timing-dependent plasticity (STDP), which is a biologically plausible learning rule. Numerical experiments show that the proposed method improves the accuracy without additional labeling when a small amount of labeled data is used. This feature has not been achieved by existing semi-supervised learning methods of discriminative models. It is possible to implement the proposed learning method for event-driven systems. Hence, it would be highly efficient in real-time problems if it were implemented on neuromorphic hardware. The results suggest that STDP plays an important role other than self-organization when applied after supervised learning, which differs from the previous method of using STDP as pre-training interpreted as self-organization. </details>
<details>	<summary>注释</summary>	9 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年02月23日</details>

# 58、结合脉冲神经网络和人工神经网络的图像增强分类
- [ ] Combining Spiking Neural Network and Artificial Neural Network for Enhanced Image Classification 
时间：2021年02月21日                         第一作者：Naoya Muramatsu                        [链接](https://arxiv.org/abs/2102.10592).                     
## 摘要：随着深度神经网络的不断创新，更接近生物脑突触的脉冲神经网络（spiking neural networks，SNNs）因其低功耗而备受关注。然而，对于连续数据值，它们必须采用编码过程将值转换为峰值序列。因此，它们还没有超过直接处理这些值的人工神经网络（ANNs）的性能。为此，我们将人工神经网络和神经网络相结合，建立了多功能混合神经网络（HNNs），以提高相关性能。
<details>	<summary>英文摘要</summary>	With the continued innovations of deep neural networks, spiking neural networks (SNNs) that more closely resemble biological brain synapses have attracted attention owing to their low power consumption. However, for continuous data values, they must employ a coding process to convert the values to spike trains. Thus, they have not yet exceeded the performance of artificial neural networks (ANNs), which handle such values directly. To this end, we combine an ANN and an SNN to build versatile hybrid neural networks (HNNs) that improve the concerned performance. </details>
<details>	<summary>注释</summary>	This paper written for DEIM 2021 (https://db-event.jpn.org/deim2021/) has 12 pages, 6 figures and 3 tables MSC-class: 68T05 (Primary) 68T05 (Secondary) ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2021年02月23日</details>

# 57、脉冲神经形态芯片学习纠缠量子态
- [ ] Spiking neuromorphic chip learns entangled quantum states 
时间：2021年02月18日                         第一作者：Stefanie Czischek                       [链接](https://arxiv.org/abs/2008.01039).                     
<details>	<summary>注释</summary>	21 pages, 6 figures Submission to SciPost </details>
<details>	<summary>邮件日期</summary>	2021年02月22日</details>

# 56、方程：神经形态实现的脉冲驱动平衡传播
- [ ] EqSpike: Spike-driven Equilibrium Propagation for Neuromorphic Implementations 
时间：2021年02月17日                         第一作者：Erwann Martin                       [链接](https://arxiv.org/abs/2010.07859).                     
<details>	<summary>邮件日期</summary>	2021年02月18日</details>

# 55、阴阳数据集
- [ ] The Yin-Yang dataset 
时间：2021年02月16日                         第一作者：Laura Kriener                       [链接](https://arxiv.org/abs/2102.08211).                     
## 摘要：阴阳数据集是为研究生物似然误差反向传播和脉冲神经网络的深度学习而开发的。它提供了一些优势，可以替代经典的深度学习数据集，特别是在算法和模型原型场景中。首先，它体积更小，因此学习速度更快，因此更适合部署在网络规模有限的神经形态芯片上。第二，与深度神经网络相比，它在使用浅层神经网络所能达到的精度之间存在着非常明显的差距。
<details>	<summary>英文摘要</summary>	The Yin-Yang dataset was developed for research on biologically plausible error backpropagation and deep learning in spiking neural networks. It serves as an alternative to classic deep learning datasets, especially in algorithm- and model-prototyping scenarios, by providing several advantages. First, it is smaller and therefore faster to learn, thereby being better suited for the deployment on neuromorphic chips with limited network sizes. Second, it exhibits a very clear gap between the accuracies achievable using shallow as compared to deep neural networks. </details>
<details>	<summary>注释</summary>	3 pages, 3 figures, 2 tables </details>
<details>	<summary>邮件日期</summary>	2021年02月17日</details>

# 54、用GANs归化神经形态视觉事件流
- [ ] Naturalizing Neuromorphic Vision Event Streams Using GANs 
时间：2021年02月14日                         第一作者：Dennis Robey                       [链接](https://arxiv.org/abs/2102.07243).                     
## 摘要：动态视觉传感器能够在资源受限的环境中以高时间分辨率工作，但代价是捕获静态内容。事件流的稀疏特性使得下游处理任务更高效，因为它们适合于功率高效的脉冲神经网络。与神经形态视觉相关的挑战之一是缺乏事件流的可解释性。虽然大多数应用用例并不打算让事件流被除分类网络之外的任何东西直观地解释，但是在传统高速CMOS传感器无法到达的空间中集成这些传感器的机会将丢失。例如，像内窥镜这样的生物入侵传感器必须符合严格的电源预算，这就不允许以兆赫的速度进行图像集成。虽然动态视觉传感可以填补这一空白，解释的挑战仍然存在，并将降低临床诊断的信心。产生式对抗网络的使用为克服和补偿视觉芯片空间分辨率差和缺乏可解释性提供了一种可能的解决方案。本文系统地应用Pix2Pix网络对CIFAR-10和linnaeus5数据集的事件流进行自然化处理。通过对归化的事件流进行图像分类（其收敛到等效原始图像的2.81%以内），并对CIFAR-10和Linnaeus 5数据集的未处理事件流进行13.19%的相关改进，对网络的质量进行了基准测试。
<details>	<summary>英文摘要</summary>	Dynamic vision sensors are able to operate at high temporal resolutions within resource constrained environments, though at the expense of capturing static content. The sparse nature of event streams enables efficient downstream processing tasks as they are suited for power-efficient spiking neural networks. One of the challenges associated with neuromorphic vision is the lack of interpretability of event streams. While most application use-cases do not intend for the event stream to be visually interpreted by anything other than a classification network, there is a lost opportunity to integrating these sensors in spaces that conventional high-speed CMOS sensors cannot go. For example, biologically invasive sensors such as endoscopes must fit within stringent power budgets, which do not allow MHz-speeds of image integration. While dynamic vision sensing can fill this void, the interpretation challenge remains and will degrade confidence in clinical diagnostics. The use of generative adversarial networks presents a possible solution to overcoming and compensating for a vision chip's poor spatial resolution and lack of interpretability. In this paper, we methodically apply the Pix2Pix network to naturalize the event stream from spike-converted CIFAR-10 and Linnaeus 5 datasets. The quality of the network is benchmarked by performing image classification of naturalized event streams, which converges to within 2.81% of equivalent raw images, and an associated improvement over unprocessed event streams by 13.19% for the CIFAR-10 and Linnaeus 5 datasets. </details>
<details>	<summary>注释</summary>	5 pages, 7 figures </details>
<details>	<summary>邮件日期</summary>	2021年02月16日</details>

# 53、利用混合信号脉冲学习电路实现高效均衡网络
- [ ] Implementing efficient balanced networks with mixed-signal spike-based learning circuits 
时间：2021年02月12日                         第一作者：Julian B\"uchel                       [链接](https://arxiv.org/abs/2010.14353).                     
<details>	<summary>注释</summary>	5 pages, 6 figures. Accepted at IEEE International Symposium on Circuits and Systems 2021 </details>
<details>	<summary>邮件日期</summary>	2021年02月15日</details>

# 52、基于广义期望最大化的脉冲神经网络多样本在线学习
- [ ] Multi-Sample Online Learning for Spiking Neural Networks based on Generalized Expectation Maximization 
时间：2021年02月05日                         第一作者：Hyeryung Jang                        [链接](https://arxiv.org/abs/2102.03280).                     
## 摘要：脉冲神经网络（SNNs）提供了一种新的计算范式，它通过二元神经动态激活来获取生物大脑的一些效率。概率SNN模型通常通过使用对数似然梯度的无偏估计来训练以最大化期望输出的可能性。虽然先前的工作使用单样本估计器从一次运行的网络，本文提出利用多个隔间采样独立的脉冲信号，同时共享突触权重。其关键思想是利用这些信号来获得对数似然训练准则及其梯度的更精确的统计估计。该方法基于广义期望最大化（GEM），利用重要性抽样优化了对数似然的近似。导出的在线学习算法实现了一个具有全局每隔室学习信号的三因素规则。在神经形态MNIST-DVS数据集上的分类任务的实验结果表明，当增加用于训练和推理的隔室数量时，在对数似然性、准确性和校准方面有显著的改进。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) offer a novel computational paradigm that captures some of the efficiency of biological brains by processing through binary neural dynamic activations. Probabilistic SNN models are typically trained to maximize the likelihood of the desired outputs by using unbiased estimates of the log-likelihood gradients. While prior work used single-sample estimators obtained from a single run of the network, this paper proposes to leverage multiple compartments that sample independent spiking signals while sharing synaptic weights. The key idea is to use these signals to obtain more accurate statistical estimates of the log-likelihood training criterion, as well as of its gradient. The approach is based on generalized expectation-maximization (GEM), which optimizes a tighter approximation of the log-likelihood using importance sampling. The derived online learning algorithm implements a three-factor rule with global per-compartment learning signals. Experimental results on a classification task on the neuromorphic MNIST-DVS data set demonstrate significant improvements in terms of log-likelihood, accuracy, and calibration when increasing the number of compartments used for training and inference. </details>
<details>	<summary>注释</summary>	To be presented at ICASSP 2021. Author's Accepted Manuscript. (A longer version can be found at arXiv:2007.11894), Author's Accepted Manuscript. arXiv admin note: text overlap with arXiv:2007.11894 </details>
<details>	<summary>邮件日期</summary>	2021年02月08日</details>

# 51、优化的脉冲神经元通过双峰时间编码对图像进行高精度分类
- [ ] Optimized spiking neurons classify images with high accuracy through temporal coding with two spikes 
时间：2021年01月26日                         第一作者：Christoph St\"ockl                        [链接](https://arxiv.org/abs/2002.00860).                     
<details>	<summary>注释</summary>	23 pages, 5 figures, 1 tables </details>
<details>	<summary>邮件日期</summary>	2021年01月27日</details>

# 50、一种基于Loihi神经形态处理器的DVS摄像机手势识别算法
- [ ] An Efficient Spiking Neural Network for Recognizing Gestures with a DVS Camera on the Loihi Neuromorphic Processor 
时间：2021年01月25日                         第一作者：Riccardo Massa                       [链接](https://arxiv.org/abs/2006.09985).                     
<details>	<summary>注释</summary>	Accepted for publication at the 2020 International Joint Conference on Neural Networks (IJCNN) </details>
<details>	<summary>邮件日期</summary>	2021年01月26日</details>

# 49、腿型机器人仿生运动的神经形态自适应脉冲CPG
- [ ] Neuromorphic adaptive spiking CPG towards bio-inspired locomotion of legged robots 
时间：2021年01月24日                         第一作者：Pablo Lopez-Osorio                       [链接](https://arxiv.org/abs/2101.09709).                     
## 摘要：近年来，脊椎动物的运动机制为机器人系统性能的提高提供了灵感。这些机制包括它们的运动对通过生物传感器记录的环境变化的适应性。在这方面，我们的目标是复制这种适应性的腿机器人通过一个脉冲中心模式发生器。这种脉冲中心模式发生器产生不同的运动（节奏）模式，这些模式由外部刺激驱动，即连接到机器人的力敏感电阻器的输出，以提供反馈。脉冲中枢模式发生器由五个漏神经元群组成，这些漏神经元群具有特定的拓扑结构，使得节律模式可以由上述外部刺激产生和驱动。因此，末端机器人平台（任意腿机器人）的运动可以通过使用任意传感器作为输入来适应地形。采用brian2模拟器和SpiNNaker神经形态平台，对具有自适应学习的脉冲中心模式发生器进行了软硬件仿真验证。特别是，我们的实验清楚地表明，当输入刺激不同时，脉冲中央模式发生器群体中产生的脉冲之间的振荡频率发生了适应性变化。为了验证脉冲中心模式发生器的鲁棒性和适应性，我们通过改变传感器的输出进行了多次测试。这些实验在brian2和SpiNNaker中进行；两个实现都显示出相似的行为，Pearson相关系数为0.905。
<details>	<summary>英文摘要</summary>	In recent years, locomotion mechanisms exhibited by vertebrate animals have been the inspiration for the improvement in the performance of robotic systems. These mechanisms include the adaptability of their locomotion to any change registered in the environment through their biological sensors. In this regard, we aim to replicate such kind of adaptability in legged robots through a Spiking Central Pattern Generator. This Spiking Central Pattern Generator generates different locomotion (rhythmic) patterns which are driven by an external stimulus, that is, the output of a Force Sensitive Resistor connected to the robot to provide feedback. The Spiking Central Pattern Generator consists of a network of five populations of Leaky Integrate-and-Fire neurons designed with a specific topology in such a way that the rhythmic patterns can be generated and driven by the aforementioned external stimulus. Therefore, the locomotion of the end robotic platform (any-legged robot) can be adapted to the terrain by using any sensor as input. The Spiking Central Pattern Generator with adaptive learning has been numerically validated at software and hardware level, using the Brian 2 simulator and the SpiNNaker neuromorphic platform for the latest. In particular, our experiments clearly show an adaptation in the oscillation frequencies between the spikes produced in the populations of the Spiking Central Pattern Generator while the input stimulus varies. To validate the robustness and adaptability of the Spiking Central Pattern Generator, we have performed several tests by variating the output of the sensor. These experiments were carried out in Brian 2 and SpiNNaker; both implementations showed a similar behavior with a Pearson correlation coefficient of 0.905. </details>
<details>	<summary>注释</summary>	23 pages, 12 figures </details>
<details>	<summary>邮件日期</summary>	2021年01月26日</details>

# 48、事件驱动目标识别的脉冲学习系统
- [ ] A Spike Learning System for Event-driven Object Recognition 
时间：2021年01月21日                         第一作者：Shibo Zhou                       [链接](https://arxiv.org/abs/2101.08850).                     
## 摘要：事件驱动传感器，如激光雷达和动态视觉传感器（DVS）在高分辨率和高速应用中受到越来越多的关注。为了提高识别精度，人们做了大量的工作。然而，对于识别延迟或时间效率这一基本问题的研究还远远不够。在本文中，我们提出了一个脉冲学习系统，该系统使用脉冲神经网络（SNN）和一种新的时态编码来实现精确快速的目标识别。提出的时态编码方案将每个事件的到达时间和数据映射到SNN脉冲时间，使得异步到达的事件立即得到处理而没有延迟。该方案很好地结合了SNN的异步处理能力，提高了时间效率。与现有系统相比的一个关键优势是，每个识别任务的事件累积时间由系统自动确定，而不是由用户预先设置。系统可以在不等待所有输入事件的情况下提前完成识别。在7个激光雷达和DVS数据集上进行了广泛的实验。结果表明，该系统在取得显著时间效率的同时，具有最先进的识别精度。实验结果表明，在不同的实验条件下，在KITTI数据集上，识别延迟降低了56.3%-91.7%。
<details>	<summary>英文摘要</summary>	Event-driven sensors such as LiDAR and dynamic vision sensor (DVS) have found increased attention in high-resolution and high-speed applications. A lot of work has been conducted to enhance recognition accuracy. However, the essential topic of recognition delay or time efficiency is largely under-explored. In this paper, we present a spiking learning system that uses the spiking neural network (SNN) with a novel temporal coding for accurate and fast object recognition. The proposed temporal coding scheme maps each event's arrival time and data into SNN spike time so that asynchronously-arrived events are processed immediately without delay. The scheme is integrated nicely with the SNN's asynchronous processing capability to enhance time efficiency. A key advantage over existing systems is that the event accumulation time for each recognition task is determined automatically by the system rather than pre-set by the user. The system can finish recognition early without waiting for all the input events. Extensive experiments were conducted over a list of 7 LiDAR and DVS datasets. The results demonstrated that the proposed system had state-of-the-art recognition accuracy while achieving remarkable time efficiency. Recognition delay was shown to reduce by 56.3% to 91.7% in various experiment settings over the popular KITTI dataset. </details>
<details>	<summary>注释</summary>	Shibo Zhou and Wei Wang contributed equally to this work ACM-class: I.5.1; I.5.4; I.2.6; I.2.10 </details>
<details>	<summary>邮件日期</summary>	2021年01月25日</details>

# 47、亚稳材料的自主合成
- [ ] Autonomous synthesis of metastable materials 
时间：2021年01月19日                         第一作者：Sebastian Ament                       [链接](https://arxiv.org/abs/2101.07385).                     
## 摘要：人工智能的自主实验为加速科学发现提供了新的范例。非平衡材料合成是复杂的、资源密集型实验的标志，其加速将是材料发现和发展的分水岭。非平衡合成相图的绘制最近通过高通量实验得到了加速，但由于参数空间太大而无法进行详尽的探索，因此仍然限制了材料的研究。我们演示了加速合成和探索亚稳材料通过分层自主实验所管辖的科学自主推理代理（SARA）。SARA集成了机器人材料的合成和表征以及一系列人工智能方法，有效地揭示了加工相图的结构。SARA设计了用于平行材料合成的横向梯度激光脉冲退火（lg-LSA）实验，并利用光谱技术快速识别相变。多维参数空间的有效探索是通过嵌套的主动学习（AL）循环来实现的，该循环建立在先进的机器学习模型之上，该模型结合了实验的基本物理以及端到端的不确定性量化。有了这一点，以及在多个尺度上的协调，SARA体现了人工智能对复杂科学任务的利用。我们通过自主绘制Bi$\u2$O$\u3$系统的合成相边界来证明它的性能，从而在建立一个合成相图时产生数量级的加速，该合成相图包括在室温下动力学稳定$\delta$-Bi$\u2$O$\u3$的条件，固体氧化物燃料电池等电化学技术的关键发展。
<details>	<summary>英文摘要</summary>	Autonomous experimentation enabled by artificial intelligence (AI) offers a new paradigm for accelerating scientific discovery. Non-equilibrium materials synthesis is emblematic of complex, resource-intensive experimentation whose acceleration would be a watershed for materials discovery and development. The mapping of non-equilibrium synthesis phase diagrams has recently been accelerated via high throughput experimentation but still limits materials research because the parameter space is too vast to be exhaustively explored. We demonstrate accelerated synthesis and exploration of metastable materials through hierarchical autonomous experimentation governed by the Scientific Autonomous Reasoning Agent (SARA). SARA integrates robotic materials synthesis and characterization along with a hierarchy of AI methods that efficiently reveal the structure of processing phase diagrams. SARA designs lateral gradient laser spike annealing (lg-LSA) experiments for parallel materials synthesis and employs optical spectroscopy to rapidly identify phase transitions. Efficient exploration of the multi-dimensional parameter space is achieved with nested active learning (AL) cycles built upon advanced machine learning models that incorporate the underlying physics of the experiments as well as end-to-end uncertainty quantification. With this, and the coordination of AL at multiple scales, SARA embodies AI harnessing of complex scientific tasks. We demonstrate its performance by autonomously mapping synthesis phase boundaries for the Bi$_2$O$_3$ system, leading to orders-of-magnitude acceleration in establishment of a synthesis phase diagram that includes conditions for kinetically stabilizing $\delta$-Bi$_2$O$_3$ at room temperature, a critical development for electrochemical technologies such as solid oxide fuel cells. </details>
<details>	<summary>邮件日期</summary>	2021年01月20日</details>

# 46、模拟七鳃鳗机器人在SpiNNaker和Loihi神经形态板上运行的脉冲中心模式发生器
- [ ] A Spiking Central Pattern Generator for the control of a simulated lamprey robot running on SpiNNaker and Loihi neuromorphic boards 
时间：2021年01月18日                         第一作者：Emmanouil Angelidis                       [链接](https://arxiv.org/abs/2101.07001).                     
## 摘要：中枢模式发生器（CPGs）模型长期以来被用来研究动物运动的神经机制，也被用作机器人研究的工具。在这项工作中，我们提出了一个脉冲CPG神经网络及其在神经形态硬件上的实现作为一种手段来控制一个模拟七鳃鳗模型。为了建立我们的CPG模型，我们采用了自然出现的动态系统，这些系统是通过在神经工程框架（NEF）中使用递归神经种群而产生的。我们定义了我们模型背后的数学公式，它由一个由高电平信号调制的耦合抽象振荡器系统组成，能够产生各种输出步态。我们证明，利用这种中央模式发生器模型的数学公式，可以将该模型转化为一个脉冲神经网络（SNN），该网络可以很容易地用SNN模拟器Nengo进行模拟。然后利用脉冲CPG模型生成不同场景下模拟七鳃鳗机器人模型的游动步态。我们证明，通过修改网络的输入（可以由感官信息提供），机器人可以在方向和速度上进行动态控制。该方法可推广应用于工程应用和科学研究中的其它类型的cpg。我们在两个神经形态平台上测试我们的系统，SpiNNaker和Loihi。最后，我们证明了这类脉冲算法在能量效率和计算速度方面显示了利用神经形态硬件理论优势的潜力。
<details>	<summary>英文摘要</summary>	Central Pattern Generators (CPGs) models have been long used to investigate both the neural mechanisms that underlie animal locomotion as well as a tool for robotic research. In this work we propose a spiking CPG neural network and its implementation on neuromorphic hardware as a means to control a simulated lamprey model. To construct our CPG model, we employ the naturally emerging dynamical systems that arise through the use of recurrent neural populations in the Neural Engineering Framework (NEF). We define the mathematical formulation behind our model, which consists of a system of coupled abstract oscillators modulated by high-level signals, capable of producing a variety of output gaits. We show that with this mathematical formulation of the Central Pattern Generator model, the model can be turned into a Spiking Neural Network (SNN) that can be easily simulated with Nengo, an SNN simulator. The spiking CPG model is then used to produce the swimming gaits of a simulated lamprey robot model in various scenarios. We show that by modifying the input to the network, which can be provided by sensory information, the robot can be controlled dynamically in direction and pace. The proposed methodology can be generalized to other types of CPGs suitable for both engineering applications and scientific research. We test our system on two neuromorphic platforms, SpiNNaker and Loihi. Finally, we show that this category of spiking algorithms shows a promising potential to exploit the theoretical advantages of neuromorphic hardware in terms of energy efficiency and computational speed. </details>
<details>	<summary>注释</summary>	25 pages, 15 figures </details>
<details>	<summary>邮件日期</summary>	2021年01月19日</details>

# 45、用于时空特征提取的卷积脉冲神经网络
- [ ] Convolutional Spiking Neural Networks for Spatio-Temporal Feature Extraction 
时间：2021年01月18日                         第一作者：Ali Samadzadeh                       [链接](https://arxiv.org/abs/2003.12346).                     
<details>	<summary>注释</summary>	10 pages, 7 figures, 2 tables </details>
<details>	<summary>邮件日期</summary>	2021年01月20日</details>

# 44、方程：神经形态实现的脉冲驱动平衡传播
- [ ] EqSpike: Spike-driven Equilibrium Propagation for Neuromorphic Implementations 
时间：2021年01月15日                         第一作者：Erwann Martin                       [链接](https://arxiv.org/abs/2010.07859).                     
<details>	<summary>邮件日期</summary>	2021年01月18日</details>

# 43、概率脉冲神经网络的多样本在线学习
- [ ] Multi-Sample Online Learning for Probabilistic Spiking Neural Networks 
时间：2021年01月05日                         第一作者：Hyeryung Jang                        [链接](https://arxiv.org/abs/2007.11894).                     
<details>	<summary>注释</summary>	Submitted </details>
<details>	<summary>邮件日期</summary>	2021年01月06日</details>

# 42、人工脉冲量子神经元
- [ ] An Artificial Spiking Quantum Neuron 
时间：2020年12月30日                         第一作者：Lasse Bj{\o}rn Kristensen                       [链接](https://arxiv.org/abs/1907.06269).                     
<details>	<summary>邮件日期</summary>	2021年01月01日</details>

# 41、深Q网络向事件驱动脉冲神经网络转化的策略与基准
- [ ] Strategy and Benchmark for Converting Deep Q-Networks to Event-Driven Spiking Neural Networks 
时间：2020年12月23日                         第一作者：Weihao Tan                       [链接](https://arxiv.org/abs/2009.14456).                     
<details>	<summary>注释</summary>	Accepted by AAAI2021 </details>
<details>	<summary>邮件日期</summary>	2020年12月24日</details>

# 40、生成模型的进化变分优化
- [ ] Evolutionary Variational Optimization of Generative Models 
时间：2020年12月22日                         第一作者：Jakob Drefs                       [链接](https://arxiv.org/abs/2012.12294).                     
## 摘要：我们结合两种流行的优化方法来推导生成模型的学习算法：变分优化和进化算法。利用截断后验概率作为变分分布族，实现了离散时滞生成模型的组合。截断后验概率的变分参数是一组潜在状态。通过将这些状态解释为个体的基因组，并利用变分下界来定义适应度，我们可以应用进化算法来实现变分循环。所使用的变分分布是非常灵活的，我们证明了进化算法可以有效地优化变分界。此外，变分回路通常适用（“黑盒”），无需分析推导。为了说明该方法的普遍适用性，我们将该方法应用于三种生成模型（使用噪声或贝叶斯网、二进制稀疏编码以及脉冲和板稀疏编码）。为了证明新的变分方法的有效性和效率，我们使用了图像去噪和修复的标准竞争基准。这些基准允许对各种方法进行定量比较，包括概率方法、深层确定性和生成性网络以及非局部图像处理方法。在“零镜头”学习（当只使用损坏的图像进行训练时）的范畴中，我们观察到进化变分算法在许多基准设置中显著改善了最新的状态。对于一个著名的修复基准，我们还观察到了各种算法的最新性能，尽管我们只对损坏的图像进行训练。总的来说，我们的研究强调了研究生成模型的优化方法以提高性能的重要性。
<details>	<summary>英文摘要</summary>	We combine two popular optimization approaches to derive learning algorithms for generative models: variational optimization and evolutionary algorithms. The combination is realized for generative models with discrete latents by using truncated posteriors as the family of variational distributions. The variational parameters of truncated posteriors are sets of latent states. By interpreting these states as genomes of individuals and by using the variational lower bound to define a fitness, we can apply evolutionary algorithms to realize the variational loop. The used variational distributions are very flexible and we show that evolutionary algorithms can effectively and efficiently optimize the variational bound. Furthermore, the variational loop is generally applicable ("black box") with no analytical derivations required. To show general applicability, we apply the approach to three generative models (we use noisy-OR Bayes Nets, Binary Sparse Coding, and Spike-and-Slab Sparse Coding). To demonstrate effectiveness and efficiency of the novel variational approach, we use the standard competitive benchmarks of image denoising and inpainting. The benchmarks allow quantitative comparisons to a wide range of methods including probabilistic approaches, deep deterministic and generative networks, and non-local image processing methods. In the category of "zero-shot" learning (when only the corrupted image is used for training), we observed the evolutionary variational algorithm to significantly improve the state-of-the-art in many benchmark settings. For one well-known inpainting benchmark, we also observed state-of-the-art performance across all categories of algorithms although we only train on the corrupted image. In general, our investigations highlight the importance of research on optimization methods for generative models to achieve performance improvements. </details>
<details>	<summary>邮件日期</summary>	2020年12月24日</details>

# 39、用直接训练的更大的脉冲神经网络进行更深入的研究
- [ ] Going Deeper With Directly-Trained Larger Spiking Neural Networks 
时间：2020年12月18日                         第一作者：Hanle Zheng                       [链接](https://arxiv.org/abs/2011.05280).                     
<details>	<summary>注释</summary>	12 pages, 6 figures, conference or other essential info </details>
<details>	<summary>邮件日期</summary>	2020年12月21日</details>

# 38、脉冲神经网络到神经形态硬件的热感知编译
- [ ] Thermal-Aware Compilation of Spiking Neural Networks to Neuromorphic Hardware 
时间：2020年12月17日                         第一作者：Twisha Titirsha                        [链接](https://arxiv.org/abs/2010.04773).                     
<details>	<summary>注释</summary>	Accepted for publication at LCPC 2020 </details>
<details>	<summary>邮件日期</summary>	2020年12月21日</details>

# 37、基于贝叶斯学习的二元权值脉冲神经网络训练
- [ ] BiSNN: Training Spiking Neural Networks with Binary Weights via Bayesian Learning 
时间：2020年12月15日                         第一作者：Hyeryung Jang                        [链接](https://arxiv.org/abs/2012.08300).                     
## 摘要：基于人工神经网络（ANN）的电池供电设备的推理可以通过限制突触权值为二进制，从而消除执行乘法的需要，从而提高能量效率。另一种新兴的方法依赖于使用脉冲神经网络（SNNs），这是一种受生物启发的动态事件驱动模型，通过使用二进制稀疏激活来提高能源效率。本文介绍了一种SNN模型，它结合了时间稀疏二进制激活和二进制权值的优点。推导了两种学习规则，第一种基于直通和代理梯度技术的组合，第二种基于贝叶斯范式。实验验证了全精度实现的性能损失，并证明了贝叶斯范式在精度和校准方面的优势。
<details>	<summary>英文摘要</summary>	Artificial Neural Network (ANN)-based inference on battery-powered devices can be made more energy-efficient by restricting the synaptic weights to be binary, hence eliminating the need to perform multiplications. An alternative, emerging, approach relies on the use of Spiking Neural Networks (SNNs), biologically inspired, dynamic, event-driven models that enhance energy efficiency via the use of binary, sparse, activations. In this paper, an SNN model is introduced that combines the benefits of temporally sparse binary activations and of binary weights. Two learning rules are derived, the first based on the combination of straight-through and surrogate gradient techniques, and the second based on a Bayesian paradigm. Experiments validate the performance loss with respect to full-precision implementations, and demonstrate the advantage of the Bayesian paradigm in terms of accuracy and calibration. </details>
<details>	<summary>注释</summary>	Submitted </details>
<details>	<summary>邮件日期</summary>	2020年12月16日</details>

# 36、脉冲神经元Hebbian和STDP学习权值的约束
- [ ] Constraints on Hebbian and STDP learned weights of a spiking neuron 
时间：2020年12月14日                         第一作者：Dominique Chu                        [链接](https://arxiv.org/abs/2012.07664).                     
## 摘要：我们从数学上分析了Hebbian和STDP学习规则对权值的限制，这些规则应用于权值归一化的脉冲神经元。在纯Hebbian学习的情况下，我们发现标准化的权值等于权值的提升概率，直到依赖于学习率的修正项，并且通常很小。对于STDP算法，可以导出类似的关系，其中标准化的权重值反映了权重的提升和降级概率之间的差异。这些关系实际上很有用，因为它们允许检查Hebbian和STDP算法的收敛性。另一个应用是新颖性检测。我们使用MNIST数据集演示了这一点。
<details>	<summary>英文摘要</summary>	We analyse mathematically the constraints on weights resulting from Hebbian and STDP learning rules applied to a spiking neuron with weight normalisation. In the case of pure Hebbian learning, we find that the normalised weights equal the promotion probabilities of weights up to correction terms that depend on the learning rate and are usually small. A similar relation can be derived for STDP algorithms, where the normalised weight values reflect a difference between the promotion and demotion probabilities of the weight. These relations are practically useful in that they allow checking for convergence of Hebbian and STDP algorithms. Another application is novelty detection. We demonstrate this using the MNIST dataset. </details>
<details>	<summary>邮件日期</summary>	2020年12月15日</details>

# 35、生物神经网络的低阶模型
- [ ] Low-Order Model of Biological Neural Networks 
时间：2020年12月12日                         第一作者：Huachuan Wang                        [链接](https://arxiv.org/abs/2012.06720).                     
## 摘要：生物神经网络的生物似真低阶模型（LOM）是由树突节点/树、脉冲/非脉冲神经元、无监督/有监督协方差/累积学习机制、反馈连接和最大泛化方案组成的递归层次网络。这些组件模型的动机和必要性在于使LOM易于学习和检索，而无需区分、优化或迭代，以及聚类、检测和识别多个/层次损坏、扭曲和闭塞的时间和空间模式。
<details>	<summary>英文摘要</summary>	A biologically plausible low-order model (LOM) of biological neural networks is a recurrent hierarchical network of dendritic nodes/trees, spiking/nonspiking neurons, unsupervised/ supervised covariance/accumulative learning mechanisms, feedback connections, and a scheme for maximal generalization. These component models are motivated and necessitated by making LOM learn and retrieve easily without differentiation, optimization, or iteration, and cluster, detect and recognize multiple/hierarchical corrupted, distorted, and occluded temporal and spatial patterns. </details>
<details>	<summary>邮件日期</summary>	2020年12月15日</details>

# 34、脉冲神经网络第一部分：空间模式检测
- [ ] Spiking Neural Networks -- Part I: Detecting Spatial Patterns 
时间：2020年12月09日                         第一作者：Hyeryung Jang                       [链接](https://arxiv.org/abs/2010.14208).                     
<details>	<summary>注释</summary>	Submitted </details>
<details>	<summary>邮件日期</summary>	2020年12月10日</details>

# 33、脉冲神经网络第二部分：时空模式检测
- [ ] Spiking Neural Networks -- Part II: Detecting Spatio-Temporal Patterns 
时间：2020年12月09日                         第一作者：Nicolas Skatchkovsky                       [链接](https://arxiv.org/abs/2010.14217).                     
<details>	<summary>注释</summary>	Submitted. The first two authors have equally contributed to this work </details>
<details>	<summary>邮件日期</summary>	2020年12月10日</details>

# 32、脉冲神经网络第三部分：神经形态通信
- [ ] Spiking Neural Networks -- Part III: Neuromorphic Communications 
时间：2020年12月09日                         第一作者：Nicolas Skatchkovsky                       [链接](https://arxiv.org/abs/2010.14220).                     
<details>	<summary>注释</summary>	Submitted </details>
<details>	<summary>邮件日期</summary>	2020年12月10日</details>

# 31、一种训练脉冲神经网络的多智能体进化机器人框架
- [ ] A multi-agent evolutionary robotics framework to train spiking neural networks 
时间：2020年12月07日                         第一作者：Souvik Das                       [链接](https://arxiv.org/abs/2012.03485).                     
## 摘要：提出了一种基于多智能体进化机器人（ER）的训练脉冲神经网络（SNN）的新框架。snn群体的权重以及它们在ER环境中控制的机器人的形态参数被视为表型。该框架的规则根据某些机器人在竞争环境中捕获食物的效率，选择它们及其snn进行繁殖，而选择其他snn进行淘汰。虽然机器人和它们的snn没有通过任何损失函数获得生存或繁衍的明确奖励，但当它们进化到捕猎食物并在这些规则下生存时，这些驱动力隐而不露。它们捕获食物的效率随着世代的变化而呈现出间断平衡的进化特征。给出了两种表型遗传算法：变异遗传算法和带变异交叉遗传算法。通过对每种算法进行100个实验，比较了这些算法的性能。我们发现，在SNN中，带突变的交叉比仅带统计显著性差异的突变能提高40%的学习速度。
<details>	<summary>英文摘要</summary>	A novel multi-agent evolutionary robotics (ER) based framework, inspired by competitive evolutionary environments in nature, is demonstrated for training Spiking Neural Networks (SNN). The weights of a population of SNNs along with morphological parameters of bots they control in the ER environment are treated as phenotypes. Rules of the framework select certain bots and their SNNs for reproduction and others for elimination based on their efficacy in capturing food in a competitive environment. While the bots and their SNNs are given no explicit reward to survive or reproduce via any loss function, these drives emerge implicitly as they evolve to hunt food and survive within these rules. Their efficiency in capturing food as a function of generations exhibit the evolutionary signature of punctuated equilibria. Two evolutionary inheritance algorithms on the phenotypes, Mutation and Crossover with Mutation, are demonstrated. Performances of these algorithms are compared using ensembles of 100 experiments for each algorithm. We find that Crossover with Mutation promotes 40% faster learning in the SNN than mere Mutation with a statistically significant margin. </details>
<details>	<summary>注释</summary>	9 pages, 11 figures </details>
<details>	<summary>邮件日期</summary>	2020年12月08日</details>

# 30、大脑的功能是否像一台使用相位三值计算的量子相位计算机？
- [ ] Does the brain function as a quantum phase computer using phase ternary computation? 
时间：2020年12月04日                         第一作者：Andrew Simon Johnson                        [链接](https://arxiv.org/abs/2012.06537).                     
## 摘要：在这里，我们提供的证据表明，神经通信的基本基础来自于压力脉冲/孤子，它能够以足够的时间精度进行计算，以克服任何处理错误。神经系统内的信号传递和计算是复杂而不同的现象。动作电位是塑性的，这使得动作电位峰值对于神经计算来说是一个不合适的固定点，但是动作电位阈值适合于这个目的。此外，由脉冲神经元计时的神经模型的运算速率低于克服加工误差所需的速率。以视网膜处理为例，我们证明了基于电缆理论的当代神经传导理论不适合解释视网膜和大脑其他部分的完整功能所需的短计算时间。此外，电缆理论不能帮助传播的行动电位，因为在激活阈值没有足够的电荷在激活地点连续离子通道静电开放。对大脑神经网络的解构表明它是一组量子相位计算机中的一员，其中图灵机是最简单的：大脑是另一个基于相位三值计算的计算机。然而，使用图灵机制的尝试无法解决视网膜的编码或智能的计算，因为基于图灵的计算机的技术是根本不同的。我们证明了大脑神经网络中的编码是基于量子的，其中量子有一个时间变量和一个相位基变量，这使得相位三值计算成为可能，正如之前在视网膜中所证明的那样。
<details>	<summary>英文摘要</summary>	Here we provide evidence that the fundamental basis of nervous communication is derived from a pressure pulse/soliton capable of computation with sufficient temporal precision to overcome any processing errors. Signalling and computing within the nervous system are complex and different phenomena. Action potentials are plastic and this makes the action potential peak an inappropriate fixed point for neural computation, but the action potential threshold is suitable for this purpose. Furthermore, neural models timed by spiking neurons operate below the rate necessary to overcome processing error. Using retinal processing as our example, we demonstrate that the contemporary theory of nerve conduction based on cable theory is inappropriate to account for the short computational time necessary for the full functioning of the retina and by implication the rest of the brain. Moreover, cable theory cannot be instrumental in the propagation of the action potential because at the activation-threshold there is insufficient charge at the activation site for successive ion channels to be electrostatically opened. Deconstruction of the brain neural network suggests that it is a member of a group of Quantum phase computers of which the Turing machine is the simplest: the brain is another based upon phase ternary computation. However, attempts to use Turing based mechanisms cannot resolve the coding of the retina or the computation of intelligence, as the technology of Turing based computers is fundamentally different. We demonstrate that that coding in the brain neural network is quantum based, where the quanta have a temporal variable and a phase-base variable enabling phase ternary computation as previously demonstrated in the retina. </details>
<details>	<summary>注释</summary>	16 pages, 7 figures. Key Words: Plasticity; Action potential; Timing; Error redaction; Synchronization; Quantum phase computation; Phase ternary computation; Retinal model ACM-class: I.2; J.3 </details>
<details>	<summary>邮件日期</summary>	2020年12月14日</details>

# 29、DIET-SNN：深脉冲神经网络中带泄漏和阈值优化的直接输入编码
- [ ] DIET-SNN: Direct Input Encoding With Leakage and Threshold Optimization in Deep Spiking Neural Networks 
时间：2020年12月02日                         第一作者：Nitin Rathi                       [链接](https://arxiv.org/abs/2008.03658).                     
<details>	<summary>邮件日期</summary>	2020年12月03日</details>

# 28、从头开始训练低潜伏期深脉冲神经网络的批标准化研究
- [ ] Revisiting Batch Normalization for Training Low-latency Deep Spiking Neural Networks from Scratch 
时间：2020年11月30日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2010.01729).                     
<details>	<summary>邮件日期</summary>	2020年12月01日</details>

# 27、DIET-SNN：深脉冲神经网络中带泄漏和阈值优化的直接输入编码
- [ ] DIET-SNN: Direct Input Encoding With Leakage and Threshold Optimization in Deep Spiking Neural Networks 
时间：2020年11月29日                         第一作者：Nitin Rathi                       [链接](https://arxiv.org/abs/2008.03658).                     
<details>	<summary>邮件日期</summary>	2020年12月01日</details>

# 26、编译脉冲神经网络以减轻神经形态的硬件约束
- [ ] Compiling Spiking Neural Networks to Mitigate Neuromorphic Hardware Constraints 
时间：2020年11月27日                         第一作者：Adarsha Balaji                        [链接](https://arxiv.org/abs/2011.13965).                     
## 摘要：脉冲神经网络（SNNs）是在{resource}和{power}约束平台上进行时空模式识别的有效计算模型。在神经形态硬件上执行snn可以进一步降低这些平台的能耗。随着模型尺寸和复杂性的增加，将基于SNN的应用程序映射到基于tile的神经形态硬件变得越来越具有挑战性。这归因于神经突触核心的局限性，即。一种横杆，每个突触后神经元只能容纳固定数量的突触前连接。对于具有许多神经元和每个神经元的突触前连接的基于SNN的复杂模型，（1）在训练后可能需要修剪连接以适应交叉资源，导致模型质量的损失，例如准确性，（2）神经元和突触需要分块并放置在硬件的神经系统核心上，这可能导致延迟和能量消耗增加。在这项工作中，我们提出（1）一种新的展开技术，将具有许多突触前连接的神经元功能分解为一系列同质的神经单元，以显著提高交叉杆的利用率并保留所有突触前连接，（2）SpiNeMap，提出了一种在神经形态硬件上映射snn的新方法，旨在最小化能量消耗和峰值潜伏期。
<details>	<summary>英文摘要</summary>	Spiking Neural Networks (SNNs) are efficient computation models to perform spatio-temporal pattern recognition on {resource}- and {power}-constrained platforms. SNNs executed on neuromorphic hardware can further reduce energy consumption of these platforms. With increasing model size and complexity, mapping SNN-based applications to tile-based neuromorphic hardware is becoming increasingly challenging. This is attributed to the limitations of neuro-synaptic cores, viz. a crossbar, to accommodate only a fixed number of pre-synaptic connections per post-synaptic neuron. For complex SNN-based models that have many neurons and pre-synaptic connections per neuron, (1) connections may need to be pruned after training to fit onto the crossbar resources, leading to a loss in model quality, e.g., accuracy, and (2) the neurons and synapses need to be partitioned and placed on the neuro-sypatic cores of the hardware, which could lead to increased latency and energy consumption. In this work, we propose (1) a novel unrolling technique that decomposes a neuron function with many pre-synaptic connections into a sequence of homogeneous neural units to significantly improve the crossbar utilization and retain all pre-synaptic connections, and (2) SpiNeMap, a novel methodology to map SNNs on neuromorphic hardware with an aim to minimize energy consumption and spike latency. </details>
<details>	<summary>邮件日期</summary>	2020年12月01日</details>

# 25、一种用于在线学习的时态神经网络结构
- [ ] A Temporal Neural Network Architecture for Online Learning 
时间：2020年11月27日                         第一作者：James E. Smith                       [链接](https://arxiv.org/abs/2011.13844).                     
## 摘要：一个长期存在的观点是，通过模拟大脑新皮质的运作，脉冲神经网络（SNN）可以实现类似的理想特性：灵活的学习、速度和效率。时态神经网络（TNNs）是一种snn，用来传递和处理编码为相对峰值时间的信息（与峰值速率相反）。提出了一种TNN体系结构，并在在线监督分类的大背景下证明了TNN的操作。首先，通过无监督学习，TNN根据相似性将输入模式划分为多个簇。然后TNN将一个簇标识符传递给一个简单的在线监督解码器，解码器完成分类任务。TNN学习过程只使用每个突触的局部信号来调整突触的权重，聚类行为在全局范围内出现。系统架构是在抽象层描述的，类似于传统数字设计中的门和寄存器传输层。除了整体架构的特性之外，一些TNN组件对于这项工作来说是新的。虽然没有直接解决，但总体研究目标是TNNs的直接硬件实现。因此，所有的架构元素都很简单，处理的精度很低。重要的是，低精度导致学习时间非常快。使用历史悠久的MNIST数据集的仿真结果表明，学习时间比其他在线方法至少快一个数量级，同时提供了类似的错误率。
<details>	<summary>英文摘要</summary>	A long-standing proposition is that by emulating the operation of the brain's neocortex, a spiking neural network (SNN) can achieve similar desirable features: flexible learning, speed, and efficiency. Temporal neural networks (TNNs) are SNNs that communicate and process information encoded as relative spike times (in contrast to spike rates). A TNN architecture is proposed, and, as a proof-of-concept, TNN operation is demonstrated within the larger context of online supervised classification. First, through unsupervised learning, a TNN partitions input patterns into clusters based on similarity. The TNN then passes a cluster identifier to a simple online supervised decoder which finishes the classification task. The TNN learning process adjusts synaptic weights by using only signals local to each synapse, and clustering behavior emerges globally. The system architecture is described at an abstraction level analogous to the gate and register transfer levels in conventional digital design. Besides features of the overall architecture, several TNN components are new to this work. Although not addressed directly, the overall research objective is a direct hardware implementation of TNNs. Consequently, all the architecture elements are simple, and processing is done at very low precision. Importantly, low precision leads to very fast learning times. Simulation results using the time-honored MNIST dataset demonstrate learning times at least an order of magnitude faster than other online approaches while providing similar error rates. </details>
<details>	<summary>注释</summary>	13 pages, 10 figures ACM-class: C.3; I.2.6; I.5.3 </details>
<details>	<summary>邮件日期</summary>	2020年11月30日</details>

# 24、结合可学习膜时间常数提高脉冲神经网络的学习能力
- [ ] Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks 
时间：2020年11月27日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2007.05785).                     
<details>	<summary>邮件日期</summary>	2020年11月30日</details>

# 23、PeleNet：Loihi的储层计算框架
- [ ] PeleNet: A Reservoir Computing Framework for Loihi 
时间：2020年11月24日                         第一作者：Carlo Michaelis                       [链接](https://arxiv.org/abs/2011.12338).                     
## 摘要：脉冲神经网络的高级框架是快速原型化和复杂算法高效开发的关键因素。在过去的几年里，这种框架已经出现在传统的计算机上，但是编程神经形态的硬件仍然是一个挑战。通常需要具备神经形态芯片硬件知识的低级编程。PeleNet框架旨在简化神经形态硬件Loihi的储层计算。它是在英特尔的NxSDK之上构建的，是用Python编写的。该框架管理权重矩阵、参数和探测。特别是，它提供了一个自动和有效的网络分布在几个核心和芯片。这样，用户就不用面对技术细节，可以集中精力进行实验。
<details>	<summary>英文摘要</summary>	High-level frameworks for spiking neural networks are a key factor for fast prototyping and efficient development of complex algorithms. Such frameworks have emerged in the last years for traditional computers, but programming neuromorphic hardware is still a challenge. Often low level programming with knowledge about the hardware of the neuromorphic chip is required. The PeleNet framework aims to simplify reservoir computing for the neuromorphic hardware Loihi. It is build on top of the NxSDK from Intel and is written in Python. The framework manages weight matrices, parameters and probes. In particular, it provides an automatic and efficient distribution of networks over several cores and chips. With this, the user is not confronted with technical details and can concentrate on experiments. </details>
<details>	<summary>邮件日期</summary>	2020年11月26日</details>

# 22、面向零镜头跨语言图像检索
- [ ] Towards Zero-shot Cross-lingual Image Retrieval 
时间：2020年11月24日                         第一作者：Pranav Aggarwal                       [链接](https://arxiv.org/abs/2012.05107).                     
## 摘要：最近人们对多模态语言和视觉问题的兴趣激增。在语言方面，由于大多数多模态数据集都是单语的，所以这些模型主要关注英语。我们试图通过在文本方面进行跨语言预训练的零镜头方法来弥补这一差距。我们提出了一个简单而实用的方法来建立一个跨语言图像检索模型，该模型在单语训练数据集上进行训练，但可以在推理过程中以零镜头的跨语言方式使用。我们还引入了一个新的目标函数，通过相互推送不同的文本来收紧文本嵌入簇。最后，我们介绍了一个新的1K多语种MSCOCO2014字幕测试数据集（XTD10），该数据集采用7种语言，我们使用众包平台收集。我们使用它作为跨语言评估零炮模型性能的测试集。XTD10数据集在以下位置公开：https://github.com/adobe-research/Cross-lingual-Test-Dataset-XTD10
<details>	<summary>英文摘要</summary>	There has been a recent spike in interest in multi-modal Language and Vision problems. On the language side, most of these models primarily focus on English since most multi-modal datasets are monolingual. We try to bridge this gap with a zero-shot approach for learning multi-modal representations using cross-lingual pre-training on the text side. We present a simple yet practical approach for building a cross-lingual image retrieval model which trains on a monolingual training dataset but can be used in a zero-shot cross-lingual fashion during inference. We also introduce a new objective function which tightens the text embedding clusters by pushing dissimilar texts from each other. Finally, we introduce a new 1K multi-lingual MSCOCO2014 caption test dataset (XTD10) in 7 languages that we collected using a crowdsourcing platform. We use this as the test set for evaluating zero-shot model performance across languages. XTD10 dataset is made publicly available here: https://github.com/adobe-research/Cross-lingual-Test-Dataset-XTD10 </details>
<details>	<summary>邮件日期</summary>	2020年12月10日</details>

# 21、一种更具生物学意义的人工神经网络局部学习规则
- [ ] A More Biologically Plausible Local Learning Rule for ANNs 
时间：2020年11月24日                         第一作者：Shashi Kant Gupta                       [链接](https://arxiv.org/abs/2011.12012).                     
## 摘要：反向传播算法因其生物学合理性而经常引起争论。然而，为了寻求更具生物学意义的学习，人们提出了各种神经结构的学习方法。他们中的大多数人都试图解决“重量传输问题”，并试图通过一些替代方法在体系结构中向后传播错误。在这项工作中，我们研究了一种稍有不同的方法，它只使用局部信息来捕获脉冲定时信息，而不会传播错误。所提出的学习规则来自于脉冲时间依赖的可塑性和神经元联系的概念。对具有两个隐藏层的MNIST和IRIS数据集的二元分类进行的初步评估表明，其性能与反向传播相当。与通过交叉熵损失反向传播学习的模型相比，使用该方法学习的模型对FGSM攻击具有更好的鲁棒性。学习的局部性为网络中大规模的分布式并行学习提供了可能。最后，提出的方法是一个更符合生物学的方法，可能有助于理解生物神经元如何学习不同的抽象。
<details>	<summary>英文摘要</summary>	The backpropagation algorithm is often debated for its biological plausibility. However, various learning methods for neural architecture have been proposed in search of more biologically plausible learning. Most of them have tried to solve the "weight transport problem" and try to propagate errors backward in the architecture via some alternative methods. In this work, we investigated a slightly different approach that uses only the local information which captures spike timing information with no propagation of errors. The proposed learning rule is derived from the concepts of spike timing dependant plasticity and neuronal association. A preliminary evaluation done on the binary classification of MNIST and IRIS datasets with two hidden layers shows comparable performance with backpropagation. The model learned using this method also shows a possibility of better adversarial robustness against the FGSM attack compared to the model learned through backpropagation of cross-entropy loss. The local nature of learning gives a possibility of large scale distributed and parallel learning in the network. And finally, the proposed method is a more biologically sound method that can probably help in understanding how biological neurons learn different abstractions. </details>
<details>	<summary>注释</summary>	8 pages (4 main + 1 reference + 3 supplementary) </details>
<details>	<summary>邮件日期</summary>	2020年11月25日</details>

# 20、从头开始训练低潜伏期深脉冲神经网络的批标准化研究
- [ ] Revisiting Batch Normalization for Training Low-latency Deep Spiking Neural Networks from Scratch 
时间：2020年11月24日                         第一作者：Youngeun Kim                       [链接](https://arxiv.org/abs/2010.01729).                     
<details>	<summary>邮件日期</summary>	2020年11月25日</details>

# 19、结合可学习膜时间常数提高脉冲神经网络的学习能力
- [ ] Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks 
时间：2020年11月24日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2007.05785).                     
<details>	<summary>邮件日期</summary>	2020年11月25日</details>

# 18、脉冲神经元的自然梯度学习
- [ ] Natural-gradient learning for spiking neurons 
时间：2020年11月23日                         第一作者：Elena Kreutzer                       [链接](https://arxiv.org/abs/2011.11710).                     
## 摘要：在许多突触可塑性的规范理论中，权重的更新隐含地依赖于所选择的权重参数化。例如，这个问题与神经元形态有关：在功能上对躯体放电的影响相当的突触，由于其在树突树上的位置不同，其脊柱大小可能有很大差异。基于欧氏梯度下降的经典理论很容易由于这种参数化依赖而导致不一致。这些问题是在黎曼几何的框架下解决的，在黎曼几何中，我们提出塑性应遵循自然梯度下降。在这一假设下，我们推导出了一个突触学习规则，该规则将功能效率与树突状民主、乘法标度和异突触可塑性等生物学现象的解释结合起来。因此，我们认为，在寻找功能性突触可塑性的过程中，进化可能产生了自己版本的自然梯度下降。
<details>	<summary>英文摘要</summary>	In many normative theories of synaptic plasticity, weight updates implicitly depend on the chosen parametrization of the weights. This problem relates, for example, to neuronal morphology: synapses which are functionally equivalent in terms of their impact on somatic firing can differ substantially in spine size due to their different positions along the dendritic tree. Classical theories based on Euclidean gradient descent can easily lead to inconsistencies due to such parametrization dependence. The issues are solved in the framework of Riemannian geometry, in which we propose that plasticity instead follows natural gradient descent. Under this hypothesis, we derive a synaptic learning rule for spiking neurons that couples functional efficiency with the explanation of several well-documented biological phenomena such as dendritic democracy, multiplicative scaling and heterosynaptic plasticity. We therefore suggest that in its search for functional synaptic plasticity, evolution might have come up with its own version of natural gradient descent. </details>
<details>	<summary>注释</summary>	Joint senior authorship: Walter M. Senn and Mihai A. Petrovici </details>
<details>	<summary>邮件日期</summary>	2020年11月25日</details>

# 17、多层记忆脉冲神经网络的片上错误触发学习
- [ ] On-Chip Error-triggered Learning of Multi-layer Memristive Spiking Neural Networks 
时间：2020年11月21日                         第一作者：Melika Payv                       [链接](https://arxiv.org/abs/2011.10852).                     
## 摘要：神经形态计算的最新突破表明，梯度下降学习的局部形式与脉冲神经网络（SNNs）和突触可塑性是相容的。虽然SNNs可以用神经形态的VLSI可伸缩地实现，但是仍然缺少一种可以在原地使用梯度下降进行学习的体系结构。在本文中，我们提出了一个局部的，梯度为基础的，错误触发学习算法与在线三元权值更新。所提出的算法可以在线训练多层snn与记忆神经形态的硬件表现出小损失的性能相比，国家的最新技术。我们还提出了一种基于忆阻纵横制阵列的硬件结构来执行所需的向量矩阵乘法。采用标准180nmcmos工艺，在亚阈值范围内设计了在线训练所需的外围电路，包括突触前、突触后和写电路。
<details>	<summary>英文摘要</summary>	Recent breakthroughs in neuromorphic computing show that local forms of gradient descent learning are compatible with Spiking Neural Networks (SNNs) and synaptic plasticity. Although SNNs can be scalably implemented using neuromorphic VLSI, an architecture that can learn using gradient-descent in situ is still missing. In this paper, we propose a local, gradient-based, error-triggered learning algorithm with online ternary weight updates. The proposed algorithm enables online training of multi-layer SNNs with memristive neuromorphic hardware showing a small loss in the performance compared with the state of the art. We also propose a hardware architecture based on memristive crossbar arrays to perform the required vector-matrix multiplications. The necessary peripheral circuitry including pre-synaptic, post-synaptic and write circuits required for online training, have been designed in the sub-threshold regime for power saving with a standard 180 nm CMOS process. </details>
<details>	<summary>注释</summary>	15 pages, 11 figures, Journal of Emerging Technology in Circuits and Systems (JETCAS) </details>
<details>	<summary>邮件日期</summary>	2020年11月24日</details>

# 16、快速而深入：具有第一脉冲时间的节能神经形态学习
- [ ] Fast and deep: energy-efficient neuromorphic learning with first-spike times 
时间：2020年11月19日                         第一作者：Julian G\"oltz                       [链接](https://arxiv.org/abs/1912.11443).                     
<details>	<summary>注释</summary>	20 pages, 8 figures </details>
<details>	<summary>邮件日期</summary>	2020年11月20日</details>

# 15、基于生物似然无监督延迟学习的脉冲神经网络时间特征提取
- [ ] Bio-plausible Unsupervised Delay Learning for Extracting Temporal Features in Spiking Neural Networks 
时间：2020年11月18日                         第一作者：Alireza Nadafian                       [链接](https://arxiv.org/abs/2011.09380).                     
## 摘要：神经元间传导延迟的可塑性在学习中起着基础性作用。然而，大脑中这种调节的确切机制仍然是一个开放的问题。了解突触延迟的精确调节可以帮助我们开发有效的大脑启发计算模型，提供与实验证据一致的见解。在这篇论文中，我们提出一个无监督的生物学上合理的学习规则来调整神经网络中的突触延迟。然后，我们提供了一些数学证明来证明我们的学习规则赋予神经元学习重复时空模式的能力。此外，将基于STDP的脉冲神经网络与我们提出的延迟学习规则相结合，应用于随机点运动图的实验结果表明了所提出的延迟学习规则在提取时间特征方面的有效性。
<details>	<summary>英文摘要</summary>	The plasticity of the conduction delay between neurons plays a fundamental role in learning. However, the exact underlying mechanisms in the brain for this modulation is still an open problem. Understanding the precise adjustment of synaptic delays could help us in developing effective brain-inspired computational models in providing aligned insights with the experimental evidence. In this paper, we propose an unsupervised biologically plausible learning rule for adjusting the synaptic delays in spiking neural networks. Then, we provided some mathematical proofs to show that our learning rule gives a neuron the ability to learn repeating spatio-temporal patterns. Furthermore, the experimental results of applying an STDP-based spiking neural network equipped with our proposed delay learning rule on Random Dot Kinematogram indicate the efficacy of the proposed delay learning rule in extracting temporal features. </details>
<details>	<summary>邮件日期</summary>	2020年11月19日</details>

# 14、脉冲神经网络的时间代理反向传播算法
- [ ] Temporal Surrogate Back-propagation for Spiking Neural Networks 
时间：2020年11月18日                         第一作者：Yukun Yang                       [链接](https://arxiv.org/abs/2011.09964).                     
## 摘要：脉冲神经网络（SNN）通常比人工神经网络（ANN）更节能，其工作方式与我们的大脑有很大的相似性。近年来，BP算法在神经网络训练中显示出了强大的能力。然而，由于脉冲行为是不可微的，BP不能直接应用于SNN。虽然已有的工作证明了在空间和时间方向上通过替代梯度或随机性来逼近BP梯度的几种方法，但是它们忽略了每一步之间重置机制引入的时间依赖性。本文以理论完善为目标，深入研究了缺失项的影响。通过增加重置机制的时间依赖性，新算法对玩具数据集的学习率调整更具鲁棒性，但对CIFAR-10等较大的学习任务没有太大的改进。从经验上讲，缺失项的好处不值得额外的计算开销。在许多情况下，可以忽略缺少的项。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNN) are usually more energy-efficient as compared to Artificial neural networks (ANN), and the way they work has a great similarity with our brain. Back-propagation (BP) has shown its strong power in training ANN in recent years. However, since spike behavior is non-differentiable, BP cannot be applied to SNN directly. Although prior works demonstrated several ways to approximate the BP-gradient in both spatial and temporal directions either through surrogate gradient or randomness, they omitted the temporal dependency introduced by the reset mechanism between each step. In this article, we target on theoretical completion and investigate the effect of the missing term thoroughly. By adding the temporal dependency of the reset mechanism, the new algorithm is more robust to learning-rate adjustments on a toy dataset but does not show much improvement on larger learning tasks like CIFAR-10. Empirically speaking, the benefits of the missing term are not worth the additional computational overhead. In many cases, the missing term can be ignored. </details>
<details>	<summary>注释</summary>	4 pases, 3 figures, 3 tables, 10 eqs </details>
<details>	<summary>邮件日期</summary>	2020年11月20日</details>

# 13、一种用于术中心电图高频振荡检测的脉冲神经网络（SNN）
- [ ] A Spiking Neural Network (SNN) for detecting High Frequency Oscillations (HFOs) in the intraoperative ECoG 
时间：2020年11月17日                         第一作者：Karla Burelo                        [链接](https://arxiv.org/abs/2011.08783).                     
## 摘要：癫痫手术需要彻底切除致痫脑组织，才能实现癫痫发作的自由。在术中的ECoG记录中，由致痫组织产生的高频振荡（HFOs）可以用来调整切除边缘。然而，实时自动检测HFOs仍然是一个开放的挑战。在这里，我们提出了一个脉冲神经网络（SNN）的自动HFO检测，是最适合神经形态的硬件实现。我们训练SNN来检测术中ECoG在线测量的HFO信号，使用一个独立标记的数据集。我们针对快速纹波频率范围（250-500hz）的HFO检测，并将网络结果与标记的HFO数据进行比较。我们赋予SNN一种新的伪影抑制机制来抑制突变，并在ECoG数据集上验证了其有效性。该SNN检测到的HFO率（术前记录中位数为6.6 HFO/min）与数据集中公布的HFO率（58 min，16次记录）相当。所有8例患者术后癫痫发作结果的“预测”准确率均为100%。这些结果为建立一个可在癫痫手术中用于指导致痫区切除的实时便携式电池供电HFO检测系统提供了进一步的进展。
<details>	<summary>英文摘要</summary>	To achieve seizure freedom, epilepsy surgery requires the complete resection of the epileptogenic brain tissue. In intraoperative ECoG recordings, high frequency oscillations (HFOs) generated by epileptogenic tissue can be used to tailor the resection margin. However, automatic detection of HFOs in real-time remains an open challenge. Here we present a spiking neural network (SNN) for automatic HFO detection that is optimally suited for neuromorphic hardware implementation. We trained the SNN to detect HFO signals measured from intraoperative ECoG on-line, using an independently labeled dataset. We targeted the detection of HFOs in the fast ripple frequency range (250-500 Hz) and compared the network results with the labeled HFO data. We endowed the SNN with a novel artifact rejection mechanism to suppress sharp transients and demonstrate its effectiveness on the ECoG dataset. The HFO rates (median 6.6 HFO/min in pre-resection recordings) detected by this SNN are comparable to those published in the dataset (58 min, 16 recordings). The postsurgical seizure outcome was "predicted" with 100% accuracy for all 8 patients. These results provide a further step towards the construction of a real-time portable battery-operated HFO detection system that can be used during epilepsy surgery to guide the resection of the epileptogenic zone. </details>
<details>	<summary>注释</summary>	11 pages, 3 figures, 2 tables. The results of this publication were obtained by simulating our hardware platform, built for online processing of biological signals. This hardware combines neural recording headstages with a multi-core neuromorphic processor arxiv.org/abs/2009.11245 </details>
<details>	<summary>邮件日期</summary>	2020年11月18日</details>

# 12、结合可学习膜时间常数提高脉冲神经网络的学习能力
- [ ] Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks 
时间：2020年11月16日                         第一作者：Wei Fang                       [链接](https://arxiv.org/abs/2007.05785).                     
<details>	<summary>邮件日期</summary>	2020年11月17日</details>

# 11、具有Alpha突触功能的脉冲神经网络中的时间编码：反向传播学习
- [ ] Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function: Learning with Backpropagation 
时间：2020年11月16日                         第一作者：Iulia M. Comsa                       [链接](https://arxiv.org/abs/1907.13223).                     
<details>	<summary>注释</summary>	Open-source code related to this paper is available at https://github.com/google/ihmehimmeli v2: Added references and added some clarifications for the methods </details>
<details>	<summary>邮件日期</summary>	2020年11月18日</details>

# 10、LIAF-Net：轻量级高效时空信息处理的漏泄集成模拟消防网络
- [ ] LIAF-Net: Leaky Integrate and Analog Fire Network for Lightweight and Efficient Spatiotemporal Information Processing 
时间：2020年11月12日                         第一作者：Zhenzhi Wu                       [链接](https://arxiv.org/abs/2011.06176).                     
## 摘要：基于漏积分火灾（LIF）模型的脉冲神经网络（SNNs）已被应用于节能的时空处理任务中。由于生物似有理的神经元动力学和简单性，LIF-SNN受益于事件驱动处理，然而，通常面临性能下降的尴尬。这可能是因为在LIF-SNN中，神经元通过脉冲传递信息。为了解决这一问题，本文提出了一种漏积分模拟火灾（LIAF）神经元模型，使得模拟值可以在神经元之间传输，并在此基础上建立了一个称为LIAF网络的深层网络，以实现高效的时空处理。在时域上，LIAF遵循传统的LIF动态机制来保持其时间处理能力。在空间域中，LIAF能够通过卷积积分或全连通积分对空间信息进行集成。作为一个时空层，LIAF也可以与传统的人工神经网络（ANN）层联合使用。实验结果表明，在bAbI问答（QA）任务中，LIAF网络的性能与选通递归单元（GRU）和长短时记忆（LSTM）相当，在时空动态视觉传感器（DVS）数据集（包括MNIST-DVS、CIFAR10-DVS和DVS128手势）上，LIAF网络的性能达到了最先进的水平，但数量要少得多与传统的LSTM、GRU、卷积LSTM（ConvLSTM）或3D卷积（Conv3D）构建的网络相比，突触权值和计算开销都有较大的提高。与传统的LIF-SNN相比，LIAF网络在所有这些实验中也显示出显著的精度提高。总之，LIAF-Net提供了一个结合ANNs和SNNs优点的轻量级高效时空信息处理框架。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) based on Leaky Integrate and Fire (LIF) model have been applied to energy-efficient temporal and spatiotemporal processing tasks. Thanks to the bio-plausible neuronal dynamics and simplicity, LIF-SNN benefits from event-driven processing, however, usually faces the embarrassment of reduced performance. This may because in LIF-SNN the neurons transmit information via spikes. To address this issue, in this work, we propose a Leaky Integrate and Analog Fire (LIAF) neuron model, so that analog values can be transmitted among neurons, and a deep network termed as LIAF-Net is built on it for efficient spatiotemporal processing. In the temporal domain, LIAF follows the traditional LIF dynamics to maintain its temporal processing capability. In the spatial domain, LIAF is able to integrate spatial information through convolutional integration or fully-connected integration. As a spatiotemporal layer, LIAF can also be used with traditional artificial neural network (ANN) layers jointly. Experiment results indicate that LIAF-Net achieves comparable performance to Gated Recurrent Unit (GRU) and Long short-term memory (LSTM) on bAbI Question Answering (QA) tasks, and achieves state-of-the-art performance on spatiotemporal Dynamic Vision Sensor (DVS) datasets, including MNIST-DVS, CIFAR10-DVS and DVS128 Gesture, with much less number of synaptic weights and computational overhead compared with traditional networks built by LSTM, GRU, Convolutional LSTM (ConvLSTM) or 3D convolution (Conv3D). Compared with traditional LIF-SNN, LIAF-Net also shows dramatic accuracy gain on all these experiments. In conclusion, LIAF-Net provides a framework combining the advantages of both ANNs and SNNs for lightweight and efficient spatiotemporal information processing. </details>
<details>	<summary>注释</summary>	14 pages, 9 figures, submitted to IEEE Transactions on Neural Networks and Learning Systems ACM-class: I.2.6 </details>
<details>	<summary>邮件日期</summary>	2020年11月13日</details>

# 9、利用生物似然奖赏传播调整卷积脉冲神经网络
- [x] Tuning Convolutional Spiking Neural Network with Biologically-plausible Reward Propagation 
时间：2020年11月12日                         第一作者：Tielin Zhang                        [链接](https://arxiv.org/abs/2010.04434).                     
<details>	<summary>邮件日期</summary>	2020年11月13日</details>

# 8、基于VCSEL神经元的全光神经形态二值卷积算法
- [ ] All-optical neuromorphic binary convolution with a spiking VCSEL neuron for image gradient magnitudes 
时间：2020年11月09日                         第一作者：Yahui Zhang                       [链接](https://arxiv.org/abs/2011.04438).                     
## 摘要：首次提出了一种基于光子脉冲垂直腔面发射激光器（VCSEL）神经元的全光二值卷积方法，并进行了实验验证。从数字图像中提取并使用矩形脉冲进行时间编码的光输入被注入VCSEL神经元中，VCSEL神经元提供快速（<100ps长）脉冲发射数的卷积结果。实验和数值结果表明，采用单脉冲VCSEL神经元实现了二值卷积，全光二值卷积可用于计算图像梯度大小，检测边缘特征，分离源图像中的垂直分量和水平分量。我们还证明了这种全光脉冲二值卷积系统对噪声具有很强的鲁棒性，并且可以处理高分辨率的图像。此外，该系统还具有速度快、能量效率高、硬件实现简单等优点，突出了脉冲光子VCSEL神经元在高速神经图像处理系统和未来光子脉冲卷积神经网络中的应用潜力。
<details>	<summary>英文摘要</summary>	All-optical binary convolution with a photonic spiking vertical-cavity surface-emitting laser (VCSEL) neuron is proposed and demonstrated experimentally for the first time. Optical inputs, extracted from digital images and temporally encoded using rectangular pulses, are injected in the VCSEL neuron which delivers the convolution result in the number of fast (<100 ps long) spikes fired. Experimental and numerical results show that binary convolution is achieved successfully with a single spiking VCSEL neuron and that all-optical binary convolution can be used to calculate image gradient magnitudes to detect edge features and separate vertical and horizontal components in source images. We also show that this all-optical spiking binary convolution system is robust to noise and can operate with high-resolution images. Additionally, the proposed system offers important advantages such as ultrafast speed, high energy efficiency and simple hardware implementation, highlighting the potentials of spiking photonic VCSEL neurons for high-speed neuromorphic image processing systems and future photonic spiking convolutional neural networks. </details>
<details>	<summary>注释</summary>	jxxsy@126.com; antonio.hurtado@strath.ac.uk </details>
<details>	<summary>邮件日期</summary>	2020年11月10日</details>

# 7、用gpu快速模拟高度连接的棘波皮层模型
- [ ] Fast simulations of highly-connected spiking cortical models using GPUs 
时间：2020年11月09日                         第一作者：Bruno Golosio                       [链接](https://arxiv.org/abs/2007.14236).                     
<details>	<summary>邮件日期</summary>	2020年11月10日</details>

# 6、你只刺一次：提高能源效率神经形态推理到神经网络水平的准确性
- [ ] You Only Spike Once: Improving Energy-Efficient Neuromorphic Inference to ANN-Level Accuracy 
时间：2020年11月08日                         第一作者：Srivatsa P                        [链接](https://arxiv.org/abs/2006.09982).                     
<details>	<summary>注释</summary>	10 pages, 4 figures. This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work is an extended version of the paper accepted to the 2nd Workshop on Accelerated Machine Learning (AccML 2020) </details>
<details>	<summary>邮件日期</summary>	2020年11月10日</details>

# 5、深脉冲神经网络反向传播的校正线性突触后电位函数
- [x] Rectified Linear Postsynaptic Potential Function for Backpropagation in Deep Spiking Neural Networks 
时间：2020年11月04日                         第一作者：Malu Zhang                       [链接](https://arxiv.org/abs/2003.11837).                     
<details>	<summary>注释</summary>	This work has been submitted to the IEEE for possible publication. Copyrightmay be transferred without notice, after which this version may no longer beaccessible </details>
<details>	<summary>邮件日期</summary>	2020年11月05日</details>

# 4、脉冲耦合振荡器网络中的受控微扰诱导开关
- [x] Controlled Perturbation-Induced Switching in Pulse-Coupled Oscillator Networks 
时间：2020年11月02日                         第一作者：Fabio Schittler Neves                        [链接](https://arxiv.org/abs/2011.00888).                     
## 摘要：脉冲耦合系统，如脉冲神经网络，表现出非平凡不变集的形式吸引但不稳定的鞍周期轨道的单位同步成组。这些轨道之间的异宿连接原则上可以支持这些网络中的切换过程，并支持新的神经计算。对于耦合振子的小网络，我们在此研究在何种条件下以及系统对称性如何强制或禁止某些可能由扰动引起的开关跃迁。对于由五个振子组成的网络，我们导出了两个团簇对称性的显式跃迁规则，这些规则偏离了已知的连续耦合振子的跃迁规则。第三种对称产生异宿网络，它由所有具有这种对称性的不稳定吸引子以及它们之间的连接组成。我们的结果表明，脉冲耦合系统能够可靠地产生符合特定转换规则的复杂时空模式。我们简要地讨论了脉冲神经系统计算的可能含义。
<details>	<summary>英文摘要</summary>	Pulse-coupled systems such as spiking neural networks exhibit nontrivial invariant sets in the form of attracting yet unstable saddle periodic orbits where units are synchronized into groups. Heteroclinic connections between such orbits may in principle support switching processes in those networks and enable novel kinds of neural computations. For small networks of coupled oscillators we here investigate under which conditions and how system symmetry enforces or forbids certain switching transitions that may be induced by perturbations. For networks of five oscillators we derive explicit transition rules that for two cluster symmetries deviate from those known from oscillators coupled continuously in time. A third symmetry yields heteroclinic networks that consist of sets of all unstable attractors with that symmetry and the connections between them. Our results indicate that pulse-coupled systems can reliably generate well-defined sets of complex spatiotemporal patterns that conform to specific transition rules. We briefly discuss possible implications for computation with spiking neural systems. </details>
<details>	<summary>邮件日期</summary>	2020年11月03日</details>

# 3、RANC：可重构的神经形态计算体系结构
- [ ] RANC: Reconfigurable Architecture for Neuromorphic Computing 
时间：2020年11月01日                         第一作者：Joshua Mack                       [链接](https://arxiv.org/abs/2011.00624).                     
## 摘要：神经形态结构已经被引入作为能量有效的脉冲神经网络执行的平台。这些体系结构所提供的大规模并行性也引起了非机器学习应用领域的兴趣。为了提升硬件设计者和应用开发者的进入壁垒，我们提出了RANC：一种可重构的神经形态计算体系结构，一个开源的高度灵活的生态系统，通过C++仿真和硬件通过FPGA仿真，能够快速地在软件中对神经形态结构进行实验。我们展示了RANC生态系统的实用性，通过展示其重现IBM的TrueNorth行为的能力，并通过与IBM的Compass模拟环境和已发表文献的直接比较进行验证。RANC允许基于应用程序洞察优化架构，以及原型化可以完全支持新类应用程序的未来神经形态架构。通过基于Alveo U250 FPGA的定量分析，研究了体系结构变化对提高应用程序映射效率的影响，证明了RANC的高度参数化和可配置性。本文介绍了合成孔径雷达分类和矢量矩阵乘法应用的路由后资源使用和吞吐量分析，并展示了一个可扩展到模拟259K个不同神经元和733m个不同突触的神经形态结构。
<details>	<summary>英文摘要</summary>	Neuromorphic architectures have been introduced as platforms for energy efficient spiking neural network execution. The massive parallelism offered by these architectures has also triggered interest from non-machine learning application domains. In order to lift the barriers to entry for hardware designers and application developers we present RANC: a Reconfigurable Architecture for Neuromorphic Computing, an open-source highly flexible ecosystem that enables rapid experimentation with neuromorphic architectures in both software via C++ simulation and hardware via FPGA emulation. We present the utility of the RANC ecosystem by showing its ability to recreate behavior of the IBM's TrueNorth and validate with direct comparison to IBM's Compass simulation environment and published literature. RANC allows optimizing architectures based on application insights as well as prototyping future neuromorphic architectures that can support new classes of applications entirely. We demonstrate the highly parameterized and configurable nature of RANC by studying the impact of architectural changes on improving application mapping efficiency with quantitative analysis based on Alveo U250 FPGA. We present post routing resource usage and throughput analysis across implementations of Synthetic Aperture Radar classification and Vector Matrix Multiplication applications, and demonstrate a neuromorphic architecture that scales to emulating 259K distinct neurons and 73.3M distinct synapses. </details>
<details>	<summary>注释</summary>	18 pages, 12 figures, accepted for publication in IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems. For associated source files see https://github.com/UA-RCL/RANC </details>
<details>	<summary>邮件日期</summary>	2020年11月03日</details>

# 2、基于Loihi处理器的mav光流着陆神经形态控制
- [x] Neuromorphic control for optic-flow-based landings of MAVs using the Loihi processor 
时间：2020年11月01日                         第一作者：Julien Dupeyroux                       [链接](https://arxiv.org/abs/2011.00534).                     
## 摘要：像Loihi这样的神经形态处理器为微型飞行器（mav）这样的受限系统提供了一个有希望的替代传统计算模块，使其具有强大、高效和自主的技能，如起飞和着陆、避障和追踪。然而，在机器人平台上使用这种处理器的一个主要挑战是模拟和真实世界之间的现实差距。在这项研究中，我们首次提出了一个完全嵌入式应用的Loihi神经芯片原型在飞行机器人。为了实现自主着陆，提出了一种基于腹部光流场发散的脉冲神经网络（SNN）来计算推力指令。进化是使用PySNN库在基于Python的模拟器中执行的。该网络结构仅由分布在3层中的35个神经元组成。仿真和Loihi之间的定量分析表明，推力设定值的均方根误差低至0.005 g，同时，隐藏层的脉冲序列匹配率为99.8%，输出层的脉冲序列匹配率为99.7%。所提出的方法成功地填补了现实差距，为未来机器人中的神经形态应用提供了重要的见解。补充材料可在https://mavlab.tudelft.nl/loihi/。
<details>	<summary>英文摘要</summary>	Neuromorphic processors like Loihi offer a promising alternative to conventional computing modules for endowing constrained systems like micro air vehicles (MAVs) with robust, efficient and autonomous skills such as take-off and landing, obstacle avoidance, and pursuit. However, a major challenge for using such processors on robotic platforms is the reality gap between simulation and the real world. In this study, we present for the very first time a fully embedded application of the Loihi neuromorphic chip prototype in a flying robot. A spiking neural network (SNN) was evolved to compute the thrust command based on the divergence of the ventral optic flow field to perform autonomous landing. Evolution was performed in a Python-based simulator using the PySNN library. The resulting network architecture consists of only 35 neurons distributed among 3 layers. Quantitative analysis between simulation and Loihi reveals a root-mean-square error of the thrust setpoint as low as 0.005 g, along with a 99.8% matching of the spike sequences in the hidden layer, and 99.7% in the output layer. The proposed approach successfully bridges the reality gap, offering important insights for future neuromorphic applications in robotics. Supplementary material is available at https://mavlab.tudelft.nl/loihi/. </details>
<details>	<summary>邮件日期</summary>	2020年11月03日</details>

# 1、用直接训练的更大的脉冲神经网络进行更深入的研究
- [x] Going Deeper With Directly-Trained Larger Spiking Neural Networks 
时间：2020年10月29日                         第一作者：Hanle Zheng                       [链接](https://arxiv.org/abs/2011.05280).                     
## 摘要：脉冲神经网络（Spiking neural networks，SNNs）在时空信息和事件驱动信号处理的生物似然编码方面有着广阔的应用前景，非常适合于神经形态硬件的节能实现。然而，SNNs独特的工作模式使其比传统网络更难训练。目前，探索高性能深层snn的培养主要有两条途径。第一种方法是将预先训练好的神经网络模型转换为SNN模型，SNN模型通常需要较长的编码窗口才能收敛，并且在训练过程中不能利用时空特征来求解时间任务。另一种是直接在时空域训练snn。但是由于触发函数的二元脉冲活动和梯度消失或爆炸的问题，目前的方法局限于浅层结构，因此难以利用大规模数据集（如ImageNet）。为此，我们提出了一种基于时空反向传播的阈值相关批处理归一化（tdBN）方法，称为STBP-tdBN，它能够直接训练非常深的SNN并在神经形态硬件上有效地实现其推理。通过提出的方法和详细的快捷连接，我们将直接训练的snn从浅层（<10层）扩展到非常深的结构（50层）。在此基础上，从理论上分析了基于块动态等距理论的方法的有效性。最后，我们报告了更高的准确率结果，包括93.15%的CIFAR-10，67.8%的DVS-CIFAR10和67.05%的ImageNet与很少的时间步长。据我们所知，这是第一次在ImageNet上探索直接训练的高性能深度snn。
<details>	<summary>英文摘要</summary>	Spiking neural networks (SNNs) are promising in a bio-plausible coding for spatio-temporal information and event-driven signal processing, which is very suited for energy-efficient implementation in neuromorphic hardware. However, the unique working mode of SNNs makes them more difficult to train than traditional networks. Currently, there are two main routes to explore the training of deep SNNs with high performance. The first is to convert a pre-trained ANN model to its SNN version, which usually requires a long coding window for convergence and cannot exploit the spatio-temporal features during training for solving temporal tasks. The other is to directly train SNNs in the spatio-temporal domain. But due to the binary spike activity of the firing function and the problem of gradient vanishing or explosion, current methods are restricted to shallow architectures and thereby difficult in harnessing large-scale datasets (e.g. ImageNet). To this end, we propose a threshold-dependent batch normalization (tdBN) method based on the emerging spatio-temporal backpropagation, termed "STBP-tdBN", enabling direct training of a very deep SNN and the efficient implementation of its inference on neuromorphic hardware. With the proposed method and elaborated shortcut connection, we significantly extend directly-trained SNNs from a shallow structure ( < 10 layer) to a very deep structure (50 layers). Furthermore, we theoretically analyze the effectiveness of our method based on "Block Dynamical Isometry" theory. Finally, we report superior accuracy results including 93.15 % on CIFAR-10, 67.8 % on DVS-CIFAR10, and 67.05% on ImageNet with very few timesteps. To our best knowledge, it's the first time to explore the directly-trained deep SNNs with high performance on ImageNet. </details>
<details>	<summary>注释</summary>	12 pages, 6 figures, conference or other essential info </details>
<details>	<summary>邮件日期</summary>	2020年11月11日</details>

